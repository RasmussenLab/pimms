{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import config\n",
    "from src.analyzers import *\n",
    "from vaep.transform import StandardScaler, get_df_fitted_mean_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from src.logging import setup_logger\n",
    "logger = setup_logger(logger=logging.getLogger('vaep'))\n",
    "logger.info(\"Experiment 01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "- 1000 features (most abundant peptides)\n",
    "- later a subset of samples is selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES_TO_LOAD = None\n",
    "FN_PEPTIDE_INTENSITIES = config.FOLDER_DATA / 'df_intensities_N_07813_M01000'\n",
    "analysis = AnalyzePeptides(fname=FN_PEPTIDE_INTENSITIES, nrows=N_SAMPLES_TO_LOAD)\n",
    "analysis.df = analysis.df.sort_index()  # sort by date\n",
    "assert analysis.df.index.is_unique, \"Non-unique training samples\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select consecutives samples for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "N_SAMPLES = 1000\n",
    "logging.info(f\"Selected {N_SAMPLES}\")\n",
    "analysis.N_SAMPLES = N_SAMPLES\n",
    "\n",
    "def get_consecutive_data_indices(index, n_samples=N_SAMPLES):\n",
    "    start_sample = len(index) - n_samples\n",
    "    start_sample = random.randint(0, start_sample)\n",
    "    return index[start_sample:start_sample+n_samples]\n",
    "\n",
    "\n",
    "indices_selected = get_consecutive_data_indices(analysis.df.index)\n",
    "analysis.df = analysis.df.loc[indices_selected]\n",
    "analysis.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create meta data from filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "columns = 'date ms_instrument columntype researcher rest'.split()\n",
    "\n",
    "RunMetaData = namedtuple('RunMetaData', columns)\n",
    "data_meta = {}\n",
    "for filename in indices_selected:\n",
    "    # ToDo: this approach is too easy for the moment. The first two fields are in order, the rest needs matching.\n",
    "    _meta_filename = filename.split('_', maxsplit=4)\n",
    "    data_meta[filename] = _meta_filename\n",
    "    # print(RunMetaData(*_meta_filename[:6]))\n",
    "\n",
    "analysis.df_meta = pd.DataFrame.from_dict(\n",
    "    data_meta, orient='index', columns=columns)\n",
    "# analysis.df_meta['date'] = pd.to_datetime(analysis.df_meta['date'])\n",
    "analysis.df_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRACTION = 0.9\n",
    "\n",
    "class Indices(SimpleNamespace):\n",
    "    pass\n",
    "\n",
    "indices = Indices()\n",
    "indices.train, indices.val = indices_selected[:int(\n",
    "    FRACTION*N_SAMPLES)], indices_selected[int(FRACTION*N_SAMPLES):]\n",
    "analysis.indices = indices\n",
    "\n",
    "analysis.df_meta.loc[indices.train].describe(datetime_is_numeric=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This becomes part of analysis\n",
    "def compare_meta_data_for_splits(meta, indices):\n",
    "\n",
    "    _indices = vars(indices)\n",
    "    logging.info('Found vars: {}'.format(', '.join(str(x)\n",
    "                                                   for x in _indices.keys())))\n",
    "\n",
    "    for key_split, split in _indices.items():\n",
    "        print(f\"{key_split:8} - split description:\")\n",
    "        display(\n",
    "            meta.loc[split].describe(datetime_is_numeric=True)\n",
    "        )\n",
    "\n",
    "    _meta_features = list(meta.columns)\n",
    "\n",
    "    for _column in _meta_features:\n",
    "        display(\n",
    "            _ = pd.DataFrame({\n",
    "                key_split: meta.loc[split, _column].value_counts(normalize=True) for key_split, split in _indices.items()\n",
    "            }).sort_index().plot(kind='line', rot=90, figsize=(10,5), title=f\"{_column} value Counts for different splits\")\n",
    "        )\n",
    "compare_meta_data_for_splits(analysis.df_meta.iloc[:,:2], indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Transforms\n",
    "\n",
    "- illustrate using adapted scikit-learn [`StandardScaler`](https://scikit-learn.org/stable/modules/preprocessing.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, M = 10, 10  # Samples, Features\n",
    "analysis.df.iloc[:N, :M]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.df.iloc[:, :M].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StandardScaler on raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(analysis.df)\n",
    "scaler_df = get_df_fitted_mean_std(scaler, index=analysis.df.columns)\n",
    "scaler_df.head(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = scaler.transform(analysis.df.iloc[:N])\n",
    "sample.iloc[:, :M]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = scaler.inverse_transform(sample)\n",
    "sample.iloc[:, :M]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StandardScaler on log10 transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_log10 = np.log10(analysis.df)\n",
    "X_log10.iloc[:N, :M]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_log = StandardScaler(\n",
    ").fit(X=X_log10)\n",
    "scaler_log_df = get_df_fitted_mean_std(scaler_log, index=analysis.df.index)\n",
    "scaler_log_df.head(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_log10 = scaler_log.transform(X_log10.iloc[:N])\n",
    "sample_log10.iloc[:, :M]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_log.inverse_transform(sample_log10).iloc[:, :M]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation\n",
    "\n",
    "- Correlation between the computed `means_` should be nearly perfect\n",
    "- Correlation between peptide intensities should be high\n",
    "- As taking the logarithm is a monoton, but non-linear transformation, the linear Pearson correlation can change substantially. [[link]](https://stats.stackexchange.com/questions/127121/do-logs-modify-the-correlation-between-two-variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Correlation between mean values of linear vs. log-transformed values:\",\n",
    "      f\"{np.corrcoef(scaler.mean_, scaler_log.mean_)[1,0]:.4f}\", sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.3f}'.format\n",
    "\n",
    "analysis.corr_linear_vs_log = scaler.transform(X=analysis.df).corrwith(\n",
    "    other=scaler_log.transform(X_log10),\n",
    "    axis=0)\n",
    "analysis.corr_linear_vs_log.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# own implemention could be slightly faster as data is already demeanded and standardized.\n",
    "# pd.DataFrame.corrwith?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from vaep.utils import sample_iterable\n",
    "\n",
    "columns_sampled = sample_iterable(list(analysis.df.columns), n=12)\n",
    "print(columns_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scaled_sample(columns_sampled: list, scaler, df: pd.DataFrame = analysis.df):\n",
    "    _scaled = scaler.transform(df)\n",
    "    display(_scaled.describe())\n",
    "    _min, _max = _scaled.min().min(), _scaled.max().max()\n",
    "    return _min, _max\n",
    "    print(list(range(_min, _max, step=0.5)))\n",
    "\n",
    "\n",
    "_min, _max = plot_scaled_sample(columns_sampled=columns_sampled, scaler=scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if bins should be equal between plots\n",
    "# addon\n",
    "import math\n",
    "xlim = [-5, 5]\n",
    "FACTOR = 1\n",
    "[x/FACTOR for x in range(math.floor(xlim[0])*FACTOR,\n",
    "                         math.ceil(xlim[1])*FACTOR+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "columns_sampled = sample_iterable(list(analysis.df.columns), n=9)\n",
    "subplot_kw = {'xlim': [-5, 5], 'ylim': [0, 600]}\n",
    "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(\n",
    "    15, 15), sharey=True, sharex=True, subplot_kw=subplot_kw)\n",
    "_ = scaler_log.transform(X_log10)[columns_sampled].hist(\n",
    "    figsize=(15, 15), ax=axes)\n",
    "axes = scaler.transform(analysis.df)[columns_sampled].hist(\n",
    "    figsize=(15, 15), ax=axes)\n",
    "_ = fig.legend(('linear', 'log'), loc=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption = (f\"Frequencies are capped at {subplot_kw['ylim'][1]} and \"\n",
    "           \"their standardized intensity values plotted between {} and {}.\".format(\n",
    "               *subplot_kw['xlim'])\n",
    "           )\n",
    "print(caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fastai Dataloader\n",
    "\n",
    "> fastai includes a replacement for Pytorch's DataLoader which is largely API-compatible, and adds a lot of useful functionality and flexibility. Before we look at the class, there are a couple of helpers we'll need to define. [[link](https://docs.fast.ai/data.load.html)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai.tabular.all as tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.tabular.data import TabularDataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataloaders using an appropriate factory method from `TabularDataLoaders` class, here [`from_df`](https://docs.fast.ai/tabular.data.html#TabularDataLoaders.from_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame is shuffled\n",
    "N_VAL = 100\n",
    "valid_idx = list(range(N_VAL))\n",
    "dls = TabularDataLoaders.from_df(df=analysis.df, valid_idx=valid_idx, bs=64)\n",
    "analysis.dls = dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.show_batch()  # loses object index attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.valid.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dls.train:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now the `loss_func` signature and the `NN_Module` forward path have to be adapted. Unsure how to do this in plain PyTorch yet. So we only use the dataloader for now.\n",
    "\n",
    "- Callback needed to set `xb` to `yb`, see [callback-attributes](https://docs.fast.ai/callback.core.html#Attributes-available-to-callbacks) and [example](https://github.com/dhuynh95/fastai_autoencoder/blob/bc357927f26273d676dca9a41018411408b97430/fastai_autoencoder/callback.py#L16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_function(recon_x=recon_batch, x=batch, mask=mask, mu=mu, logvar=logvar)\n",
    "# learn = Learner(dls, NN_Module, opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# import importlib; importlib.reload(vaep.model)\n",
    "from vaep.model import train\n",
    "from vaep.model import VAE\n",
    "from vaep.model import loss_function\n",
    "from vaep.cmd import get_args\n",
    "\n",
    "from vaep.io.datasets import PeptideDatasetInMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = get_args(no_cuda=True)\n",
    "kwargs = {'num_workers': 2, 'pin_memory': True} if args.cuda else {}\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "# EpochAverages = namedtuple(\"EpochAverages\", 'loss mse kld')\n",
    "\n",
    "\n",
    "def eval(epoch, model, data_loader, device, writer=None):\n",
    "    model.eval()\n",
    "    metrics = {'loss': 0, 'mse': 0,  'kld': 0}\n",
    "\n",
    "    for batch, mask in data_loader:\n",
    "        recon_batch, mu, logvar = model(batch)\n",
    "        loss, mse, kld = loss_function(\n",
    "            recon_x=recon_batch, x=batch, mask=mask, mu=mu, logvar=logvar)\n",
    "        metrics['loss'] += loss.item()\n",
    "        metrics['mse'] += mse.item()\n",
    "        metrics['kld'] += kld.item()\n",
    "    if writer is not None:\n",
    "        n_samples = len(data_loader.dataset)\n",
    "        writer.add_scalar('avg validation loss',\n",
    "                          metrics['loss'] / n_samples,\n",
    "                          epoch)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_limit = np.log10(analysis.df).min().min()  # all zeros become nan.\n",
    "\"Detection limit: {:6.3f}, corresponding to intensity value of {:,d}\".format(\n",
    "    detection_limit,\n",
    "    int(10 ** detection_limit)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_features = analysis.df.shape\n",
    "\"N samples: {:10,d} - N Peptides: {:10,d}\".format(n_samples, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random sample is drawn\n",
    "PROPORTION_TRAIN = 0.9\n",
    "\n",
    "# could be a method on analysis\n",
    "analysis.indices_train = analysis.df.sample(\n",
    "    int(n_samples*PROPORTION_TRAIN)\n",
    ").index\n",
    "analysis.indices_val = analysis.df.index.difference(analysis.indices_train)\n",
    "\n",
    "analysis.df_train = analysis.df.loc[analysis.indices_train]\n",
    "analysis.df_val = analysis.df.loc[analysis.indices_val]\n",
    "\n",
    "len(analysis.indices_val), analysis.indices_val[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling non-log transformed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale samples according to training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select initial data: transformed vs not log transformed\n",
    "scaler = scaler.fit(analysis.df_train)\n",
    "# five examples from validation dataset\n",
    "scaler.transform(analysis.df_val.iloc[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = PeptideDatasetInMemory(data=scaler.transform(analysis.df_train))\n",
    "data_valid = PeptideDatasetInMemory(data=scaler.transform(analysis.df_val))\n",
    "\n",
    "dl_train = torch.utils.data.DataLoader(\n",
    "    dataset=data_train,\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "dl_val = torch.utils.data.DataLoader(\n",
    "    dataset=data_valid,\n",
    "    batch_size=args.batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "print(\n",
    "    \"N train: {:5,d} \\nN valid: {:5,d}\".format(\n",
    "        len(dl_train.dataset), len(dl_val.dataset))\n",
    ")\n",
    "\n",
    "\n",
    "data, mask = next(iter(dl_train))\n",
    "\n",
    "prefix_fn = 'experiment_01'\n",
    "logdir = f'runs/{prefix_fn}_{format(datetime.now(), \"%y%m%d_%H%M\")}'\n",
    "writer = SummaryWriter(logdir)\n",
    "writer.add_image(\n",
    "    f'{len(data)} batch of sampled data (as heatmap)', data, dataformats='HW')\n",
    "writer.add_image(\n",
    "    f'{len(mask)} mask for this batch of samples', mask, dataformats='HW')\n",
    "\n",
    "\n",
    "n_neurons = max(30, int(n_features/6))\n",
    "print(f'Latent layer neurons: {n_neurons}')\n",
    "\n",
    "model = VAE(n_features=n_features, n_neurons=n_neurons).double()\n",
    "# model.eval()\n",
    "print(model)\n",
    "model = model.to(device, non_blocking=True)\n",
    "writer.add_graph(model, input_to_model=data)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {}\n",
    "evaluation = {}\n",
    "msg_eval_epoch = \"Validation Set - Epoch: {:3d} - loss: {:7.3f} - mse: {:5.3f} - KLD: {:5.3f}\"\n",
    "\n",
    "# args.epochs = 10; args.epochs\n",
    "\n",
    "for epoch in range(1, args.epochs):\n",
    "    losses[epoch] = train(epoch, model=model, train_loader=dl_train,\n",
    "                          optimizer=optimizer, device=device, writer=writer)\n",
    "    evaluation[epoch] = eval(\n",
    "        epoch, model=model, data_loader=dl_val, device=device, writer=writer)\n",
    "    if not epoch % 10:\n",
    "        print(msg_eval_epoch.format(epoch, *evaluation[epoch].values()))\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first time, it timesout, second time it starts, see https://github.com/tensorflow/tensorboard/issues/2481#issuecomment-516819768\n",
    "%tensorboard --logdir {logdir} --host localhost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter comparison\n",
    "\n",
    "- [x] order data by date: consecutive samples from training to validation\n",
    "- [ ] stratify data based on machine and column length (from filename)\n",
    "- [ ] compare performance regarding data normalization\n",
    "    - in original intensity space (non-log-transformed)\n",
    "- [ ] compare performance regarding several hyperparameters of VAE (layers, activation, etc)\n",
    "    - plot different losses in one plot as validation data set is the same\n",
    "- [ ] increase number of samples in training set and create result plot\n",
    "\n",
    "\n",
    "- Current optimum for comparision is zero\n",
    "\n",
    "> The comparison where relatively low abundant, but not super low-abundant peptides will be masked, could skew the comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vaep",
   "language": "python",
   "name": "vaep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
