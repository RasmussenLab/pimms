{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADD_TENSORBOARD = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.nb_imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figures = {}  # collection of ax or figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from src.logging import setup_logger\n",
    "\n",
    "logger = setup_logger(logger=logging.getLogger('vaep'))\n",
    "logger.info(\"Experiment 01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "- 1000 features (most abundant peptides)\n",
    "- later a subset of samples is selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES_TO_LOAD = None\n",
    "FN_PEPTIDE_INTENSITIES = config.FOLDER_DATA / 'df_intensities_N07813_M01000'\n",
    "FN_PEPTIDE_INTENSITIES = config.FOLDER_DATA / 'df_intensities_N_00090_M01000'\n",
    "\n",
    "analysis = AnalyzePeptides(\n",
    "    fname=FN_PEPTIDE_INTENSITIES, nrows=N_SAMPLES_TO_LOAD)\n",
    "analysis.df = analysis.df.sort_index()  # sort by date\n",
    "assert analysis.df.index.is_unique, \"Non-unique training samples\"\n",
    "analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select consecutives samples for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import log2\n",
    "import random\n",
    "from vaep.utils import sample_iterable\n",
    "\n",
    "N_SAMPLES = min(len(analysis.df), 1000)\n",
    "logger.info(f\"Selected {N_SAMPLES}\")\n",
    "analysis.N_SAMPLES = N_SAMPLES\n",
    "\n",
    "M = 10\n",
    "\n",
    "columns_selected = sorted(sample_iterable(list(analysis.df.columns), n=M))\n",
    "analysis.df = analysis.df.loc[:, columns_selected]\n",
    "\n",
    "\n",
    "def get_consecutive_data_indices(index, n_samples=N_SAMPLES):\n",
    "    start_sample = len(index) - n_samples\n",
    "    start_sample = random.randint(0, start_sample)\n",
    "    return index[start_sample:start_sample+n_samples]\n",
    "\n",
    "\n",
    "indices_selected = get_consecutive_data_indices(analysis.df.index)\n",
    "analysis.samples = indices_selected\n",
    "analysis.df = analysis.df.loc[indices_selected]\n",
    "\n",
    "LOG_TRANSFORM = log2  # None\n",
    "if LOG_TRANSFORM:\n",
    "    analysis.log_transform(LOG_TRANSFORM)\n",
    "\n",
    "FRACTION = 0.8\n",
    "\n",
    "\n",
    "class Indices(SimpleNamespace):\n",
    "    pass\n",
    "\n",
    "\n",
    "indices = Indices()\n",
    "indices.train, indices.valid = indices_selected[:int(\n",
    "    FRACTION*N_SAMPLES)], indices_selected[int(FRACTION*N_SAMPLES):]\n",
    "analysis.indices = indices\n",
    "\n",
    "analysis.df_train = analysis.df.loc[indices.train]\n",
    "analysis.df_valid = analysis.df.loc[indices.valid]\n",
    "\n",
    "#rebuild original with multi-index\n",
    "analysis.df_by_split = pd.concat((analysis.df_train, analysis.df_valid), keys=['train', 'valid'])\n",
    "\n",
    "analysis.df_by_split.sample(n=10, axis=0).sample(n=5, axis=1).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_features = analysis.df.shape\n",
    "msg = \"Total:\\nN samples: {:10,d} - N Peptides: {:10,d}\\n\".format(\n",
    "    n_samples, n_features)\n",
    "n_train, n_valid = len(analysis.df_train), len(analysis.df_valid)\n",
    "msg += \"N train set: {:8,d} - N valid set: {:9,d}\".format(n_train, n_valid)\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.get_dectection_limit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create meta data from filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import metadata\n",
    "\n",
    "data_meta = metadata.get_metadata_from_filenames(indices_selected)\n",
    "analysis.df_meta = pd.DataFrame.from_dict(\n",
    "    data_meta, orient='index')\n",
    "# analysis.df_meta['date'] = pd.to_datetime(analysis.df_meta['date'])\n",
    "analysis.df_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- possibility to group data in time along `(machine, lc)` pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.df_meta.loc[indices.train].describe(datetime_is_numeric=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This becomes part of analysis\n",
    "def compare_meta_data_for_splits(meta, indices):\n",
    "\n",
    "    _indices = vars(indices)\n",
    "    logger.info('Found vars: {}'.format(', '.join(str(x)\n",
    "                                                  for x in _indices.keys())))\n",
    "\n",
    "    for key_split, split in _indices.items():\n",
    "        print(f\"{key_split:8} - split description:\")\n",
    "        display(\n",
    "            meta.loc[split].describe(datetime_is_numeric=True)\n",
    "        )\n",
    "\n",
    "    _meta_features = list(meta.columns)\n",
    "\n",
    "    for _column in _meta_features:\n",
    "        display(\n",
    "            _=pd.DataFrame({\n",
    "                key_split: meta.loc[split, _column].value_counts(normalize=True) for key_split, split in _indices.items()\n",
    "            }).sort_index().plot(kind='line', rot=90, figsize=(10, 5), title=f\"{_column} value Counts for different splits\")\n",
    "        )\n",
    "\n",
    "\n",
    "compare_meta_data_for_splits(analysis.df_meta.iloc[:, :2], indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA plot of original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(analysis.df)\n",
    "pca = analyzers.run_pca(df=scaler.transform(analysis.df_by_split, copy=None))\n",
    "cols = list(pca.columns)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(15, 8))\n",
    "\n",
    "# by split\n",
    "ax = axes[0]\n",
    "ax = pca.loc['train'].plot.scatter(\n",
    "    x=cols[0], y=cols[1], color='blue', label='train', ax=ax)\n",
    "ax = pca.loc['valid'].plot.scatter(\n",
    "    x=cols[0], y=cols[1], color='orange', label='valid', ax=ax)\n",
    "\n",
    "# by dates\n",
    "ax = axes[1]\n",
    "ax = analyzers.scatter_plot_w_dates(ax, pca, dates=analysis.df_meta.date)\n",
    "\n",
    "# add colorbar for dates\n",
    "ax = analyzers.add_date_colorbar(ax=ax, fig=fig)\n",
    "\n",
    "figures[('pca', 'original')] = fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x] color sample by date (heatmap?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis state so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzers.corr_lower_triangle(analysis.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Helper function and results dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.results = {}\n",
    "\n",
    "\n",
    "def describe_abs_diff(y_true: pd.DataFrame, y_pred: pd.DataFrame):\n",
    "    _abs_diff = y_true - y_pred\n",
    "    return _abs_diff.abs().describe().to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline supervised RF models\n",
    "\n",
    "- M RandomForest baseline models, each predicting one feature based on the M-1 other features\n",
    "- get an idea of a possible baseline performance\n",
    "    - could be used together with imputation of inputs\n",
    "    - with some effort this could be scaled to predict only missing peptides\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "metrics = {}\n",
    "\n",
    "\n",
    "peptides = list(analysis.df_train.columns)\n",
    "metrics = {}\n",
    "pred_valid = {}\n",
    "\n",
    "for i in range(M):\n",
    "    train_columns = list(range(M))\n",
    "    test_column = i\n",
    "    train_columns.remove(i)\n",
    "    train_columns = [peptides[i] for i in train_columns]\n",
    "    test_column = peptides[test_column]\n",
    "    logger.debug(\n",
    "        f\"Train columns: {', '.join(train_columns)}\\nTest column: {test_column}\")\n",
    "    _df_train, _y_train = analysis.df_train[train_columns], analysis.df_train[test_column]\n",
    "    _df_valid, _y_valid = analysis.df_valid[train_columns], analysis.df_valid[test_column]\n",
    "    rf_reg = RandomForestRegressor()\n",
    "    rf_reg.fit(X=_df_train, y=_y_train)\n",
    "    # metrics\n",
    "    _metrics = {}\n",
    "    _metrics[('MSE', 'train')] = mean_squared_error(\n",
    "        y_true=_y_train, y_pred=rf_reg.predict(_df_train))\n",
    "    y_pred_valid = rf_reg.predict(_df_valid)\n",
    "    _metrics[('MSE', 'valid')] = mean_squared_error(\n",
    "        y_true=_y_valid, y_pred=y_pred_valid)\n",
    "    metrics[test_column] = _metrics\n",
    "    # predictions\n",
    "    pred_valid[test_column] = y_pred_valid\n",
    "pd.DataFrame(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.pred_rf = pd.DataFrame(pred_valid, index=analysis.df_valid.index)\n",
    "analysis.pred_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfits to training data as it should."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.df_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.results['RF baseline'] = describe_abs_diff(\n",
    "    y_true=analysis.df_valid, y_pred=analysis.pred_rf)\n",
    "pd.DataFrame(analysis.results['RF baseline'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could a model help in identifying extraordinar differences in samples? Something to focus on?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vaep.model as vaep_model\n",
    "from vaep.cmd import get_args\n",
    "\n",
    "BATCH_SIZE, EPOCHS = 8, 30\n",
    "args = get_args(batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "                no_cuda=True)  # data transfer to GPU seems slow\n",
    "kwargs = {'num_workers': 2, 'pin_memory': True} if args.cuda else {}\n",
    "\n",
    "# torch.manual_seed(args.seed)\n",
    "device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "device\n",
    "\n",
    "print(args, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple AE\n",
    "- should also heavily overfit the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaep.transform import ShiftedStandardScaler\n",
    "\n",
    "args_ae = {}\n",
    "args_ae['SCALER'] = StandardScaler\n",
    "args_ae['SCALER'] = ShiftedStandardScaler\n",
    "\n",
    "# select initial data: transformed vs not log transformed\n",
    "scaler = args_ae['SCALER'](scale_var=2).fit(analysis.df_train)\n",
    "# five examples from validation dataset\n",
    "scaler.transform(analysis.df_train).describe(percentiles=[0.025, 0.975])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.transform(analysis.df_valid).describe(percentiles=[0.025, 0.975])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from vaep.io.datasets import PeptideDatasetInMemoryNoMissings\n",
    "\n",
    "# ToDo: replace with helper class (see below)\n",
    "tf_norm = None  # replace with Normalizer\n",
    "\n",
    "dataset_train = PeptideDatasetInMemoryNoMissings(\n",
    "    data=scaler.transform(analysis.df_train), transform=tf_norm)\n",
    "dataset_valid = PeptideDatasetInMemoryNoMissings(\n",
    "    data=scaler.transform(analysis.df_valid), transform=tf_norm)\n",
    "dl_train = DataLoader(dataset_train, batch_size=args.batch_size, shuffle=True)\n",
    "dl_valid = DataLoader(dataset_valid, batch_size=args.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vaep_model.Autoencoder(n_features=M, n_neurons=int(\n",
    "    M/2), last_decoder_activation=None, dim_latent=3).double()\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    ")\n",
    "\n",
    "# Train standard autoencoder (AE)\n",
    "\n",
    "train_losses, valid_losses = [], []\n",
    "\n",
    "# do = nn.Dropout()  # for denoising AE\n",
    "for epoch in range(args.epochs):\n",
    "    # ===================train==========================\n",
    "    for data in dl_train:\n",
    "        model.train()\n",
    "        data = data.to(device)\n",
    "        # noise = do(torch.ones(data.shape)).to(device) # for denoising AE\n",
    "        # data_corrupted = (data * noise).to(device)    # for denoising AE\n",
    "        # ===================forward=====================\n",
    "        output = model(data)\n",
    "        loss = criterion(output, data)\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "    # ===================validate========================\n",
    "    for data in dl_valid:\n",
    "        model.eval()\n",
    "        data = data.to(device)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, data)\n",
    "        valid_losses.append(loss.item())\n",
    "\n",
    "    # ===================log=============================\n",
    "    print(f'epoch [{epoch + 1:03d}/{args.epochs}], '\n",
    "          f'train-loss: {np.mean(train_losses[-len(dl_train):]):.4f},'\n",
    "          f'valid-loss: {np.mean(valid_losses[-len(dl_valid):]):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_losses = vaep_model.process_train_loss({'MSE train': train_losses})\n",
    "\n",
    "# Plotting is boilerplate code:\n",
    "_ = df_train_losses.plot(kind='scatter', x='steps', y='MSE train smoothed', figsize=(\n",
    "    15, 8),  title='Exponential smoothed training loss', ylim=(0, None))\n",
    "df_train_losses.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(model, dataloader):\n",
    "    pred = []\n",
    "    model.eval()\n",
    "    for data in dataloader:\n",
    "        data = data.to(device)\n",
    "        output = model(data)\n",
    "        pred.append(output.detach().numpy())\n",
    "    return pred\n",
    "\n",
    "\n",
    "pred = get_pred(model, dl_valid)\n",
    "analysis.pred_aa_simple = vaep_model.build_df_from_pred_batches(\n",
    "    pred, scaler, index=analysis.df_valid.index, columns=analysis.df_valid.columns)\n",
    "analysis.pred_aa_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(analysis.df_valid)  # true values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.results['Simple AE'] = describe_abs_diff(\n",
    "    y_true=analysis.df_valid, y_pred=analysis.pred_aa_simple)\n",
    "pd.DataFrame(analysis.results['Simple AE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With noise: Denoising AE\n",
    "\n",
    "- noise is added during training: some values are set to zero (which is the center for standard normalized intensities)\n",
    "- noise model could be adapted to reflect the observed noise in the training data - > extrapolation to near future should hold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vaep_model.Autoencoder(n_features=M, n_neurons=int(\n",
    "    M/2), last_decoder_activation=None, dim_latent=3).double()\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    ")\n",
    "\n",
    "# Train denoising autoencoder (AE)\n",
    "\n",
    "train_losses, valid_losses = [], []\n",
    "\n",
    "do = torch.nn.Dropout()  # for denoising AE\n",
    "for epoch in range(args.epochs):\n",
    "    # ===================train==========================\n",
    "    for data in dl_train:\n",
    "        model.train()\n",
    "        data = data.to(device)\n",
    "        noise = do(torch.ones(data.shape)).to(device)  # for denoising AE\n",
    "        data_corrupted = (data * noise).to(device)    # for denoising AE\n",
    "        # ===================forward=====================\n",
    "        output = model(data)\n",
    "        loss = criterion(output, data)\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "    # ===================validate========================\n",
    "    for data in dl_valid:\n",
    "        model.eval()\n",
    "        data = data.to(device)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, data)\n",
    "        valid_losses.append(loss.item())\n",
    "\n",
    "    # ===================log=============================\n",
    "    print(f'epoch [{epoch + 1:03d}/{args.epochs}], '\n",
    "          f'train-loss: {np.mean(train_losses[-len(dl_train):]):.4f},'\n",
    "          f'valid-loss: {np.mean(valid_losses[-len(dl_valid):]):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_losses = vaep_model.process_train_loss({'MSE train': train_losses})\n",
    "\n",
    "# Plotting is boilerplate code:\n",
    "_ = df_train_losses.plot(kind='scatter', x='steps', y='MSE train smoothed', figsize=(\n",
    "    15, 8),  title='Exponential smoothed training loss', ylim=(0, None))\n",
    "df_train_losses.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = get_pred(model, dl_valid)\n",
    "analysis.pred_aa_denoised = vaep_model.build_df_from_pred_batches(\n",
    "    pred, scaler, index=analysis.df_valid.index, columns=analysis.df_valid.columns)\n",
    "analysis.results['denoising AE'] = describe_abs_diff(\n",
    "    y_true=analysis.df_valid, y_pred=analysis.pred_aa_denoised)\n",
    "pd.DataFrame(analysis.results['denoising AE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering setup\n",
    "\n",
    "Components\n",
    "- each sample has an embedding vector and an intercept\n",
    "- each peptide has an embedding vector and an intercept\n",
    "- scalar product of embeddings yields predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.collab import CollabDataLoaders, MSELossFlat, Learner\n",
    "from types import SimpleNamespace\n",
    "# data format\n",
    "\n",
    "analysis.collab = Analysis()\n",
    "collab = analysis.collab\n",
    "collab.columns = 'peptide,Sample ID,intensity'.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.collab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collab.df = analysis.df.unstack().reset_index(drop=False).rename(\n",
    "    columns={'level_0': 'peptide', 0: 'intensity'})\n",
    "collab.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = CollabDataLoaders.from_df(\n",
    "    collab.df, user_name='Sample ID', item_name='peptide', rating_name='intensity', bs=64)\n",
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dls.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai.torch_core\n",
    "device = torch.device('cpu')\n",
    "fastai.torch_core.defaults.device = torch.device('cpu')\n",
    "\n",
    "\n",
    "collab.model_args = {}\n",
    "collab.model_args['n_samples'] = len(dls.classes['Sample ID'])\n",
    "collab.model_args['n_peptides'] = len(dls.classes['peptide'])\n",
    "collab.model_args['dim_latent_factors'] = 5\n",
    "collab.model_args['y_range'] = (\n",
    "    int(collab.df['intensity'].min()), int(collab.df['intensity'].max())+1)\n",
    "\n",
    "collab.model_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vaep_model.DotProductBias(**collab.model_args)\n",
    "learn = Learner(dls, model, loss_func=MSELossFlat())\n",
    "learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(args.epochs, 5e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this shows it along the mini-batches, no easy customization\n",
    "learn.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.recorder.plot_loss??\n",
    "import matplotlib.pyplot as plt\n",
    "from fastcore.foundation import L\n",
    "\n",
    "\n",
    "def plot_loss(self, skip_start=5, with_valid=True, ax=None):\n",
    "    if not ax:\n",
    "        fig, ax = plt.subplots()\n",
    "    ax.plot(list(range(skip_start, len(self.losses))),\n",
    "            self.losses[skip_start:], label='train')\n",
    "    if with_valid:\n",
    "        idx = (np.array(self.iters) < skip_start).sum()\n",
    "        ax.plot(self.iters[idx:], L(\n",
    "            self.values[idx:]).itemgot(1), label='valid')\n",
    "        ax.legend()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "plot_loss(learn.recorder, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch values\n",
    "for x in learn.recorder.values:\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_preds is overloaded, but hardly documented https://docs.fast.ai/learner.html#Learner.get_preds\n",
    "encodings, pred, target = learn.get_preds(\n",
    "    with_input=True)  # per default validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis concept changes. Here only the (masked) missing peptides could be assessed - without the having entire samples as validation cohorts. Although there is no need for a complete sample, one needs at least some information of a sample to train the sample embedding, leading to a change in the setup.\n",
    "\n",
    " - Collaborative Filtering can be trained on all available data to infer the missing peptides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame([{'Sample ID': dls.classes['Sample ID'][obs[0]], 'peptide': dls.classes['peptide']\n",
    "                         [obs[1]], 'intensity': pred_intensity.item()} for obs, pred_intensity in zip(encodings, pred)])\n",
    "pred_df = pred_df.pivot(index='Sample ID', columns='peptide')\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch Model used directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dl = learn.dls.valid\n",
    "model.to(device)\n",
    "for X, target in valid_dl:\n",
    "    print(learn.model(X[:1]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Switching from DotProduct to FNN based on embeddings as implemented in fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.collab import collab_learner\n",
    "from fastai.collab import get_emb_sz\n",
    "# get_emb_sz??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_emb_sz(dls)  # default embedding sizes based on dataloader for NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collab.model_args  # from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.collab import collab_learner\n",
    "\n",
    "learn = collab_learner(\n",
    "    dls, use_nn=True, y_range=collab.model_args['y_range'], layers=[20, 10])\n",
    "learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(30, 5e-3, wd=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform: Non-log transformed data (Single run)\n",
    "\n",
    "Scale samples according to training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "args_vae = {}\n",
    "args_vae['SCALER'] = MinMaxScaler\n",
    "# select initial data: transformed vs not log transformed\n",
    "scaler = args_vae['SCALER']().fit(analysis.df_train)\n",
    "scaler.transform(analysis.df_valid.iloc[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaep.io.datasets import PeptideDatasetInMemoryNoMissings\n",
    "from vaep.io.dataloaders import DataLoadersCreator\n",
    "\n",
    "data_loader_creator = DataLoadersCreator(\n",
    "    df_train=analysis.df_train,\n",
    "    df_valid=analysis.df_valid,\n",
    "    scaler=scaler,\n",
    "    DataSetClass=PeptideDatasetInMemoryNoMissings,\n",
    "    batch_size=args.batch_size)\n",
    "\n",
    "dl_train, dl_valid = data_loader_creator.get_dls(shuffle_train=True)\n",
    "\n",
    "logger.info(\n",
    "    \"N train: {:5,d} \\nN valid: {:5,d}\".format(\n",
    "        len(dl_train.dataset), len(dl_valid.dataset))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "from torch.nn import Sigmoid\n",
    "from vaep.model import VAE\n",
    "\n",
    "n_neurons = 6\n",
    "logger.info(f'Latent layer neurons: {n_neurons}')\n",
    "\n",
    "model = vaep_model.VAE(n_features=n_features,\n",
    "                       n_neurons=n_neurons,\n",
    "                       last_decoder_activation=Sigmoid,\n",
    "                       last_encoder_activation=None,\n",
    "                       dim_latent=4).double()\n",
    "model = model.to(device)\n",
    "\n",
    "logger.info(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "optimizer = optim.Adam(params=model.parameters(),\n",
    "                       lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ADD_TENSORBOARD:\n",
    "    tensorboard_model_namer = TensorboardModelNamer(\n",
    "        prefix_folder='experiment_01')\n",
    "    writer = tensorboard_model_namer.get_writer(1, [n_neurons], 'scaler')\n",
    "    logger.info(f\"Logging to: {writer.get_logdir()}\")\n",
    "\n",
    "    # data, mask = next(iter(dl_train))\n",
    "    # writer.add_image(\n",
    "    #     f'{len(mask)} mask for this batch of samples', mask, dataformats='HW')\n",
    "\n",
    "    data = next(iter(dl_train))\n",
    "    writer.add_image(\n",
    "        f'{len(data)} batch of sampled data (as heatmap)', data, dataformats='HW')\n",
    "\n",
    "    # ToDo: compiler warning: error or tracer error?\n",
    "    writer.add_graph(model, input_to_model=data.to(\n",
    "        device))  # try to add after training?\n",
    "    writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def run_experiment(model, dls, writer, args):\n",
    "    metrics = defaultdict(dict)\n",
    "    metrics_per_batch = defaultdict(list)\n",
    "    dl_train, dl_valid = dls\n",
    "    msg_eval_epoch = \"Validation Set - Epoch: {:3d} - loss: {:7.3f} - mse: {:5.3f} - KLD: {:5.3f}\"\n",
    "\n",
    "    def _append_batch_metrics(batch_metrics_epoch, d_metrics=metrics_per_batch, dataset_name='train'):\n",
    "        \"\"\"Append single batch metrics to global dictionary.\"\"\"\n",
    "        for d in batch_metrics_epoch.values():\n",
    "            for key, value in d.items():\n",
    "                d_metrics[(dataset_name, key)].append(d[key])\n",
    "        return None  # Signal in-place operation\n",
    "\n",
    "    def _agg_metric_per_epoch(batch_metrics_epoch, epoch, d_metrics=metrics, dataset_name='train'):\n",
    "        keys = next(iter(batch_metrics_epoch.values())).keys()\n",
    "        for key in keys:\n",
    "            d_metrics[(dataset_name, key)][epoch] = np.mean([d[key]\n",
    "                                                             for d in batch_metrics_epoch.values()])\n",
    "        return None  # Signal in-place operation\n",
    "\n",
    "    for epoch in range(1, args.epochs+1):\n",
    "        _epoch_metrics = vaep_model.train(model=model, train_loader=dl_train,\n",
    "                                          optimizer=optimizer, device=device)\n",
    "        n_batches = len(dl_train)\n",
    "\n",
    "        _append_batch_metrics(_epoch_metrics)\n",
    "\n",
    "        _agg_metric_per_epoch(_epoch_metrics, epoch)\n",
    "\n",
    "        _epoch_metrics_valid = vaep_model.evaluate(\n",
    "            model=model, data_loader=dl_valid, device=device)\n",
    "        n_batches = len(dl_valid)\n",
    "        _append_batch_metrics(_epoch_metrics_valid, dataset_name='valid')\n",
    "        _agg_metric_per_epoch(_epoch_metrics_valid,\n",
    "                              epoch, dataset_name='valid')\n",
    "\n",
    "        if writer:\n",
    "            writer.add_scalar('avg validation loss',\n",
    "                              _epoch_metric_valid['loss'] / n_batchnes,\n",
    "                              epoch)\n",
    "\n",
    "    return dict(metrics), dict(metrics_per_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics, metrics_per_batch = run_experiment(model=model, dls=(\n",
    "    dl_train, dl_valid), writer=None, args=args)  # decide about format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_losses = vaep_model.process_train_loss(\n",
    "    {'training loss': metrics_per_batch[('train', 'loss')]})\n",
    "\n",
    "# Plotting is boilerplate code:\n",
    "_ = df_train_losses.plot(kind='scatter', x='steps', y='training loss smoothed', figsize=(\n",
    "    15, 8),  title='Exponential smoothed training loss', ylim=(0, None))\n",
    "df_train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_per_batch.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.DEBUG)\n",
    "batch_metrics_last_epoch = vaep_model.train(model=model, train_loader=dl_train,\n",
    "                                            optimizer=optimizer, device=device)\n",
    "pd.DataFrame.from_dict(batch_metrics_last_epoch, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently: No improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame(metrics)\n",
    "_ = metrics.plot(\n",
    "    figsize=(18, 6), xlim=(1, args.epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = [(_split, _metric)\n",
    "            for _split in ['train', 'valid']\n",
    "            for _metric in ['loss']\n",
    "            ]\n",
    "_ = metrics[selected].plot(\n",
    "    figsize=(18, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_epoch_metric_valid, pred = vaep_model.evaluate(\n",
    "    model=model, data_loader=dl_valid, device=device, return_pred=True)\n",
    "# raw predictions\n",
    "pd.DataFrame(np.vstack(pred), index=analysis.df_valid.index,\n",
    "             columns=analysis.df_valid.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integrate label in dataloader\n",
    "analysis.pred_vae = vaep_model.build_df_from_pred_batches(\n",
    "    pred, scaler, index=analysis.df_valid.index, columns=analysis.df_valid.columns)\n",
    "analysis.pred_vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.df_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolute differences between VAE prediction and true values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.results['VAE'] = describe_abs_diff(\n",
    "    y_true=analysis.df_valid, y_pred=analysis.pred_vae)\n",
    "pd.DataFrame(analysis.results['VAE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolute differences in case of mean prediction using **training** data means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(analysis.df_valid - analysis.df_train.mean()).abs().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean and logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train, dl_valid = data_loader_creator.get_dls(\n",
    "    shuffle_train=False)  # to have know the samples\n",
    "\n",
    "latent_space = defaultdict(list)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def _add_pred_to_d(d, k, pred_fct):\n",
    "    _recon, _mu, _logvar = pred_fct(x)\n",
    "    _recon, _mu, _logvar = _recon.detach().numpy(), _mu.detach().numpy(), _logvar.detach().numpy()\n",
    "    d[(k, \"mu\")].append(_mu)\n",
    "    d[(k, \"logvar\")].append(_logvar)\n",
    "    d[(k, \"recon\")].append(_recon)\n",
    "\n",
    "\n",
    "for x in dl_train:\n",
    "    key = 'train'\n",
    "    _add_pred_to_d(d=latent_space, k=key, pred_fct=model)\n",
    "for x in dl_valid:\n",
    "    key = 'valid'\n",
    "    _add_pred_to_d(d=latent_space, k=key, pred_fct=model)\n",
    "\n",
    "# import importlib; importlib.reload(vaep_model)\n",
    "for (split, stat), arrays in latent_space.items():\n",
    "    _index = getattr(analysis.indices, split)\n",
    "    latent_space[(split, stat)] = vaep_model.build_df_from_pred_batches(\n",
    "        pred=arrays, index=_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_space.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA plot of latent means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis.vae.mu\n",
    "vae_mu = pd.concat([\n",
    "    latent_space[('train', 'mu')],\n",
    "    latent_space[('valid', 'mu')]\n",
    "], keys=['train', 'valid'])\n",
    "vae_mu\n",
    "\n",
    "pca = analyzers.run_pca(vae_mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 'train'\n",
    "ax = pca.loc[split].plot.scatter(\n",
    "    x=cols[0], y=cols[1], title='First two PCs of mu', color='blue', label=split)\n",
    "split = 'valid'\n",
    "ax = pca.loc[split].plot.scatter(\n",
    "    x=cols[0], y=cols[1], title='First two PCs of encoding(mu)', color='orange', label=split, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(pca.columns)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(15, 8))\n",
    "\n",
    "# by split\n",
    "ax = axes[0]\n",
    "\n",
    "split = 'train'\n",
    "ax = pca.loc[split].plot.scatter(\n",
    "    x=cols[0], y=cols[1], title='First two PCs of mu', color='blue', label=split, ax=ax)\n",
    "split = 'valid'\n",
    "ax = pca.loc[split].plot.scatter(\n",
    "    x=cols[0], y=cols[1], title='First two PCs of encoding(mu)', color='orange', label=split, ax=ax)\n",
    "\n",
    "\n",
    "def scatter_plot_w_dates(ax, df, dates=None):\n",
    "    \"\"\"plot first vs. second column in DataFrame.\n",
    "    Use dates to color data.\"\"\"\n",
    "\n",
    "    cols = df.columns\n",
    "\n",
    "    if isinstance(dates, str):\n",
    "        dates = df['dates']\n",
    "\n",
    "    ax = ax.scatter(\n",
    "        x=df[cols[0]],\n",
    "        y=df[cols[1]],\n",
    "        c=[mdates.date2num(t) for t in pd.to_datetime(dates)\n",
    "           ] if dates is not None else None\n",
    "    )\n",
    "    return ax\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "def add_date_colorbar(ax, fig):\n",
    "    loc = mdates.AutoDateLocator()\n",
    "    _ = fig.colorbar(ax, ticks=loc,\n",
    "                 format=mdates.AutoDateFormatter(loc))\n",
    "    return ax\n",
    "# by dates\n",
    "ax = axes[1]\n",
    "ax = scatter_plot_w_dates(ax, pca, dates=analysis.df_meta.date)\n",
    "\n",
    "loc = mdates.AutoDateLocator()\n",
    "_ = fig.colorbar(ax, ticks=loc,\n",
    "                 format=mdates.AutoDateFormatter(loc))\n",
    "\n",
    "figures[('pca', 'vae')] = fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to original PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figures[('pca', 'original')] # pca on std-normalized data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance of samples from mean sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidian_distance(df, axis=0):\n",
    "    X = (df - df.mean(axis=axis))**2\n",
    "    axis = 1-axis\n",
    "    X = X.sum(axis=axis)\n",
    "    X = X**0.5\n",
    "    return X\n",
    "    \n",
    "\n",
    "# latent\n",
    "dist = {}\n",
    "dist['vae_mu'] =euclidian_distance(vae_mu)\n",
    "\n",
    "# reconstructed\n",
    "vae_recon = pd.concat([\n",
    "    latent_space[('train', 'recon')],\n",
    "    latent_space[('valid', 'recon')]\n",
    "], keys=['train', 'valid'])\n",
    "\n",
    "dist['vae_recon'] =euclidian_distance(vae_recon)\n",
    "\n",
    "#non-scaled original\n",
    "dist['original'] = euclidian_distance(analysis.df_by_split)\n",
    "\n",
    "#scaled original\n",
    "scaler = StandardScaler().fit(analysis.df_train)\n",
    "X = scaler.transform(analysis.df_by_split)\n",
    "dist['original_normalized'] = euclidian_distance(X)\n",
    "\n",
    "# can different dimensionality be compared directly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "fig, axes = plt.subplots(nrows=len(dist), figsize=(10,7*len(dist)))\n",
    "axes = itertools.chain(axes)\n",
    "\n",
    "for i, (key, _s) in enumerate(dist.items()):\n",
    "    ax = next(axes)\n",
    "    _ = _s.sort_values().plot(rot=90, ax=ax, title=key)\n",
    "_ = fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = pd.DataFrame(dist)#.sort_values('original')\n",
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = dist.sort_values(by='original').plot(rot=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard\n",
    "\n",
    "- can be run from notebook\n",
    "- or in a separate process to inspect currently running training loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ADD_TENSORBOARD:\n",
    "    print(\"Run to see updates: \\n\\n\\ttensorboard \"\n",
    "          f\"--logdir {tensorboard_model_namer.folder.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare metrics on AE and VAE\n",
    "\n",
    "- Collaborative Filtering currently not comparable as setup differs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Choose from list of keys: \",\n",
    "      \", \".join(\n",
    "          list(next(iter(next(iter(analysis.results.values())).values())).keys()))\n",
    "      )\n",
    "_selected_metric = \"50%\"  # median\n",
    "print(\"Currently selected:\", _selected_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Comparison Series\n",
    "# comparison = {(peptide, model_name): stats[_selected_metric] for model_name, description in analysis.results.items() for peptide, stats in description.items()}\n",
    "# pd.Series(comparison).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Comparison as DataFrame\n",
    "comparison = {}\n",
    "for model_name, description in analysis.results.items():\n",
    "    comparison[model_name] = {peptide: stats[_selected_metric]\n",
    "                              for peptide, stats in description.items()}\n",
    "\n",
    "pd.DataFrame(comparison).style.apply(vaep.pandas.highlight_min, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter comparison\n",
    "\n",
    "- [x] order data by date: consecutive samples from training to validation\n",
    "- [x] check stratification based on machine and column length between splits\n",
    "    - Do validation and training data have same proportion of machine types? -> generally no, would need to be added\n",
    "       - not (all) machines are running continously or are continously checked\n",
    "- [x] complete meta data reading based on filenames\n",
    "- [x] compare performance regarding data normalization\n",
    "    - in original intensity space (non-log-transformed) - > \n",
    "- [ ] compare performance regarding several hyperparameters of VAE (layers, activation, etc)\n",
    "    - plot different losses in one plot as validation data set is the same\n",
    "- [ ] increase number of samples in training set and create result plot\n",
    "- [ ] increase the number of peptides (features)\n",
    "- [ ] mask some values in the validation set missing (Quality Assessment)\n",
    "- [ ] write main function which trains an entire model (including data transformations)\n",
    "- [ ] add initial PCA plot with samples. Is there any grouping observable? (plotly express)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debug\n",
    "- [x] Check reporting of loss again: average sample loss or average peptide loss?\n",
    "- [x] take a close look at VAE tutorial of PyTorch (data normalization, etc)\n",
    "- [x] reduce the features size to fewer samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VAE\n",
    "- original inputs between 0 and 1 as decoder outputs are transformed originally using the sigmoid fct\n",
    "- original model use `tanh` activations\n",
    "- think about the definition of `MSE` in a mini-batch. Should be peptide wise?\n",
    "    - VAMB does sum over a sample and then takes the mean of the sum (alternative?)\n",
    "    - multi-output regression?\n",
    "- learning requires active masking: Mask inputs which should be learned to be recovered. Feed original, \n",
    "  not masked image as target to loss.\n",
    "\n",
    "- [ ] Run MNIST example with MSE loss. Does it still work?\n",
    "- [x] Normalize inputs to zero and one, use MNIST VAE. Does it work?\n",
    "  - yes, it learns better then\n",
    "- [x] Regress M peptide intensities on 1 other peptide intensity. Does it work? (Reference performance)\n",
    "    - RF baseline model established\n",
    "- [x] Build a normal AE without probabilistic bottleneck. Does this work?\n",
    "    - yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refactoring\n",
    "\n",
    "- [x] get epoch out of train, eval etc\n",
    "\n",
    "\n",
    "Ideas\n",
    "  - combine 1000 most abundant peptides as guidance for different sets of low abundant peptides\n",
    "  - show the difference between original and reconstruction using a cm in an Image? batch-wise?\n",
    "\n",
    "- Current optimum for comparision is zero\n",
    "\n",
    "> The comparison where relatively low abundant, but not super low-abundant peptides will be masked, could skew the comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writer # new writer\n",
    "# dls = get_dls(data_in_memory, scaler)\n",
    "# model = VAE()\n",
    "# writer =  # new writer for each setup\n",
    "# metrics = run_experiment(model, dls, writer)\n",
    "# overview['experiment_name'] = metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect batches of the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_valid = analysis.df_valid.index\n",
    "index_train = analysis.df_train.index\n",
    "columns_ = analysis.df_train.columns\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training batch example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cpu')\n",
    "iter_dl_train = iter(dl_train)\n",
    "batch = next(iter_dl_train)\n",
    "batch_mask = None\n",
    "try:\n",
    "    batch, batch_mask = batch\n",
    "    batch_masked = batch * batch_mask\n",
    "except ValueError:\n",
    "    batch = batch\n",
    "batch_recon, mu, logvar = model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_batch_metrics = vaep_model.loss_function(batch_recon, batch, mu, logvar)\n",
    "_batch_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if batch_mask:\n",
    "    # avg per peptide loss -> should be close to zero (ref: std=1)\n",
    "    _mse = ((batch * batch_mask) - (batch_recon * batch_mask)).pow(2).sum()\n",
    "else:\n",
    "    _mse = (batch - batch_recon).pow(2).sum()\n",
    "_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "loss = nn.MSELoss(reduction='sum')\n",
    "if batch_mask:\n",
    "    _mse = loss(input=batch_recon*batch_mask, target=batch * batch_mask)\n",
    "else:\n",
    "    _mse = loss(input=batch_recon, target=batch)\n",
    "_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "if batch_mask:\n",
    "    batch_sse = F.mse_loss(input=batch_recon*batch_mask,\n",
    "                           target=batch * batch_mask, reduction='sum')\n",
    "else:\n",
    "    batch_sse = F.mse_loss(input=batch_recon,\n",
    "                           target=batch, reduction='sum')\n",
    "batch_sse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Validation batch example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation data loader is not shuffled\n",
    "N_valid = len(dl_valid.dataset)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "iter_dl_valid = iter(dl_valid)\n",
    "\n",
    "batch = next(iter_dl_valid)\n",
    "batch_mask = None\n",
    "try:\n",
    "    batch, batch_mask = batch\n",
    "    batch_masked = batch * batch_mask\n",
    "except ValueError:\n",
    "    batch = batch\n",
    "\n",
    "batch_recon, mu, logvar = model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_batch_metrics = vaep_model.loss_function(batch_recon, batch, mu, logvar)\n",
    "_batch_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if batch_mask:\n",
    "    # avg per peptide loss -> should be close to zero (ref: std=1)\n",
    "    _mse = ((batch * batch_mask) - (batch_recon * batch_mask)).pow(2).sum()\n",
    "else:\n",
    "    _mse = (batch - batch_recon).pow(2).sum()\n",
    "_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "loss = nn.MSELoss(reduction='sum')\n",
    "if batch_mask:\n",
    "    _mse = loss(input=batch_recon*batch_mask, target=batch * batch_mask)\n",
    "else:\n",
    "    _mse = loss(input=batch_recon, target=batch)\n",
    "_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "if batch_mask:\n",
    "    batch_sse = F.mse_loss(input=batch_recon*batch_mask,\n",
    "                           target=batch * batch_mask, reduction='sum')\n",
    "else:\n",
    "    batch_sse = F.mse_loss(input=batch_recon,\n",
    "                           target=batch, reduction='sum')\n",
    "batch_sse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Validation data\n",
    "\n",
    "- VAMB training epoch normalizes by number of batches, [see](https://github.com/RasmussenLab/vamb/blob/734b741b85296377937de54166b7db274bc7ba9c/vamb/encode.py#L284-L335)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation data loader is not shuffled\n",
    "iter_dl_valid = iter(dl_valid)\n",
    "\n",
    "batch = next(iter_dl_valid)\n",
    "batch_mask = None\n",
    "try:\n",
    "    batch, batch_mask = batch\n",
    "    batch_masked = batch * batch_mask\n",
    "except ValueError:\n",
    "    batch = batch\n",
    "\n",
    "M = batch.shape[-1]\n",
    "batch_recon, _, _ = model(batch)\n",
    "\n",
    "data = batch.detach().numpy()\n",
    "if batch_mask:\n",
    "    mask = batch_mask.detach().numpy()\n",
    "pred = batch_recon.detach().numpy()\n",
    "\n",
    "for batch in iter_dl_valid:\n",
    "    try:\n",
    "        # ToDo: Test if this works\n",
    "        if not type(batch) == torch.Tensor:\n",
    "            batch, batch_mask = batch\n",
    "            batch_masked = batch * batch_mask\n",
    "    except ValueError:\n",
    "        batch = batch\n",
    "    batch_recon, _, _ = model(batch)\n",
    "    data = np.append(data, batch.view([-1, M]), axis=0)\n",
    "\n",
    "    if batch_mask:\n",
    "        mask = np.append(mask, batch_mask, axis=0)\n",
    "    pred = np.append(pred, batch_recon.detach().numpy().reshape(-1, M), axis=0)\n",
    "\n",
    "expected_shape = analysis.df_valid.shape\n",
    "assert data.shape == expected_shape\n",
    "assert pred.shape == expected_shape\n",
    "if batch_mask:\n",
    "    assert mask.shape == expected_shape\n",
    "\n",
    "data = pd.DataFrame(data, index=index_valid,\n",
    "                    columns=columns_).replace(0.0, np.nan)\n",
    "pred = pd.DataFrame(pred, index=index_valid, columns=columns_)\n",
    "mask = pd.DataFrame(mask, index=index_valid,\n",
    "                    columns=columns_) if batch_mask else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    scaler.inverse_transform(pred),\n",
    "    index=index_valid,\n",
    "    columns=columns_\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.iloc[-1]  # mse loss get's most weight in combined loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.iloc[-1].loc[('valid', 'recon_loss')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average prediction error per peptides:\n",
    "\n",
    "-  std. dev is one, so a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that losses reported match loss calculated form predictions\n",
    "((pred - data)**2).sum().sum() / data.notna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pred - data).iloc[:10, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((pred - data).iloc[:10, :5])**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pred - data).notna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, M = data.shape\n",
    "data.isna().sum().sum() / (N*M)  # only few missings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vaep",
   "language": "python",
   "name": "vaep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
