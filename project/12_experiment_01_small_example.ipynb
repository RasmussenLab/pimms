{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from src import config\n",
    "from src.analyzers import *\n",
    "from vaep.transform import StandardScaler, get_df_fitted_mean_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from src.logging import setup_logger\n",
    "\n",
    "logger = logging.getLogger()  # returns root-logger\n",
    "logger.setLevel(logging.CRITICAL)  # silence for everything else\n",
    "logger.handlers = []\n",
    "\n",
    "\n",
    "logger = setup_logger(logger=logging.getLogger('vaep'))\n",
    "logger.info(\"Experiment 01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "- 1000 features (most abundant peptides)\n",
    "- later a subset of samples is selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES_TO_LOAD = None\n",
    "FN_PEPTIDE_INTENSITIES = config.FOLDER_DATA / 'df_intensities_N_07813_M01000'\n",
    "analysis = AnalyzePeptides(\n",
    "    fname=FN_PEPTIDE_INTENSITIES, nrows=N_SAMPLES_TO_LOAD)\n",
    "analysis.df = analysis.df.sort_index()  # sort by date\n",
    "assert analysis.df.index.is_unique, \"Non-unique training samples\"\n",
    "analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select consecutives samples for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "N_SAMPLES = 1000\n",
    "logger.info(f\"Selected {N_SAMPLES}\")\n",
    "analysis.N_SAMPLES = N_SAMPLES\n",
    "\n",
    "\n",
    "def get_consecutive_data_indices(index, n_samples=N_SAMPLES):\n",
    "    start_sample = len(index) - n_samples\n",
    "    start_sample = random.randint(0, start_sample)\n",
    "    return index[start_sample:start_sample+n_samples]\n",
    "\n",
    "\n",
    "indices_selected = get_consecutive_data_indices(analysis.df.index)\n",
    "analysis.samples = indices_selected\n",
    "analysis.df = analysis.df.loc[indices_selected]\n",
    "analysis.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create meta data from filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import re\n",
    "\n",
    "columns = 'date ms_instrument lc_instrument researcher rest'.split()\n",
    "\n",
    "regex_researcher = '[A-Z][a-z][A-Z][a-zA-Z]'\n",
    "\n",
    "\n",
    "\n",
    "assert re.search(regex_researcher, 'HeWe_').group()   == 'HeWe'\n",
    "assert re.search(regex_researcher, '_HeWe_').group()   == 'HeWe'\n",
    "assert re.search(regex_researcher, 'HeWE_').group()   == 'HeWE'\n",
    "assert re.search(regex_researcher, '_HeWE_').group()   == 'HeWE'\n",
    "\n",
    "regex_lc_instrument = '[nN]*((lc)|(LC)|([eE]vo))[a-zA-Z0-9]*'\n",
    "assert re.search(regex_lc_instrument, 'nlc1_').group() == 'nlc1'\n",
    "assert re.search(regex_lc_instrument, 'Evo_').group() == 'Evo'\n",
    "\n",
    "\n",
    "regex_hela = '[Hh]e[Ll]a'\n",
    "assert re.search(regex_hela, 'HeLa').group() == 'HeLa'\n",
    "assert re.search(regex_hela, 'Hela').group() == 'Hela'\n",
    "assert re.search(regex_hela, 'hela').group() == 'hela'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunMetaData = namedtuple('RunMetaData', columns)\n",
    "data_meta = {}\n",
    "for filename in indices_selected:\n",
    "    # ToDo: this approach is too easy for the moment. The first two fields are in order, the rest needs matching.\n",
    "    _entry = {}\n",
    "    _entry['date'], _entry['ms_instrument'], _rest_filename = filename.split('_', maxsplit=2)\n",
    "    try:\n",
    "        _entry['researcher'] = re.search(regex_researcher, _rest_filename).group()\n",
    "        if re.search(regex_hela, _entry['researcher']):\n",
    "            _cleaned_filename = _rest_filename.replace(_entry['researcher'], '').replace('__', '_')\n",
    "            _entry['researcher'] = re.search(regex_researcher, _cleaned_filename).group()\n",
    "        _rest_filename = _rest_filename.replace(_entry['researcher'], '').replace('__', '_')\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            _entry['researcher'] = re.search('[A-Z][a-zA-Z]*[-]*[A-Z][a-zA-Z]*_', _rest_filename).group()[:-1]\n",
    "            logger.debug(f\"Found irregular researcher ID: {_entry['researcher']} (from: {filename})\")\n",
    "            _rest_filename = _rest_filename.replace(_entry['researcher']+'_', '').replace('__', '_')\n",
    "        except AttributeError:\n",
    "            raise\n",
    "    try:\n",
    "        _entry['lc_instrument'] = re.search(regex_lc_instrument, _rest_filename).group()\n",
    "        _rest_filename = _rest_filename.replace(_entry['lc_instrument']+'_', '').replace('__', '_')\n",
    "    except AttributeError:\n",
    "        try: \n",
    "            _entry['lc_instrument'] = re.search('[Bb][Rr][0-9]+', _rest_filename).group()\n",
    "            _rest_filename = _rest_filename.replace(_entry['lc_instrument']+'_', '').replace('__', '_')\n",
    "        except AttributeError:\n",
    "            _entry['lc_instrument'] = None\n",
    "            logger.error(f'Could not find LC instrument in {filename}')\n",
    "            \n",
    "    \n",
    "    _entry['rest'] = _rest_filename\n",
    "    data_meta[filename] = _entry\n",
    "    \n",
    "    # print(RunMetaData(*_meta_filename[:6]))\n",
    "\n",
    "# from pprint import pprint\n",
    "# pprint(data_meta)\n",
    "analysis.df_meta = pd.DataFrame.from_dict(\n",
    "    data_meta, orient='index')\n",
    "# analysis.df_meta['date'] = pd.to_datetime(analysis.df_meta['date'])\n",
    "analysis.df_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRACTION = 0.9\n",
    "\n",
    "\n",
    "class Indices(SimpleNamespace):\n",
    "    pass\n",
    "\n",
    "\n",
    "indices = Indices()\n",
    "indices.train, indices.valid = indices_selected[:int(\n",
    "    FRACTION*N_SAMPLES)], indices_selected[int(FRACTION*N_SAMPLES):]\n",
    "analysis.indices = indices\n",
    "\n",
    "analysis.df_train = analysis.df.loc[indices.train]\n",
    "analysis.df_valid = analysis.df.loc[indices.valid]\n",
    "\n",
    "analysis.df_meta.loc[indices.train].describe(datetime_is_numeric=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This becomes part of analysis\n",
    "def compare_meta_data_for_splits(meta, indices):\n",
    "\n",
    "    _indices = vars(indices)\n",
    "    logger.info('Found vars: {}'.format(', '.join(str(x)\n",
    "                                                  for x in _indices.keys())))\n",
    "\n",
    "    for key_split, split in _indices.items():\n",
    "        print(f\"{key_split:8} - split description:\")\n",
    "        display(\n",
    "            meta.loc[split].describe(datetime_is_numeric=True)\n",
    "        )\n",
    "\n",
    "    _meta_features = list(meta.columns)\n",
    "\n",
    "    for _column in _meta_features:\n",
    "        display(\n",
    "            _=pd.DataFrame({\n",
    "                key_split: meta.loc[split, _column].value_counts(normalize=True) for key_split, split in _indices.items()\n",
    "            }).sort_index().plot(kind='line', rot=90, figsize=(10, 5), title=f\"{_column} value Counts for different splits\")\n",
    "        )\n",
    "\n",
    "\n",
    "compare_meta_data_for_splits(analysis.df_meta.iloc[:, :2], indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Transforms\n",
    "\n",
    "- illustrate using adapted scikit-learn [`StandardScaler`](https://scikit-learn.org/stable/modules/preprocessing.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, M = 10, 10  # Samples, Features\n",
    "analysis.df_train.iloc[:N, :M]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.df_train.iloc[:, :M].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StandardScaler on raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(analysis.df_train)\n",
    "scaler_df = get_df_fitted_mean_std(scaler, index=analysis.df_train.columns)\n",
    "scaler_df.head(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = scaler.transform(analysis.df_train.iloc[:N])\n",
    "sample.iloc[:, :M]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = scaler.inverse_transform(sample)\n",
    "sample.iloc[:, :M]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StandardScaler on log10 transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_log10 = np.log10(analysis.df_train)\n",
    "X_log10.iloc[:N, :M]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_log = StandardScaler(\n",
    ").fit(X=X_log10)\n",
    "scaler_log_df = get_df_fitted_mean_std(scaler_log, index=analysis.df.index)\n",
    "scaler_log_df.head(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_log10 = scaler_log.transform(X_log10.iloc[:N])\n",
    "sample_log10.iloc[:, :M]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_log.inverse_transform(sample_log10).iloc[:, :M]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation\n",
    "\n",
    "- Correlation between the computed `means_` should be nearly perfect\n",
    "- Correlation between peptide intensities should be high\n",
    "- As taking the logarithm is a monoton, but non-linear transformation, the linear Pearson correlation can change substantially. [[link]](https://stats.stackexchange.com/questions/127121/do-logs-modify-the-correlation-between-two-variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Correlation between mean values of linear vs. log-transformed values:\",\n",
    "      f\"{np.corrcoef(scaler.mean_, scaler_log.mean_)[1,0]:.4f}\", sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.3f}'.format\n",
    "\n",
    "analysis.corr_linear_vs_log = scaler.transform(X=analysis.df).corrwith(\n",
    "    other=scaler_log.transform(X_log10),\n",
    "    axis=0)\n",
    "analysis.corr_linear_vs_log.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# own implemention could be slightly faster as data is already demeanded and standardized.\n",
    "# pd.DataFrame.corrwith?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from vaep.utils import sample_iterable\n",
    "\n",
    "columns_sampled = sample_iterable(list(analysis.df.columns), n=12)\n",
    "print(columns_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scaled_sample(columns_sampled: list, scaler, df: pd.DataFrame = analysis.df):\n",
    "    _scaled = scaler.transform(df)\n",
    "    display(_scaled.describe())\n",
    "    _min, _max = _scaled.min().min(), _scaled.max().max()\n",
    "    return _min, _max\n",
    "    print(list(range(_min, _max, step=0.5)))\n",
    "\n",
    "\n",
    "_min, _max = plot_scaled_sample(columns_sampled=columns_sampled, scaler=scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if bins should be equal between plots\n",
    "# addon\n",
    "import math\n",
    "xlim = [-5, 5]\n",
    "FACTOR = 1\n",
    "[x/FACTOR for x in range(math.floor(xlim[0])*FACTOR,\n",
    "                         math.ceil(xlim[1])*FACTOR+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "columns_sampled = sample_iterable(list(analysis.df.columns), n=9)\n",
    "subplot_kw = {'xlim': [-5, 5], 'ylim': [0, 600]}\n",
    "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(\n",
    "    15, 15), sharey=True, sharex=True, subplot_kw=subplot_kw)\n",
    "_ = scaler_log.transform(X_log10)[columns_sampled].hist(\n",
    "    figsize=(15, 15), ax=axes)\n",
    "axes = scaler.transform(analysis.df)[columns_sampled].hist(\n",
    "    figsize=(15, 15), ax=axes)\n",
    "_ = fig.legend(('linear', 'log'), loc=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption = (f\"Frequencies are capped at {subplot_kw['ylim'][1]} and \"\n",
    "           \"their standardized intensity values plotted between {} and {}.\".format(\n",
    "               *subplot_kw['xlim'])\n",
    "           )\n",
    "print(caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis state so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fastai Dataloader\n",
    "\n",
    "> fastai includes a replacement for Pytorch's DataLoader which is largely API-compatible, and adds a lot of useful functionality and flexibility. Before we look at the class, there are a couple of helpers we'll need to define. [[link](https://docs.fast.ai/data.load.html)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fastai.tabular.all as tab\n",
    "from fastcore.transform import Transform\n",
    "\n",
    "from fastai.tabular.data import TabularDataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataloaders using an appropriate factory method from `TabularDataLoaders` class, here [`from_df`](https://docs.fast.ai/tabular.data.html#TabularDataLoaders.from_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame is shuffled\n",
    "N_VAL = 100\n",
    "valid_idx = list(range(N_VAL))\n",
    "dls = TabularDataLoaders.from_df(df=analysis.df, valid_idx=valid_idx, bs=64)\n",
    "analysis.dls = dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.show_batch()  # loses object index attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.valid.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dls.train:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize(Transform):\n",
    "    def setup(self, array):\n",
    "        self.mean = array.mean()  # this assumes tensor, numpy arrays and alike\n",
    "        # should be applied along axis 0 (over the samples)\n",
    "        self.std = array.std()  # ddof=0 in scikit-learn\n",
    "\n",
    "    def encodes(self, x):\n",
    "        x_enc = (x - self.mean) / self.std\n",
    "        return x_enc\n",
    "\n",
    "    def decodes(self, x_enc):\n",
    "        x = (self.std * x_enc) + self.mean\n",
    "        return x\n",
    "\n",
    "\n",
    "tf_norm = Normalize()\n",
    "tf_norm.setup(analysis.df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare results to scikit learn implementation of [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html).\n",
    "\n",
    "Differences seem to arrive due to iterative computation of mean and standard-deviation in scikit-learn, see [`_incremental_mean_and_var`](https://github.com/scikit-learn/scikit-learn/blob/15a949460dbf19e5e196b8ef48f9712b72a3b3c3/sklearn/utils/extmath.py#L792)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 5\n",
    "\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        ('Transform', 'mean'): tf_norm.mean[:M],\n",
    "        ('Transform', 'std'): tf_norm.std[:M],\n",
    "        ('StandardScaler', 'mean'): scaler.mean_[:M],\n",
    "        ('StandardScaler', 'std'): scaler.scale_[:M]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "tf_norm(analysis.df_train.iloc[:N]).iloc[:, :M]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.transform(analysis.df_train.iloc[:N]).iloc[:, :M]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now the `loss_func` signature and the `NN_Module` forward path have to be adapted. Unsure how to do this in plain PyTorch yet. So we only use the dataloader for now.\n",
    "\n",
    "- Callback needed to set `xb` to `yb`, see [callback-attributes](https://docs.fast.ai/callback.core.html#Attributes-available-to-callbacks) and [example](https://github.com/dhuynh95/fastai_autoencoder/blob/bc357927f26273d676dca9a41018411408b97430/fastai_autoencoder/callback.py#L16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_function(recon_x=recon_batch, x=batch, mask=mask, mu=mu, logvar=logvar)\n",
    "# learn = Learner(dls, NN_Module, opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# import importlib; importlib.reload(vaep.model)\n",
    "from vaep.model import train\n",
    "from vaep.model import VAE\n",
    "from vaep.model import loss_function\n",
    "from vaep.cmd import get_args\n",
    "\n",
    "from vaep.io.datasets import PeptideDatasetInMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = get_args(no_cuda=True)\n",
    "kwargs = {'num_workers': 2, 'pin_memory': True} if args.cuda else {}\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "# EpochAverages = namedtuple(\"EpochAverages\", 'loss mse kld')\n",
    "\n",
    "\n",
    "def eval(epoch, model, data_loader, device, writer=None):\n",
    "    model.eval()\n",
    "    metrics = {'loss': 0, 'mse': 0,  'kld': 0}\n",
    "\n",
    "    for batch, mask in data_loader:\n",
    "        recon_batch, mu, logvar = model(batch)\n",
    "        loss, mse, kld = loss_function(\n",
    "            recon_x=recon_batch, x=batch, mask=mask, mu=mu, logvar=logvar)\n",
    "        metrics['loss'] += loss.item()\n",
    "        metrics['mse'] += mse.item()\n",
    "        metrics['kld'] += kld.item()\n",
    "    if writer is not None:\n",
    "        n_samples = len(data_loader.dataset)\n",
    "        writer.add_scalar('avg validation loss',\n",
    "                          metrics['loss'] / n_samples,\n",
    "                          epoch)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_limit = np.log10(analysis.df).min().min()  # all zeros become nan.\n",
    "\"Detection limit: {:6.3f}, corresponding to intensity value of {:,d}\".format(\n",
    "    detection_limit,\n",
    "    int(10 ** detection_limit)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_features = analysis.df.shape\n",
    "\"N samples: {:10,d} - N Peptides: {:10,d}\".format(n_samples, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random sample is drawn\n",
    "PROPORTION_TRAIN = 0.9\n",
    "\n",
    "# could be a method on analysis\n",
    "analysis.indices_train = analysis.df.sample(\n",
    "    int(n_samples*PROPORTION_TRAIN)\n",
    ").index\n",
    "analysis.indices_val = analysis.df.index.difference(analysis.indices_train)\n",
    "\n",
    "analysis.df_train = analysis.df.loc[analysis.indices_train]\n",
    "analysis.df_valid = analysis.df.loc[analysis.indices_val]\n",
    "\n",
    "len(analysis.indices_val), analysis.indices_val[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-log transformed data (Single run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale samples according to training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select initial data: transformed vs not log transformed\n",
    "scaler = scaler.fit(analysis.df_train)\n",
    "# five examples from validation dataset\n",
    "scaler.transform(analysis.df_valid.iloc[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(df_train, df_valid, scaler):\n",
    "    data_train = PeptideDatasetInMemory(\n",
    "        data=scaler.transform(df_train))\n",
    "    data_valid = PeptideDatasetInMemory(data=scaler.transform(df_valid))\n",
    "\n",
    "    dl_train = torch.utils.data.DataLoader(\n",
    "        dataset=data_train,\n",
    "        batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    dl_valid = torch.utils.data.DataLoader(\n",
    "        dataset=data_valid,\n",
    "        batch_size=args.batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "    return dl_train, dl_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorboardModelNamer():\n",
    "    def __init__(self, prefix_folder, root_dir=Path('runs')):\n",
    "        self.prefix_folder = prefix_folder\n",
    "        self.root_logdir = Path(root_dir)\n",
    "        self.folder = (self.root_logdir /\n",
    "                       f'{self.prefix_folder}_{format(datetime.now(), \"%y%m%d_%H%M\")}')\n",
    "\n",
    "    def get_model_name(self, hidden_layers: int,\n",
    "                       neurons: list,\n",
    "                       scaler: str,\n",
    "                       ):\n",
    "        name = 'model_'\n",
    "        name += f'hl{hidden_layers:02d}'\n",
    "\n",
    "        if type(neurons) == str:\n",
    "            neurons = neurons.split()\n",
    "        elif not type(neurons) in [list, tuple]:\n",
    "            raise TypeError(\n",
    "                \"Provide expected format for neurons: [12, 13, 14], '12 13 14' or '12_13_14'\")\n",
    "\n",
    "        for x in neurons:\n",
    "            name += f'_{x}'\n",
    "\n",
    "        if type(scaler) == str:\n",
    "            name += f'_{scaler}'\n",
    "        else:\n",
    "            name += f'_{scaler!r}'\n",
    "        return name\n",
    "\n",
    "    def get_writer(self, hidden_layers: int,\n",
    "                   neurons: list,\n",
    "                   scaler: str,\n",
    "                   ):\n",
    "        model_name = self.get_model_name(hidden_layers=hidden_layers,\n",
    "                                         neurons=neurons,\n",
    "                                         scaler=scaler)\n",
    "        return SummaryWriter(log_dir=self.folder / model_name)\n",
    "\n",
    "\n",
    "expected = 'model_hl01_12_13_14_scaler'\n",
    "\n",
    "tensorboard_model_namer = TensorboardModelNamer(prefix_folder='experiment_01')\n",
    "\n",
    "assert tensorboard_model_namer.get_model_name(\n",
    "    hidden_layers=1, neurons=[12, 13, 14], scaler='scaler') == expected\n",
    "assert tensorboard_model_namer.get_model_name(\n",
    "    hidden_layers=1, neurons='12 13 14', scaler='scaler') == expected\n",
    "assert tensorboard_model_namer.get_model_name(\n",
    "    hidden_layers=1, neurons='12_13_14', scaler='scaler') == expected\n",
    "assert tensorboard_model_namer.get_model_name(\n",
    "    hidden_layers=1, neurons='12_13_14', scaler=scaler) == 'model_hl01_12_13_14_StandardScaler()'\n",
    "# assert get_writer(hidden_layers=1, neurons=1, scaler=scaler) == TypeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neurons = max(30, int(n_features/6))\n",
    "logger.info(f'Latent layer neurons: {n_neurons}')\n",
    "\n",
    "writer = tensorboard_model_namer.get_writer(1, [n_neurons], 'scaler')\n",
    "logger.info(f\"Logging to: {writer.get_logdir()}\")\n",
    "\n",
    "\n",
    "dl_train, dl_valid = get_dataloaders(\n",
    "    df_train=analysis.df_train,\n",
    "    df_valid=analysis.df_valid,\n",
    "    scaler=scaler)\n",
    "\n",
    "logger.info(\n",
    "    \"N train: {:5,d} \\nN valid: {:5,d}\".format(\n",
    "        len(dl_train.dataset), len(dl_valid.dataset))\n",
    ")\n",
    "\n",
    "data, mask = next(iter(dl_train))\n",
    "\n",
    "writer.add_image(\n",
    "    f'{len(data)} batch of sampled data (as heatmap)', data, dataformats='HW')\n",
    "writer.add_image(\n",
    "    f'{len(mask)} mask for this batch of samples', mask, dataformats='HW')\n",
    "\n",
    "\n",
    "model = VAE(n_features=n_features, n_neurons=n_neurons)\n",
    "\n",
    "logger.info(model)\n",
    "# model = model.to(device, non_blocking=True)\n",
    "\n",
    "# ToDo: compiler warning: error or tracer error?\n",
    "writer.add_graph(model, input_to_model=data)  # try to add after training?\n",
    "writer.flush()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def run_experiment(model, dls, writer, args):\n",
    "    metrics = defaultdict(dict)\n",
    "    dl_train, dl_valid = dls\n",
    "    msg_eval_epoch = \"Validation Set - Epoch: {:3d} - loss: {:7.3f} - mse: {:5.3f} - KLD: {:5.3f}\"\n",
    "\n",
    "    for epoch in range(1, args.epochs):\n",
    "        metrics[('train', 'loss')][epoch] = float(train(epoch, model=model, train_loader=dl_train,\n",
    "                                                        optimizer=optimizer, device=device, writer=writer))\n",
    "        # ToDo: Pull out writer from eval function\n",
    "        _epoch_metric_valid = eval(\n",
    "            epoch, model=model, data_loader=dl_valid, device=device, writer=writer)\n",
    "        metrics[('valid', 'loss')][epoch] = _epoch_metric_valid['loss']\n",
    "        metrics[('valid', 'mse')][epoch] = _epoch_metric_valid['mse']\n",
    "        metrics[('valid', 'kld')][epoch] = _epoch_metric_valid['kld']\n",
    "        if not epoch % 10:\n",
    "            logger.info(msg_eval_epoch.format(\n",
    "                epoch, *_epoch_metric_valid.values()))\n",
    "    writer.flush()\n",
    "    writer.close()  # closes all internal writers of SummaryWriter\n",
    "    return metrics\n",
    "\n",
    "\n",
    "args.epochs = 200\n",
    "metrics = run_experiment(model=model, dls=(\n",
    "    dl_train, dl_valid), writer=writer, args=args)  # decide about format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame(metrics)\n",
    "_ = metrics.plot(\n",
    "    figsize=(18, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics[('valid', 'norm_loss')] = metrics[(\n",
    "    'valid', 'loss')] / len(dl_valid.dataset)\n",
    "metrics[('train', 'norm_loss')] = metrics[(\n",
    "    'train', 'loss')] / len(dl_train.dataset)\n",
    "\n",
    "selected = [(_split, _metric)\n",
    "            for _split in ['train', 'valid']\n",
    "            for _metric in ['norm_loss']\n",
    "            ]\n",
    "_ = metrics[selected].plot(\n",
    "    figsize=(18, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log transformed data (Single run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.df_train_log10 = np.log10(analysis.df_train)\n",
    "analysis.df_valid_log10 = np.log10(analysis.df_valid)\n",
    "scaler_log = StandardScaler().fit(X=analysis.df_train_log10)\n",
    "# five examples from validation dataset\n",
    "scaler_log.transform(analysis.df_valid_log10.iloc[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_neurons = max(30, int(n_features/6))\n",
    "logger.info(f'Latent layer neurons: {n_neurons}')\n",
    "\n",
    "writer = tensorboard_model_namer.get_writer(1, [n_neurons], 'scaler_log')\n",
    "logger.info(f\"Logging to: {writer.get_logdir()}\")\n",
    "\n",
    "\n",
    "dl_train, dl_valid = get_dataloaders(df_train=analysis.df_train_log10, df_valid=analysis.df_valid_log10, scaler=scaler_log)\n",
    "\n",
    "logger.info(\n",
    "    \"N train: {:5,d} \\nN valid: {:5,d}\".format(\n",
    "        len(dl_train.dataset), len(dl_valid.dataset))\n",
    ")\n",
    "\n",
    "data, mask = next(iter(dl_train))\n",
    "\n",
    "writer.add_image(\n",
    "    f'{len(data)} batch of sampled data (as heatmap)', data, dataformats='HW')\n",
    "writer.add_image(\n",
    "    f'{len(mask)} mask for this batch of samples', mask, dataformats='HW')\n",
    "\n",
    "\n",
    "model = VAE(n_features=n_features, n_neurons=n_neurons)\n",
    "\n",
    "logger.info(model)\n",
    "# model = model.to(device, non_blocking=True)\n",
    "\n",
    "# ToDo: compiler warning: error or tracer error?\n",
    "writer.add_graph(model, input_to_model=data)  # try to add after training?\n",
    "writer.flush()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.epochs = 200\n",
    "metrics_log = run_experiment(model=model, dls=(\n",
    "    dl_train, dl_valid), writer=writer, args=args)  # decide about format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perfromance plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame(metrics_log)\n",
    "metrics.plot(\n",
    "    figsize=(18, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics[('valid', 'norm_loss')] = metrics[(\n",
    "    'valid', 'loss')] / len(dl_valid.dataset)\n",
    "metrics[('train', 'norm_loss')] = metrics[(\n",
    "    'train', 'loss')] / len(dl_train.dataset)\n",
    "\n",
    "selected = [(_split, _metric)\n",
    "            for _split in ['train', 'valid']\n",
    "            for _metric in ['norm_loss']\n",
    "            ]\n",
    "_ = metrics[selected].plot(\n",
    "    figsize=(18, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard\n",
    "\n",
    "- can be run from notebook\n",
    "- or in a separate process to inspect currently running training loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "# %load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # first time, it timesout, second time it starts, see https://github.com/tensorflow/tensorboard/issues/2481#issuecomment-516819768\n",
    "# %tensorboard --logdir {tensorboard_model_namer.folder} --host localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Run to see updates: \\n\\n\\ttensorboard --logdir {tensorboard_model_namer.folder.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter comparison\n",
    "\n",
    "- [x] order data by date: consecutive samples from training to validation\n",
    "- [ ] check stratification based on machine and column length between splits\n",
    "    - validation and traning data have same proportion of machine types\n",
    "- [ ] complete meta data reading based on filenames\n",
    "- [ ] compare performance regarding data normalization\n",
    "    - in original intensity space (non-log-transformed)\n",
    "- [ ] compare performance regarding several hyperparameters of VAE (layers, activation, etc)\n",
    "    - plot different losses in one plot as validation data set is the same\n",
    "- [ ] increase number of samples in training set and create result plot\n",
    "\n",
    "\n",
    "- Current optimum for comparision is zero\n",
    "\n",
    "> The comparison where relatively low abundant, but not super low-abundant peptides will be masked, could skew the comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writer # new writer\n",
    "# dls = get_dls(data_in_memory, scaler)\n",
    "# model = VAE()\n",
    "# writer =  # new writer for each setup\n",
    "# metrics = run_experiment(model, dls, writer)\n",
    "# overview['experiment_name'] = metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vaep",
   "language": "python",
   "name": "vaep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
