{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from src import config\n",
    "from src.analyzers import *\n",
    "from vaep.transform import StandardScaler, get_df_fitted_mean_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from src.logging import setup_logger\n",
    "\n",
    "logger = logging.getLogger()  # returns root-logger\n",
    "logger.setLevel(logging.CRITICAL)  # silence for everything else\n",
    "logger.handlers = []\n",
    "\n",
    "\n",
    "logger = setup_logger(logger=logging.getLogger('vaep'))\n",
    "logger.info(\"Experiment 01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "- 1000 features (most abundant peptides)\n",
    "- later a subset of samples is selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES_TO_LOAD = None\n",
    "FN_PEPTIDE_INTENSITIES = config.FOLDER_DATA / 'df_intensities_N_07813_M01000'\n",
    "analysis = AnalyzePeptides(\n",
    "    fname=FN_PEPTIDE_INTENSITIES, nrows=N_SAMPLES_TO_LOAD)\n",
    "analysis.df = analysis.df.sort_index()  # sort by date\n",
    "assert analysis.df.index.is_unique, \"Non-unique training samples\"\n",
    "analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select consecutives samples for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "N_SAMPLES = 1000\n",
    "logger.info(f\"Selected {N_SAMPLES}\")\n",
    "analysis.N_SAMPLES = N_SAMPLES\n",
    "\n",
    "\n",
    "def get_consecutive_data_indices(index, n_samples=N_SAMPLES):\n",
    "    start_sample = len(index) - n_samples\n",
    "    start_sample = random.randint(0, start_sample)\n",
    "    return index[start_sample:start_sample+n_samples]\n",
    "\n",
    "\n",
    "indices_selected = get_consecutive_data_indices(analysis.df.index)\n",
    "analysis.samples = indices_selected\n",
    "analysis.df = analysis.df.loc[indices_selected]\n",
    "\n",
    "FRACTION = 0.9\n",
    "\n",
    "class Indices(SimpleNamespace):\n",
    "    pass\n",
    "\n",
    "indices = Indices()\n",
    "indices.train, indices.valid = indices_selected[:int(\n",
    "    FRACTION*N_SAMPLES)], indices_selected[int(FRACTION*N_SAMPLES):]\n",
    "analysis.indices = indices\n",
    "\n",
    "analysis.df_train = analysis.df.loc[indices.train]\n",
    "analysis.df_valid = analysis.df.loc[indices.valid]\n",
    "\n",
    "analysis.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create meta data from filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import metadata\n",
    "\n",
    "data_meta = metadata.get_metadata_from_filenames(indices_selected)\n",
    "analysis.df_meta = pd.DataFrame.from_dict(\n",
    "    data_meta, orient='index')\n",
    "# analysis.df_meta['date'] = pd.to_datetime(analysis.df_meta['date'])\n",
    "analysis.df_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- possibility to group data in time along `(machine, lc)` pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.df_meta.loc[indices.train].describe(datetime_is_numeric=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This becomes part of analysis\n",
    "def compare_meta_data_for_splits(meta, indices):\n",
    "\n",
    "    _indices = vars(indices)\n",
    "    logger.info('Found vars: {}'.format(', '.join(str(x)\n",
    "                                                  for x in _indices.keys())))\n",
    "\n",
    "    for key_split, split in _indices.items():\n",
    "        print(f\"{key_split:8} - split description:\")\n",
    "        display(\n",
    "            meta.loc[split].describe(datetime_is_numeric=True)\n",
    "        )\n",
    "\n",
    "    _meta_features = list(meta.columns)\n",
    "\n",
    "    for _column in _meta_features:\n",
    "        display(\n",
    "            _=pd.DataFrame({\n",
    "                key_split: meta.loc[split, _column].value_counts(normalize=True) for key_split, split in _indices.items()\n",
    "            }).sort_index().plot(kind='line', rot=90, figsize=(10, 5), title=f\"{_column} value Counts for different splits\")\n",
    "        )\n",
    "\n",
    "\n",
    "compare_meta_data_for_splits(analysis.df_meta.iloc[:, :2], indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis state so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# import importlib; importlib.reload(vaep.model)\n",
    "from vaep.model import train\n",
    "from vaep.model import VAE\n",
    "from vaep.model import loss_function\n",
    "from vaep.cmd import get_args\n",
    "\n",
    "from vaep.io.datasets import PeptideDatasetInMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = get_args(no_cuda=True)\n",
    "kwargs = {'num_workers': 2, 'pin_memory': True} if args.cuda else {}\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "# EpochAverages = namedtuple(\"EpochAverages\", 'loss mse kld')\n",
    "\n",
    "\n",
    "def eval(epoch, model, data_loader, device, writer=None):\n",
    "    model.eval()\n",
    "    metrics = {'loss': 0, 'mse': 0,  'kld': 0}\n",
    "\n",
    "    for batch, mask in data_loader:\n",
    "        recon_batch, mu, logvar = model(batch)\n",
    "        loss, mse, kld = loss_function(\n",
    "            recon_x=recon_batch, x=batch, mask=mask, mu=mu, logvar=logvar)\n",
    "        metrics['loss'] += loss.item()\n",
    "        metrics['mse'] += mse.item()\n",
    "        metrics['kld'] += kld.item()\n",
    "    if writer is not None:\n",
    "        n_samples = len(data_loader.dataset)\n",
    "        writer.add_scalar('avg validation loss',\n",
    "                          metrics['loss'] / n_samples,\n",
    "                          epoch)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_limit = np.log10(analysis.df).min().min()  # all zeros become nan.\n",
    "\"Detection limit: {:6.3f}, corresponding to intensity value of {:,d}\".format(\n",
    "    detection_limit,\n",
    "    int(10 ** detection_limit)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_features = analysis.df.shape\n",
    "\"N samples: {:10,d} - N Peptides: {:10,d}\".format(n_samples, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(analysis.indices.valid), analysis.indices.valid[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-log transformed data (Single run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale samples according to training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select initial data: transformed vs not log transformed\n",
    "scaler = scaler.fit(analysis.df_train)\n",
    "# five examples from validation dataset\n",
    "scaler.transform(analysis.df_valid.iloc[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(df_train, df_valid, scaler):\n",
    "    data_train = PeptideDatasetInMemory(\n",
    "        data=scaler.transform(df_train))\n",
    "    data_valid = PeptideDatasetInMemory(data=scaler.transform(df_valid))\n",
    "\n",
    "    dl_train = torch.utils.data.DataLoader(\n",
    "        dataset=data_train,\n",
    "        batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    dl_valid = torch.utils.data.DataLoader(\n",
    "        dataset=data_valid,\n",
    "        batch_size=args.batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "    return dl_train, dl_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorboardModelNamer():\n",
    "    def __init__(self, prefix_folder, root_dir=Path('runs')):\n",
    "        self.prefix_folder = prefix_folder\n",
    "        self.root_logdir = Path(root_dir)\n",
    "        self.folder = (self.root_logdir /\n",
    "                       f'{self.prefix_folder}_{format(datetime.now(), \"%y%m%d_%H%M\")}')\n",
    "\n",
    "    def get_model_name(self, hidden_layers: int,\n",
    "                       neurons: list,\n",
    "                       scaler: str,\n",
    "                       ):\n",
    "        name = 'model_'\n",
    "        name += f'hl{hidden_layers:02d}'\n",
    "\n",
    "        if type(neurons) == str:\n",
    "            neurons = neurons.split()\n",
    "        elif not type(neurons) in [list, tuple]:\n",
    "            raise TypeError(\n",
    "                \"Provide expected format for neurons: [12, 13, 14], '12 13 14' or '12_13_14'\")\n",
    "\n",
    "        for x in neurons:\n",
    "            name += f'_{x}'\n",
    "\n",
    "        if type(scaler) == str:\n",
    "            name += f'_{scaler}'\n",
    "        else:\n",
    "            name += f'_{scaler!r}'\n",
    "        return name\n",
    "\n",
    "    def get_writer(self, hidden_layers: int,\n",
    "                   neurons: list,\n",
    "                   scaler: str,\n",
    "                   ):\n",
    "        model_name = self.get_model_name(hidden_layers=hidden_layers,\n",
    "                                         neurons=neurons,\n",
    "                                         scaler=scaler)\n",
    "        return SummaryWriter(log_dir=self.folder / model_name)\n",
    "\n",
    "\n",
    "expected = 'model_hl01_12_13_14_scaler'\n",
    "\n",
    "tensorboard_model_namer = TensorboardModelNamer(prefix_folder='experiment_01')\n",
    "\n",
    "assert tensorboard_model_namer.get_model_name(\n",
    "    hidden_layers=1, neurons=[12, 13, 14], scaler='scaler') == expected\n",
    "assert tensorboard_model_namer.get_model_name(\n",
    "    hidden_layers=1, neurons='12 13 14', scaler='scaler') == expected\n",
    "assert tensorboard_model_namer.get_model_name(\n",
    "    hidden_layers=1, neurons='12_13_14', scaler='scaler') == expected\n",
    "assert tensorboard_model_namer.get_model_name(\n",
    "    hidden_layers=1, neurons='12_13_14', scaler=scaler) == 'model_hl01_12_13_14_StandardScaler()'\n",
    "# assert get_writer(hidden_layers=1, neurons=1, scaler=scaler) == TypeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neurons = max(30, int(n_features/6))\n",
    "logger.info(f'Latent layer neurons: {n_neurons}')\n",
    "\n",
    "writer = tensorboard_model_namer.get_writer(1, [n_neurons], 'scaler')\n",
    "logger.info(f\"Logging to: {writer.get_logdir()}\")\n",
    "\n",
    "\n",
    "dl_train, dl_valid = get_dataloaders(\n",
    "    df_train=analysis.df_train,\n",
    "    df_valid=analysis.df_valid,\n",
    "    scaler=scaler)\n",
    "\n",
    "logger.info(\n",
    "    \"N train: {:5,d} \\nN valid: {:5,d}\".format(\n",
    "        len(dl_train.dataset), len(dl_valid.dataset))\n",
    ")\n",
    "\n",
    "data, mask = next(iter(dl_train))\n",
    "\n",
    "writer.add_image(\n",
    "    f'{len(data)} batch of sampled data (as heatmap)', data, dataformats='HW')\n",
    "writer.add_image(\n",
    "    f'{len(mask)} mask for this batch of samples', mask, dataformats='HW')\n",
    "\n",
    "\n",
    "model = VAE(n_features=n_features, n_neurons=n_neurons)\n",
    "\n",
    "logger.info(model)\n",
    "# model = model.to(device, non_blocking=True)\n",
    "\n",
    "# ToDo: compiler warning: error or tracer error?\n",
    "writer.add_graph(model, input_to_model=data)  # try to add after training?\n",
    "writer.flush()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def run_experiment(model, dls, writer, args):\n",
    "    metrics = defaultdict(dict)\n",
    "    dl_train, dl_valid = dls\n",
    "    msg_eval_epoch = \"Validation Set - Epoch: {:3d} - loss: {:7.3f} - mse: {:5.3f} - KLD: {:5.3f}\"\n",
    "\n",
    "    for epoch in range(1, args.epochs):\n",
    "        metrics[('train', 'loss')][epoch] = float(train(epoch, model=model, train_loader=dl_train,\n",
    "                                                        optimizer=optimizer, device=device, writer=writer))\n",
    "        # ToDo: Pull out writer from eval function\n",
    "        _epoch_metric_valid = eval(\n",
    "            epoch, model=model, data_loader=dl_valid, device=device, writer=writer)\n",
    "        metrics[('valid', 'loss')][epoch] = _epoch_metric_valid['loss']\n",
    "        metrics[('valid', 'mse')][epoch] = _epoch_metric_valid['mse']\n",
    "        metrics[('valid', 'kld')][epoch] = _epoch_metric_valid['kld']\n",
    "        if not epoch % 10:\n",
    "            logger.info(msg_eval_epoch.format(\n",
    "                epoch, *_epoch_metric_valid.values()))\n",
    "    writer.flush()\n",
    "    writer.close()  # closes all internal writers of SummaryWriter\n",
    "    return metrics\n",
    "\n",
    "\n",
    "args.epochs = 200\n",
    "metrics = run_experiment(model=model, dls=(\n",
    "    dl_train, dl_valid), writer=writer, args=args)  # decide about format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame(metrics)\n",
    "_ = metrics.plot(\n",
    "    figsize=(18, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics[('valid', 'norm_loss')] = metrics[(\n",
    "    'valid', 'loss')] / len(dl_valid.dataset)\n",
    "metrics[('train', 'norm_loss')] = metrics[(\n",
    "    'train', 'loss')] / len(dl_train.dataset)\n",
    "\n",
    "selected = [(_split, _metric)\n",
    "            for _split in ['train', 'valid']\n",
    "            for _metric in ['norm_loss']\n",
    "            ]\n",
    "_ = metrics[selected].plot(\n",
    "    figsize=(18, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log transformed data (Single run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.df_train_log10 = np.log10(analysis.df_train)\n",
    "analysis.df_valid_log10 = np.log10(analysis.df_valid)\n",
    "scaler_log = StandardScaler().fit(X=analysis.df_train_log10)\n",
    "# five examples from validation dataset\n",
    "scaler_log.transform(analysis.df_valid_log10.iloc[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_neurons = max(30, int(n_features/6))\n",
    "logger.info(f'Latent layer neurons: {n_neurons}')\n",
    "\n",
    "writer = tensorboard_model_namer.get_writer(1, [n_neurons], 'scaler_log')\n",
    "logger.info(f\"Logging to: {writer.get_logdir()}\")\n",
    "\n",
    "\n",
    "dl_train, dl_valid = get_dataloaders(df_train=analysis.df_train_log10, df_valid=analysis.df_valid_log10, scaler=scaler_log)\n",
    "\n",
    "logger.info(\n",
    "    \"N train: {:5,d} \\nN valid: {:5,d}\".format(\n",
    "        len(dl_train.dataset), len(dl_valid.dataset))\n",
    ")\n",
    "\n",
    "data, mask = next(iter(dl_train))\n",
    "\n",
    "writer.add_image(\n",
    "    f'{len(data)} batch of sampled data (as heatmap)', data, dataformats='HW')\n",
    "writer.add_image(\n",
    "    f'{len(mask)} mask for this batch of samples', mask, dataformats='HW')\n",
    "\n",
    "\n",
    "model = VAE(n_features=n_features, n_neurons=n_neurons)\n",
    "\n",
    "logger.info(model)\n",
    "# model = model.to(device, non_blocking=True)\n",
    "\n",
    "# ToDo: compiler warning: error or tracer error?\n",
    "writer.add_graph(model, input_to_model=data)  # try to add after training?\n",
    "writer.flush()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.epochs = 200\n",
    "metrics_log = run_experiment(model=model, dls=(\n",
    "    dl_train, dl_valid), writer=writer, args=args)  # decide about format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perfromance plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame(metrics_log)\n",
    "metrics.plot(\n",
    "    figsize=(18, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics[('valid', 'norm_loss')] = metrics[(\n",
    "    'valid', 'loss')] / len(dl_valid.dataset)\n",
    "metrics[('train', 'norm_loss')] = metrics[(\n",
    "    'train', 'loss')] / len(dl_train.dataset)\n",
    "\n",
    "selected = [(_split, _metric)\n",
    "            for _split in ['train', 'valid']\n",
    "            for _metric in ['norm_loss']\n",
    "            ]\n",
    "_ = metrics[selected].plot(\n",
    "    figsize=(18, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard\n",
    "\n",
    "- can be run from notebook\n",
    "- or in a separate process to inspect currently running training loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "# %load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # first time, it timesout, second time it starts, see https://github.com/tensorflow/tensorboard/issues/2481#issuecomment-516819768\n",
    "# %tensorboard --logdir {tensorboard_model_namer.folder} --host localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Run to see updates: \\n\\n\\ttensorboard --logdir {tensorboard_model_namer.folder.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter comparison\n",
    "\n",
    "- [x] order data by date: consecutive samples from training to validation\n",
    "- [ ] check stratification based on machine and column length between splits\n",
    "    - validation and traning data have same proportion of machine types\n",
    "- [ ] complete meta data reading based on filenames\n",
    "- [ ] compare performance regarding data normalization\n",
    "    - in original intensity space (non-log-transformed)\n",
    "- [ ] compare performance regarding several hyperparameters of VAE (layers, activation, etc)\n",
    "    - plot different losses in one plot as validation data set is the same\n",
    "- [ ] increase number of samples in training set and create result plot\n",
    "\n",
    "\n",
    "- Current optimum for comparision is zero\n",
    "\n",
    "> The comparison where relatively low abundant, but not super low-abundant peptides will be masked, could skew the comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writer # new writer\n",
    "# dls = get_dls(data_in_memory, scaler)\n",
    "# model = VAE()\n",
    "# writer =  # new writer for each setup\n",
    "# metrics = run_experiment(model, dls, writer)\n",
    "# overview['experiment_name'] = metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vaep",
   "language": "python",
   "name": "vaep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
