{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 03 - Data\n",
    "\n",
    "Create data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from src.nb_imports import *\n",
    "pd.options.display.max_columns = 32\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from vaep.pandas import interpolate, parse_query_expression\n",
    "from vaep.io.datasplits import DataSplits\n",
    "from vaep.io import thermo_raw_files\n",
    "from vaep.sampling import feature_frequency, frequency_by_index, sample_data\n",
    "\n",
    "import src\n",
    "from vaep.logging import setup_logger\n",
    "logger = setup_logger(logger=logging.getLogger('vaep'))\n",
    "logger.info(\"Experiment 03 - data\")\n",
    "\n",
    "figures = {}  # collection of ax or figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "FN_INTENSITIES: str =  'data/single_datasets/df_intensities_proteinGroups_long_2017_2018_2019_2020_N05015_M04547/Q_Exactive_HF_X_Orbitrap_Exactive_Series_slot_#6070.pkl'  # Intensities for feature\n",
    "# FN_PEPTIDE_FREQ: str = 'data/processed/count_all_peptides.json' # Peptide counts for all parsed files on erda (for data selection)\n",
    "fn_rawfile_metadata: str = 'data/files_selected_metadata.csv' # Machine parsed metadata from rawfile workflow\n",
    "# M: int = 5000 # M most common features\n",
    "MIN_SAMPLE: Union[int, float] = 0.5 # Minimum number or fraction of total requested features per Sample\n",
    "min_RT_max: Union[int, float] = 120 # Minum retention time (RT) in minutes\n",
    "index_col: Union[str,int] = ['Sample ID', 'Gene names'] # Can be either a string or position (typical 0 for first column)\n",
    "# query expression for subsetting\n",
    "# query_subset_meta: str = \"`instrument serial number` in ['Exactive Series slot #6070',]\" # query for metadata, see files_selected_per_instrument_counts.csv for options\n",
    "logarithm: str = 'log2' # Log transformation of initial data (select one of the existing in numpy)\n",
    "folder_experiment: str = f'runs/experiment_03/{Path(FN_INTENSITIES).parent.name}/{Path(FN_INTENSITIES).stem}'\n",
    "# columns_name: str = 'Gene names'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# # peptides\n",
    "FN_INTENSITIES: str = 'data/single_datasets/df_intensities_peptides_long_2017_2018_2019_2020_N05011_M42725/Q_Exactive_HF_X_Orbitrap_Exactive_Series_slot_#6070.pkl'  # Intensities for feature\n",
    "index_col: Union[str,int] = ['Sample ID', 'peptide'] # Can be either a string or position (typical 0 for first column)\n",
    "folder_experiment: str = f'runs/experiment_03/{Path(FN_INTENSITIES).parent.name}/{Path(FN_INTENSITIES).stem}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# # evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There must be a better way...\n",
    "@dataclass\n",
    "class DataConfig:\n",
    "    \"\"\"Documentation. Copy pasted arguments to a dataclass.\"\"\"\n",
    "    FN_INTENSITIES: str  # Samples metadata extraced from erda\n",
    "    file_ext: str # file extension\n",
    "    # FN_PEPTIDE_FREQ: str # Peptide counts for all parsed files on erda (for data selection)\n",
    "    fn_rawfile_metadata: str  # Machine parsed metadata from rawfile workflow\n",
    "    # M: int # M most common features\n",
    "    MIN_SAMPLE: Union[int, float] = 0.5 # Minimum number or fraction of total requested features per Sample\n",
    "    index_col: Union[\n",
    "        str, int\n",
    "    ] = \"Sample ID\"  # Can be either a string or position (typical 0 for first column)\n",
    "    # query expression for subsetting\n",
    "    # query_subset_meta: str = \"`instrument serial number` in ['Exactive Series slot #6070',]\" # query for metadata, see files_selected_per_instrument_counts.csv for options\n",
    "    logarithm: str = 'log2' # Log transformation of initial data (select one of the existing in numpy)\n",
    "    folder_experiment: str = 'runs/experiment_03'\n",
    "    # columns_name: str = \"peptide\"\n",
    "\n",
    "\n",
    "params = DataConfig(\n",
    "    FN_INTENSITIES=FN_INTENSITIES,\n",
    "    file_ext=Path(FN_INTENSITIES).suffix[1:],\n",
    "    # FN_PEPTIDE_FREQ=FN_PEPTIDE_FREQ,\n",
    "    fn_rawfile_metadata=fn_rawfile_metadata,\n",
    "    # M=M,\n",
    "    MIN_SAMPLE=MIN_SAMPLE,\n",
    "    index_col=index_col,\n",
    "    # query_subset_meta=query_subset_meta,\n",
    "    logarithm=logarithm,\n",
    "    folder_experiment=folder_experiment,\n",
    "    # columns_name=columns_name\n",
    ")\n",
    "\n",
    "params = OmegaConf.create(params.__dict__)\n",
    "dict(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not folder_experiment:\n",
    "#     folder_experiment = query_subset_meta.replace('_', ' ')\n",
    "#     folder_experiment = parse_query_expression(Ffolder_experiment)\n",
    "#     folder_experiment = folder_experiment.strip()\n",
    "#     folder_experiment = folder_experiment.replace(' ', '_')\n",
    "#     params.folder_experiment\n",
    "folder_experiment = Path(folder_experiment)\n",
    "folder_experiment.mkdir(exist_ok=True, parents=True)\n",
    "logger.info(f'Folder for output = {folder_experiment}')\n",
    "\n",
    "folder_data = folder_experiment / 'data'\n",
    "folder_data.mkdir(exist_ok=True)\n",
    "logger.info(f'Folder for data: {folder_data = }')\n",
    "\n",
    "folder_figures = folder_experiment / 'figures'\n",
    "folder_figures.mkdir(exist_ok=True)\n",
    "logger.info(f'Folder for figures: {folder_figures = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "process arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"{FN_INTENSITIES = }\")\n",
    "\n",
    "\n",
    "FILE_FORMAT_TO_CONSTRUCTOR = {'csv': 'from_csv',\n",
    "                              'pkl': 'from_pickle',\n",
    "                              'pickle': 'from_pickle',\n",
    "                              }\n",
    "\n",
    "FILE_EXT = Path(FN_INTENSITIES).suffix[1:]\n",
    "logger.info(f\"File format (extension): {FILE_EXT}  (!specifies data loading function!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "params.used_features = None # sorted(selected_peptides)\n",
    "if isinstance(params.index_col, str) and params.used_features: params.used_features.insert(0, params.index_col)\n",
    "constructor = getattr(AnalyzePeptides, FILE_FORMAT_TO_CONSTRUCTOR[FILE_EXT]) #AnalyzePeptides.from_csv \n",
    "analysis = constructor(fname=params.FN_INTENSITIES,\n",
    "                                     index_col=index_col,\n",
    "                                     usecols=params.used_features\n",
    "                                    )\n",
    "\n",
    "# set automatically\n",
    "# analysis.df.columns.name = params.columns_name\n",
    "columns_name = analysis.df.columns.name # ToDo: MultiIndex adaptions will be needed\n",
    "\n",
    "log_fct = getattr(np, params.logarithm)\n",
    "analysis.log_transform(log_fct)\n",
    "logger.info(f\"{analysis = }\")\n",
    "analysis.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select M most common features\n",
    "\n",
    "- if number of features should be part of experiments, the selection has to be done here\n",
    "- select between `analysis.M` (number of features available) and requested number of features `params.M` \n",
    "- can be random or based on most common features from counter objects\n",
    "\n",
    "> Ignored for now, instead select based on feature availabiltiy across samples (see below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "# from collections import Counter\n",
    "# # Use PeptideCounter instead?\n",
    "# with open(Path(params.FN_PEPTIDE_FREQ)) as f:\n",
    "#     freq_pep_all = Counter(json.load(f)['counter'])\n",
    "    \n",
    "# selected_peptides = {k: v for k, v in freq_pep_all.most_common(params.M)}\n",
    "# print(f\"No. of selected features: {len(selected_peptides):,d}\")\n",
    "analysis.M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample selection\n",
    "\n",
    "- ensure unique indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert analysis.df.index.is_unique, \"Duplicates in index\"\n",
    "analysis.df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select samples based on completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(params.MIN_SAMPLE, float):\n",
    "    msg = f'Fraction of minimum sample completeness over all features specified with: {params.MIN_SAMPLE}\\n'\n",
    "    # assumes df in wide format\n",
    "    params.MIN_SAMPLE = int(analysis.df.shape[1] * params.MIN_SAMPLE)\n",
    "    msg += f'This translates to a minimum number of total samples: {params.MIN_SAMPLE}'\n",
    "    print(msg)\n",
    "\n",
    "sample_counts = analysis.df.notna().sum(axis=1) # if DataFrame\n",
    "\n",
    "mask = sample_counts > params.MIN_SAMPLE\n",
    "msg = f'Drop {len(mask) - mask.sum()} of {len(mask)} initial samples.'\n",
    "print(msg)\n",
    "analysis.df = analysis.df.loc[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.used_samples = analysis.df.index.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine metadata\n",
    "\n",
    "- read from file using [ThermoRawFileParser](https://github.com/compomics/ThermoRawFileParser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = pd.read_csv('data/files_selected_metadata.csv', index_col=0)\n",
    "df_meta = df_meta.loc[analysis.df.index.to_list()] # index is sample index\n",
    "date_col = 'Content Creation Date'\n",
    "df_meta[date_col] = pd.to_datetime(df_meta[date_col])\n",
    "df_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_instrument = thermo_raw_files.cols_instrument\n",
    "df_meta.groupby(cols_instrument)[date_col].agg(['count','min','max']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta.describe(datetime_is_numeric=True, percentiles=np.linspace(0.05, 0.95, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set a minimum retention time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_RT_max = 120 # minutes\n",
    "msg = f\"Minimum RT time maxiumum is set to {min_RT_max} minutes (to exclude too short runs, which are potentially fractions).\"\n",
    "mask_RT = df_meta['MS max RT'] >= 120 # can be integrated into query string\n",
    "msg += f\" Total number of samples retained: {int(mask_RT.sum())}.\"\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = df_meta.loc[mask_RT]\n",
    "df_meta = df_meta.sort_values(date_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_stats = df_meta.describe(include='all', datetime_is_numeric=True)\n",
    "meta_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "subset with variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_stats.loc[:, (meta_stats.loc['unique'] > 1) |  (meta_stats.loc['std'] > 0.1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check some columns describing settings\n",
    "  - quite some variation due to `MS max charge`: Is it a parameter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_raw_settings = [\n",
    " 'Thermo Scientific instrument model',\n",
    " 'instrument serial number',\n",
    " 'Software Version', \n",
    " 'MS max charge',\n",
    " 'mass resolution',\n",
    " 'beam-type collision-induced dissociation', \n",
    " 'injection volume setting',\n",
    " 'dilution factor',\n",
    "]\n",
    "df_meta[meta_raw_settings].drop_duplicates() # index gives first example with this combination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "view without `MS max charge`:\n",
    "  - software can be updated\n",
    "  - variation by `injection volume setting` and instrument over time\n",
    "    - 500ng of peptides should be injected, based on concentration of peptides this setting is adjustd to get it\n",
    "  - missing `dilution factor`\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['MS max charge']\n",
    "df_meta[meta_raw_settings].drop(to_drop, axis=1).drop_duplicates() # index gives first example with this combination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- check for variation in `software Version` and `injection volume setting`\n",
    "\n",
    "\n",
    "Update selection of samples based on metadata (e.g. minimal retention time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_meta_data(analysis: AnalyzePeptides, df_meta:pd.DataFrame):\n",
    "    try:\n",
    "        analysis.df = analysis.df.loc[df_meta.index]\n",
    "    except KeyError as e:\n",
    "        logger.warning(e)\n",
    "        logger.warning(\"Ignore missing samples in quantified samples\")\n",
    "        analysis.df = analysis.df.loc[analysis.df.index.intersection(df_meta.index)]\n",
    "\n",
    "    analysis.df_meta = df_meta # ToDo: Don't have preset metadata from filename\n",
    "    return analysis\n",
    "\n",
    "analysis = add_meta_data(analysis, df_meta=df_meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive and Single plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatter plots need to become interactive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_counts.name = 'identified features'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 2\n",
    "pcs = analysis.get_PCA(n_components=K) # should be renamed to get_PCs\n",
    "pcs = pcs.iloc[:,:K].join(df_meta).join(sample_counts)\n",
    "\n",
    "pcs_name = pcs.columns[:2]\n",
    "pcs = pcs.reset_index()\n",
    "pcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18,10))\n",
    "analyzers.seaborn_scatter(pcs[pcs_name], fig, ax, meta=pcs['Thermo Scientific instrument model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(23,10))\n",
    "analyzers.plot_date_map(pcs[pcs_name], fig, ax, pcs[date_col])\n",
    "vaep.savefig(fig, folder_figures / 'pca_sample_by_date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- software version: Does it make a difference?\n",
    "- size: number of features in a single sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    pcs, x=pcs_name[0], y=pcs_name[1],\n",
    "    hover_name='Sample ID',\n",
    "    # hover_data=analysis.df_meta,\n",
    "    title=f'First two Principal Components of {analysis.M} most abundant peptides for {pcs.shape[0]} samples',\n",
    "    # color=pcs['Software Version'],\n",
    "    color='identified features',\n",
    "    width=1200,\n",
    "    height=600\n",
    ")\n",
    "fig.write_image(folder_figures / 'pca_identified_features.png')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Medians and percentiles\n",
    "\n",
    "- see boxplot [function in R](https://github.dev/symbioticMe/proBatch/blob/8fae15049be67693bd0d4a4383b51bfb4fb287a6/R/initial_assessment.R#L199-L317), which is used for [publication figure](https://github.dev/symbioticMe/batch_effects_workflow_code/blob/696eb609a55ba9ece68b732e616d7ebeaa660373/AgingMice_study/analysis_AgingMice/5b_Fig2_initial_assessment_normalization.R)\n",
    "- check boxplot functions: [bokeh](https://docs.bokeh.org/en/latest/docs/gallery/boxplot.html), [plotly](https://plotly.com/python/box-plots/), [eventplot](https://matplotlib.org/stable/gallery/lines_bars_and_markers/eventplot_demo.html#sphx-glr-gallery-lines-bars-and-markers-eventplot-demo-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis.df.iloc[:100].T.boxplot(rot=90, backend=None, figsize=None)\n",
    "# analysis.df.boxplot()\n",
    "analysis.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = analysis.df\n",
    "df = df.join(df_meta[date_col])\n",
    "df = df.set_index(date_col).sort_index().to_period('min').T\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "ax = df.boxplot(rot=80, figsize=(20, 10), fontsize='large', showfliers=False, showcaps=False)\n",
    "_ = vaep.plotting.select_xticks(ax)\n",
    "fig = ax.get_figure()\n",
    "vaep.savefig(fig, folder_figures / 'median_boxplot')\n",
    "figures['median_boxplot'] = fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot sample median over time\n",
    "  - check if points are equally spaced (probably QC samples are run in close proximity)\n",
    "  - the machine will be not use for intermediate periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "dates = df_meta[date_col].sort_values()\n",
    "dates.name = 'date'\n",
    "median_sample_intensity = (analysis.df\n",
    "                           .median(axis=1)\n",
    "                           .to_frame('median intensity'))\n",
    "median_sample_intensity = median_sample_intensity.join(dates)\n",
    "\n",
    "ax = median_sample_intensity.plot.scatter(x='date', y='median intensity',\n",
    "                                          rot=90,\n",
    "                                          fontsize='large',\n",
    "                                          figsize=(20, 10),\n",
    "                                          xticks=vaep.plotting.select_dates(\n",
    "                                              median_sample_intensity['date'])\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the closer the labels are there denser the samples are measured aroudn that time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split: Train, validation and test data\n",
    "\n",
    "- test data is in clinical language often denoted as independent validation cohort\n",
    "- validation data (for model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.splits = DataSplits(is_wide_format=True)\n",
    "splits = analysis.splits\n",
    "print(f\"{analysis.splits = }\")\n",
    "analysis.splits.__annotations__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentiles = (0.8, 0.9)  # change here\n",
    "\n",
    "# percent_str = [f'{int(x*100)}%' for x in percentiles]\n",
    "# split_at_date = analysis.df_meta[date_col].describe(\n",
    "#     datetime_is_numeric=True, percentiles=(0.8, 0.9)).loc[percent_str]\n",
    "# split_at_date = tuple(pd.Timestamp(t.date()) for t in split_at_date)\n",
    "\n",
    "# print(f\"{split_at_date[0] = }\", f\"{split_at_date[1] = }\", sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx_train = analysis.df_meta[date_col] < split_at_date[0]\n",
    "# analysis.splits.train_X = analysis.df.loc[idx_train]\n",
    "# analysis.splits.train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx_validation = ((analysis.df_meta[date_col] >= split_at_date[0]) & (\n",
    "#     analysis.df_meta[date_col] < split_at_date[1]))\n",
    "# analysis.splits.val_X = analysis.df.loc[idx_validation]\n",
    "# analysis.splits.val_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx_test = (analysis.df_meta[date_col] >= split_at_date[1])\n",
    "# # analysis.df_test =\n",
    "# analysis.splits.test_X = analysis.df.loc[idx_test]\n",
    "# analysis.splits.test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx_test_na = analysis.splits.test_X.stack(\n",
    "#     dropna=False).loc[splits.test_X.isna().stack()].index\n",
    "# print(f\"number of missing values in test data: {len(idx_test_na)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peptide frequency  in training data\n",
    "\n",
    "- higher count, higher probability to be sampled into training data\n",
    "- missing peptides are sampled both into training as well as into validation dataset\n",
    "- everything not in training data is validation data\n",
    "\n",
    "Based on unmodified training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = \"Total number of samples in training data split: {}\"\n",
    "print(msg.format(len(analysis.df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # analysis.splits.to_wide_format()\n",
    "# assert analysis.splits is splits, \"Sanity check failed.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recalculate feature frequency after selecting some samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_per_feature = feature_frequency(analysis.df)\n",
    "freq_per_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq_per_feature.name = 'Gene names freq' # name it differently?\n",
    "freq_per_feature.to_json(folder_data / 'freq_train.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conserning sampling with frequency weights:\n",
    "  - larger weight -> higher probablility of being sampled\n",
    "  - weights need to be alignable to index of original DataFrame before grouping (same index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample targets (Fake NAs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add goldstandard targets for valiation and test data\n",
    "- based on same day\n",
    "- same instrument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create some target values by sampling 5% of the validation and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.to_long_format(inplace=True)\n",
    "analysis.df_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis.splits.to_long_format(name_values='intensity') # long format as sample_data uses long-format \n",
    "# analysis.splits\n",
    "fake_na, splits.train_X = sample_data(analysis.df_long.squeeze(), sample_index_to_drop=0, weights=freq_per_feature, frac=0.1)\n",
    "assert len(splits.train_X) > len(fake_na)\n",
    "splits.val_y = fake_na.sample(frac=0.5).sort_index()\n",
    "splits.test_y = fake_na.loc[fake_na.index.difference(splits.val_y.index)]\n",
    "# splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits.test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits.val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# potential bug: wrong order...\n",
    "# splits.val_X, splits.val_y = sample_data(splits.val_X, sample_index_to_drop=0, weights=freq_per_peptide) # I this the wrong way around?\n",
    "# splits.test_X, splits.test_y = sample_data(splits.test_X, sample_index_to_drop=0, weights=freq_per_peptide)\n",
    "\n",
    "# for k, s in splits:\n",
    "#     s.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Save in long format\n",
    "\n",
    "- Data in long format: (peptide, sample_id, intensity)\n",
    "- no missing values kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits.dump(folder=folder_data, file_format=FILE_EXT)  # dumps data in long-format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reload from disk\n",
    "splits = DataSplits.from_folder(folder_data, file_format=FILE_EXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA plot of training data\n",
    "\n",
    "- [ ] update: uses old metadata reading to indicate metadata derived from filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ana_train_X = analyzers.AnalyzePeptides(data=splits.train_X, is_wide_format=False, ind_unstack=columns_name)\n",
    "figures['pca_train'] = ana_train_X.plot_pca()\n",
    "vaep.savefig(figures['pca_train'], folder_figures / f'pca_plot_raw_data_{ana_train_X.fname_stub}')\n",
    "# ana_train_X = add_meta_data(ana_train_X) # inplace, returns ana_train_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move: Script on matching similar samples \n",
    "\n",
    "- how to identify similar samples, e.g. using KNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add to DataSplits a inputs attribute\n",
    "\n",
    "# data_dict = {'train': splits.train_X, 'valid': splits.val_X, 'test': splits.test_X}\n",
    "# PCs = pd.DataFrame()\n",
    "# split_map = pd.Series(dtype='string')\n",
    "# for key, df in data_dict.items():\n",
    "#     df = df.unstack()\n",
    "#     PCs = PCs.append(ana_train_X.calculate_PCs(df))\n",
    "#     split_map = split_map.append(pd.Series(key, index=df.index))\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(15,8))\n",
    "# ax.legend(title='splits')\n",
    "# analyzers.seaborn_scatter(PCs.iloc[:, :2], fig, ax, meta=split_map,\n",
    "#                           title='First two principal compements (based on training data PCA)')\n",
    "# ax.get_legend().set_title(\"split\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For *Collaborative Filtering*, new samples could be initialized based on a KNN approach in the original sample space or the reduced PCA dimension.\n",
    "  - The sample embeddings of the K neighearst neighbours could be averaged for a new sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optional: Change number of principal components\n",
    "# # K = 2\n",
    "# # _ = ana_train_X.get_PCA(n_components=K)\n",
    "\n",
    "# train_PCs = ana_train_X.calculate_PCs(splits.train_X.unstack())\n",
    "# test_PCs = ana_train_X.calculate_PCs(splits.test_X.unstack())\n",
    "# nn = NearestNeighbors(n_neighbors=5).fit(train_PCs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select K neareast neighbors for first test data sample from training data. Compare equal distance mean to mean weighted by distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d, idx = nn.kneighbors(test_PCs.iloc[1:2])\n",
    "# # test_PCs.iloc[1]\n",
    "# idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_PCs.iloc[idx[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w = d / d.sum()\n",
    "# display(f\"Sample weights based on distances: {w = }\")\n",
    "# w.flatten().reshape(5,1) * train_PCs.iloc[idx[0]] # apply weights to values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame( (train_PCs.iloc[idx[0]].mean(), # mean\n",
    "#               (w.flatten().reshape(5,1) * train_PCs.iloc[idx[0]]).sum() # sum of weighted samples\n",
    "#               ), index=['mean','weighted by distance '])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add visual representation of picked points in the first two principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax.scatter(x=test_PCs.iloc[1]['PC 1'], y=test_PCs.iloc[1]['PC 2'], s=100, marker=\"v\", c='r')\n",
    "# ax.scatter(x=train_PCs.iloc[idx[0]]['PC 1'], y=train_PCs.iloc[idx[0]]['PC 2'], s=100, marker=\"s\", c='y')\n",
    "# fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(OmegaConf.to_yaml(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(folder_experiment/'data_config.yaml', 'w') as f:\n",
    "    OmegaConf.save(params, f)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cf83e9cb890c7f96eb0ae04f39a82254555f56a1a0ed2f03b23a8b40fe6cd31c"
  },
  "jupytext": {
   "formats": "ipynb,py:percent,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
