{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 03 - Data\n",
    "\n",
    "Create data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import logging\n",
    "import typing\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from src.nb_imports import *\n",
    "pd.options.display.max_columns = 32\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from vaep.pandas import interpolate, parse_query_expression\n",
    "from vaep.io.datasplits import DataSplits\n",
    "from vaep.sampling import feature_frequency, frequency_by_index, sample_data\n",
    "\n",
    "import src\n",
    "from vaep.logging import setup_logger\n",
    "logger = setup_logger(logger=logging.getLogger('vaep'))\n",
    "logger.info(\"Experiment 03 - data\")\n",
    "\n",
    "figures = {}  # collection of ax or figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "FN_PEPTIDE_INTENSITIES: str = 'data/df_intensities_N07813_M10000.csv'  # Samples metadata extraced from erda\n",
    "FN_PEPTIDE_FREQ: str = 'data/processed/count_all_peptides.json' # Peptide counts for all parsed files on erda (for data selection)\n",
    "fn_rawfile_metadata: str = 'data/files_selected_metadata.csv' # Machine parsed metadata from rawfile workflow\n",
    "M: int = 1000 # M most common features\n",
    "MIN_SAMPLE: typing.Union[int, float] = 0.5 # Minimum number or fraction of total requested features per Sample\n",
    "index_col: typing.Union[str,int] = 'Sample ID' # Can be either a string or position (typical 0 for first column)\n",
    "# query expression for subsetting\n",
    "query_subset_meta: str = \"`instrument serial number` in ['Exactive Series slot #6070',]\" # query for metadata, see files_selected_per_instrument_counts.csv for options\n",
    "logarithm: str = 'log2' # Log transformation of initial data (select one of the existing in numpy)\n",
    "experiment_folder: str = 'runs/experiment_03'\n",
    "columns_name: str = 'peptide'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There must be a better way...\n",
    "@dataclass\n",
    "class DataConfig:\n",
    "    \"\"\"Documentation. Copy pasted arguments to a dataclass.\"\"\"\n",
    "    FN_PEPTIDE_INTENSITIES: str = 'data/df_intensities_N07813_M10000.csv'  # Samples metadata extraced from erda\n",
    "    FN_PEPTIDE_FREQ: str = 'data/processed/count_all_peptides.json' # Peptide counts for all parsed files on erda (for data selection)\n",
    "    fn_rawfile_metadata: str = 'data/rawfile_metadata.csv' # Machine parsed metadata from rawfile workflow\n",
    "    M: int = 1000 # M most common features\n",
    "    MIN_SAMPLE: typing.Union[int, float] = 0.5 # Minimum number or fraction of total requested features per Sample\n",
    "    index_col: typing.Union[\n",
    "        str, int\n",
    "    ] = \"Sample ID\"  # Can be either a string or position (typical 0 for first column)\n",
    "    # query expression for subsetting\n",
    "    query_subset_meta: str = \"`instrument serial number` in ['Exactive Series slot #6070',]\" # query for metadata, see files_selected_per_instrument_counts.csv for options\n",
    "    logarithm: str = 'log2' # Log transformation of initial data (select one of the existing in numpy)\n",
    "    experiment_folder: str = 'runs/experiment_03'\n",
    "    columns_name: str = \"peptide\"\n",
    "\n",
    "\n",
    "params = DataConfig(\n",
    "    FN_PEPTIDE_INTENSITIES=FN_PEPTIDE_INTENSITIES,\n",
    "    FN_PEPTIDE_FREQ=FN_PEPTIDE_FREQ,\n",
    "    fn_rawfile_metadata=fn_rawfile_metadata,\n",
    "    M=M,\n",
    "    index_col=index_col,\n",
    "    query_subset_meta=query_subset_meta,\n",
    "    logarithm=logarithm,\n",
    "    experiment_folder=experiment_folder,\n",
    "    columns_name=columns_name\n",
    ")\n",
    "\n",
    "params = OmegaConf.create(params.__dict__)\n",
    "dict(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "process arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"{FN_PEPTIDE_INTENSITIES = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not experiment_folder:\n",
    "#     experiment_folder = query_subset_meta.replace('_', ' ')\n",
    "#     experiment_folder = parse_query_expression(experiment_folder)\n",
    "#     experiment_folder = experiment_folder.strip()\n",
    "#     experiment_folder = experiment_folder.replace(' ', '_')\n",
    "#     params.experiment_folder\n",
    "experiment_folder = Path(experiment_folder)\n",
    "experiment_folder.mkdir(exist_ok=True, parents=True)\n",
    "logger.info(f'Folder for output = {experiment_folder}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select M most common features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "# Use PeptideCounter instead?\n",
    "with open(Path(params.FN_PEPTIDE_FREQ)) as f:\n",
    "    freq_pep_all = Counter(json.load(f)['counter'])\n",
    "    \n",
    "selected_peptides = {k: v for k, v in freq_pep_all.most_common(params.M)}\n",
    "print(f\"No. of selected features: {len(selected_peptides):,d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "params.used_features = sorted(selected_peptides)\n",
    "if isinstance(params.index_col, str): params.used_features.insert(0, params.index_col)\n",
    "analysis = AnalyzePeptides.from_csv(fname=params.FN_PEPTIDE_INTENSITIES,\n",
    "                                     nrows=None,\n",
    "                                     index_col=params.index_col,\n",
    "                                     usecols=params.used_features\n",
    "                                    )\n",
    "analysis.df.columns.name = params.columns_name\n",
    "\n",
    "log_fct = getattr(np, params.logarithm)\n",
    "analysis.log_transform(log_fct)\n",
    "logger.info(f\"{analysis = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample file name cleaning and Sample selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename some samples\n",
    "- [ ] needs to be moved into the data extraction pipeline from the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # some date are not possible in the indices\n",
    "# rename_indices_w_wrong_dates = {'20161131_LUMOS1_nLC13_AH_MNT_HeLa_long_03': '20161130_LUMOS1_nLC13_AH_MNT_HeLa_long_03',\n",
    "#                                 '20180230_QE10_nLC0_MR_QC_MNT_Hela_12': '20180330_QE10_nLC0_MR_QC_MNT_Hela_12',\n",
    "#                                 '20161131_LUMOS1_nLC13_AH_MNT_HeLa_long_01': '20161130_LUMOS1_nLC13_AH_MNT_HeLa_long_01',\n",
    "#                                 '20180230_QE10_nLC0_MR_QC_MNT_Hela_11': '20180330_QE10_nLC0_MR_QC_MNT_Hela_11',\n",
    "#                                 '20161131_LUMOS1_nLC13_AH_MNT_HeLa_long_02': '20161130_LUMOS1_nLC13_AH_MNT_HeLa_long_02'}\n",
    "# analysis.df.rename(index=rename_indices_w_wrong_dates, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert analysis.df.index.is_unique, \"Duplicates in index\"\n",
    "analysis.df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select samples based on completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(params.MIN_SAMPLE, float):\n",
    "    msg = f'Fraction of minimum sample completeness over all features specified with: {params.MIN_SAMPLE}\\n'\n",
    "    # assumes df in wide format\n",
    "    params.MIN_SAMPLE = int(analysis.df.shape[1] * params.MIN_SAMPLE)\n",
    "    msg += f'This translates to a minimum number of total samples: {params.MIN_SAMPLE}'\n",
    "    print(msg)\n",
    "\n",
    "mask = analysis.df.notna().sum(axis=1) > params.MIN_SAMPLE\n",
    "msg = f'Drop {len(mask) - mask.sum()} of {len(mask)} initial samples.'\n",
    "print(msg)\n",
    "analysis.df = analysis.df.loc[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.used_samples = analysis.df.index.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine metadata\n",
    "\n",
    "- read from file using [ThermoRawFileParser](https://github.com/compomics/ThermoRawFileParser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = pd.read_csv('data/files_selected_metadata.csv', index_col=0)\n",
    "date_col = 'Content Creation Date'\n",
    "df_meta[date_col] = pd.to_datetime(df_meta[date_col])\n",
    "df_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_instrument = ['Thermo Scientific instrument model',\n",
    "                          'instrument attribute',\n",
    "                          'instrument serial number', ]\n",
    "df_meta.groupby(cols_instrument)[date_col].agg(['count','min','max']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_raw_selected = [\n",
    " 'Content Creation Date', \n",
    " 'Thermo Scientific instrument model',\n",
    " 'instrument serial number',\n",
    " 'Software Version', \n",
    " 'Number of MS1 spectra',\n",
    " 'Number of MS2 spectra', \n",
    " 'Number of scans',\n",
    " 'MS max charge',\n",
    " 'MS max RT',\n",
    " 'MS min MZ',\n",
    " 'MS max MZ',\n",
    " 'MS scan range', \n",
    " 'mass resolution',\n",
    " 'Retention time range',\n",
    " 'Mz range',\n",
    " 'beam-type collision-induced dissociation', \n",
    " 'injection volume setting',\n",
    " 'dilution factor',\n",
    "]\n",
    "df_meta[meta_raw_selected].describe(datetime_is_numeric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = df_meta.query(query_subset_meta) # can be based on table above\n",
    "df_meta[meta_raw_selected].describe(datetime_is_numeric=True, percentiles=np.linspace(0.05, 0.95, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set a minimum retention time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_RT_max = 120 # minutes\n",
    "msg = f\"Minimum RT time maxiumum is set to {min_RT_max} minutes (to exclude too short runs, which are potentially fractions).\"\n",
    "mask_RT = df_meta['MS max RT'] >= 120 # can be integrated into query string\n",
    "msg += f\" Total number of samples retained: {int(mask_RT.sum())}.\"\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = df_meta.loc[mask_RT]\n",
    "df_meta.sort_values(date_col, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_stats = df_meta.describe(include='all', datetime_is_numeric=True)\n",
    "meta_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "subset with variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_stats.loc[:, (meta_stats.loc['unique'] > 1) |  (meta_stats.loc['std'] > 0.1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check some columns describing settings\n",
    "  - quite some variation due to `MS max charge`: Is it a parameter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_raw_settings = [\n",
    " 'Thermo Scientific instrument model',\n",
    " 'instrument serial number',\n",
    " 'Software Version', \n",
    " 'MS max charge',\n",
    " 'mass resolution',\n",
    " 'beam-type collision-induced dissociation', \n",
    " 'injection volume setting',\n",
    " 'dilution factor',\n",
    "]\n",
    "df_meta[meta_raw_settings].drop_duplicates() # index gives first example with this combination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "view without `MS max charge`:\n",
    "  - software can be updated\n",
    "  - variation by `injection volume setting` and instrument over time\n",
    "  - missing `dilution factor`\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['MS max charge']\n",
    "df_meta[meta_raw_settings].drop(to_drop, axis=1).drop_duplicates() # index gives first example with this combination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- check for variation in `software Version` and `injection volume setting`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    analysis.df = analysis.df.loc[df_meta.index]\n",
    "except KeyError as e:\n",
    "    logger.warning(e)\n",
    "    logger.warning(\"Ignore missing samples in quantified samples\")\n",
    "    analysis.df = analysis.df.loc[analysis.df.index.intersection(df_meta.index)]\n",
    "\n",
    "analysis.df_meta = df_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive and Single plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatter plots need to become interactive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 2\n",
    "pcs = analysis.get_PCA(n_components=K) # should be renamed to get_PCs\n",
    "pcs = pcs.iloc[:,:K].join(analysis.df_meta)\n",
    "\n",
    "pcs_name = pcs.columns[:2]\n",
    "pcs = pcs.reset_index()\n",
    "pcs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18,10))\n",
    "analyzers.seaborn_scatter(pcs[pcs_name], fig, ax, meta=pcs['Thermo Scientific instrument model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    pcs, x=pcs_name[0], y=pcs_name[1],\n",
    "    hover_name='Sample ID',\n",
    "    hover_data=meta_raw_selected,\n",
    "    title=f'First two Principal Components of {analysis.M} most abundant peptides for {pcs.shape[0]} samples',\n",
    "    color=\"Thermo Scientific instrument model\",\n",
    "    width=1200,\n",
    "    height=600\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Medians and percentiles\n",
    "\n",
    "- see boxplot [function in R](https://github.dev/symbioticMe/proBatch/blob/8fae15049be67693bd0d4a4383b51bfb4fb287a6/R/initial_assessment.R#L199-L317), which is used for [publication figure](https://github.dev/symbioticMe/batch_effects_workflow_code/blob/696eb609a55ba9ece68b732e616d7ebeaa660373/AgingMice_study/analysis_AgingMice/5b_Fig2_initial_assessment_normalization.R)\n",
    "- check boxplot functions: [bokeh](https://docs.bokeh.org/en/latest/docs/gallery/boxplot.html), [plotly](https://plotly.com/python/box-plots/), [eventplot](https://matplotlib.org/stable/gallery/lines_bars_and_markers/eventplot_demo.html#sphx-glr-gallery-lines-bars-and-markers-eventplot-demo-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis.df.iloc[:100].T.boxplot(rot=90, backend=None, figsize=None)\n",
    "# analysis.df.boxplot()\n",
    "analysis.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = analysis.df\n",
    "df = df.join(df_meta[date_col])\n",
    "df = df.set_index(date_col).sort_index().to_period('min').T\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.boxplot(rot=80, figsize=(20, 10), fontsize='large', showfliers=False, showcaps=False)\n",
    "_ = vaep.plotting.select_xticks(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot sample median over time\n",
    "  - check if points are equally spaced (probably QC samples are run in close proximity)\n",
    "  - the machine will be not use for intermediate periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = df_meta[date_col].sort_values()\n",
    "dates.name = 'date'\n",
    "median_sample_intensity = (analysis.df\n",
    "                           .median(axis=1)\n",
    "                           .to_frame('median intensity'))\n",
    "median_sample_intensity = median_sample_intensity.join(dates)\n",
    "\n",
    "ax = median_sample_intensity.plot.scatter(x='date', y='median intensity', rot=90,  fontsize='large', figsize=(20, 10),\n",
    "                                          xticks=vaep.plotting.select_dates(\n",
    "                                              median_sample_intensity['date'])\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the closer the labels are there denser the samples are measured aroudn that time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split: Train, validation and to_periodata\n",
    "\n",
    "- test data is in clinical language often denoted as independent validation cohort\n",
    "- validation data (for model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.splits = DataSplits(is_wide_format=True)\n",
    "splits = analysis.splits\n",
    "print(f\"{analysis.splits = }\")\n",
    "analysis.splits.__annotations__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentiles = (0.8, 0.9)  # change here\n",
    "\n",
    "percent_str = [f'{int(x*100)}%' for x in percentiles]\n",
    "split_at_date = analysis.df_meta[date_col].describe(\n",
    "    datetime_is_numeric=True, percentiles=(0.8, 0.9)).loc[percent_str]\n",
    "split_at_date = tuple(pd.Timestamp(t.date()) for t in split_at_date)\n",
    "\n",
    "print(f\"{split_at_date[0] = }\", f\"{split_at_date[1] = }\", sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_train = analysis.df_meta[date_col] < split_at_date[0]\n",
    "analysis.splits.train_X = analysis.df.loc[idx_train]\n",
    "analysis.splits.train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_validation = ((analysis.df_meta[date_col] >= split_at_date[0]) & (\n",
    "    analysis.df_meta[date_col] < split_at_date[1]))\n",
    "analysis.splits.val_X = analysis.df.loc[idx_validation]\n",
    "analysis.splits.val_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_test = (analysis.df_meta[date_col] >= split_at_date[1])\n",
    "# analysis.df_test =\n",
    "analysis.splits.test_X = analysis.df.loc[idx_test]\n",
    "analysis.splits.test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_test_na = analysis.splits.test_X.stack(\n",
    "    dropna=False).loc[splits.test_X.isna().stack()].index\n",
    "print(f\"number of missing values in test data: {len(idx_test_na)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peptide frequency  in training data\n",
    "\n",
    "- higher count, higher probability to be sampled into training data\n",
    "- missing peptides are sampled both into training as well as into validation dataset\n",
    "- everything not in training data is validation data\n",
    "\n",
    "Based on unmodified training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis.splits.to_wide_format()\n",
    "assert analysis.splits is splits, \"Sanity check failed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_per_peptide = feature_frequency(analysis.splits.train_X)\n",
    "freq_per_peptide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = \"Total number of samples in training data split: {}\"\n",
    "print(msg.format(len(analysis.splits.train_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = experiment_folder / 'data' \n",
    "folder.mkdir(parents=True, exist_ok=True)\n",
    "freq_per_peptide.to_csv(folder / 'freq_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conserning sampling with frequency weights:\n",
    "  - larger weight -> higher probablility of being sampled\n",
    "  - weights need to be alignable to index of original DataFrame before grouping (same index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample targets (Fake NAs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add goldstandard targets for valiation and test data\n",
    "- based on same day\n",
    "- same instrument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create some target values by sampling 5% of the validation and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.splits.to_long_format(name_values='intensity') # long format as sample_data uses long-format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits.val_X, splits.val_y = sample_data(splits.val_X, sample_index_to_drop=0, weights=freq_per_peptide)\n",
    "splits.test_X, splits.test_y = sample_data(splits.test_X, sample_index_to_drop=0, weights=freq_per_peptide)\n",
    "\n",
    "for k, s in splits:\n",
    "    s.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Save in long format\n",
    "\n",
    "- Data in long format: (peptide, sample_id, intensity)\n",
    "- no missing values kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits.dump(folder=folder)  # dumps data in long-format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reload from disk\n",
    "# splits = DataSplits.from_folder(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA plot of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ana_train_X = analyzers.AnalyzePeptides(data=splits.train_X, is_wide_format=False, ind_unstack='peptide')\n",
    "figures['pca_train'] = ana_train_X.plot_pca()\n",
    "vaep.savefig(figures['pca_train'], experiment_folder / f'pca_plot_raw_data_{ana_train_X.fname_stub}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to DataSplits a inputs attribute\n",
    "\n",
    "data_dict = {'train': splits.train_X, 'valid': splits.val_X, 'test': splits.test_X}\n",
    "PCs = pd.DataFrame()\n",
    "split_map = pd.Series(dtype='string')\n",
    "for key, df in data_dict.items():\n",
    "    df = df.unstack()\n",
    "    PCs = PCs.append(ana_train_X.calculate_PCs(df))\n",
    "    split_map = split_map.append(pd.Series(key, index=df.index))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,8))\n",
    "ax.legend(title='splits')\n",
    "analyzers.seaborn_scatter(PCs.iloc[:, :2], fig, ax, meta=split_map,\n",
    "                          title='First two principal compements (based on training data PCA)')\n",
    "ax.get_legend().set_title(\"split\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For *Collaborative Filtering*, new samples could be initialized based on a KNN approach in the original sample space or the reduced PCA dimension.\n",
    "  - The sample embeddings of the K neighearst neighbours could be averaged for a new sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Change number of principal components\n",
    "# K = 2\n",
    "# _ = ana_train_X.get_PCA(n_components=K)\n",
    "\n",
    "train_PCs = ana_train_X.calculate_PCs(splits.train_X.unstack())\n",
    "test_PCs = ana_train_X.calculate_PCs(splits.test_X.unstack())\n",
    "nn = NearestNeighbors(n_neighbors=5).fit(train_PCs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select K neareast neighbors for first test data sample from training data. Compare equal distance mean to mean weighted by distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d, idx = nn.kneighbors(test_PCs.iloc[1:2])\n",
    "# test_PCs.iloc[1]\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_PCs.iloc[idx[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = d / d.sum()\n",
    "display(f\"Sample weights based on distances: {w = }\")\n",
    "w.flatten().reshape(5,1) * train_PCs.iloc[idx[0]] # apply weights to values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame( (train_PCs.iloc[idx[0]].mean(), # mean\n",
    "              (w.flatten().reshape(5,1) * train_PCs.iloc[idx[0]]).sum() # sum of weighted samples\n",
    "              ), index=['mean','weighted by distance '])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add visual representation of picked points in the first two principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.scatter(x=test_PCs.iloc[1]['PC 1'], y=test_PCs.iloc[1]['PC 2'], s=100, marker=\"v\", c='r')\n",
    "ax.scatter(x=train_PCs.iloc[idx[0]]['PC 1'], y=train_PCs.iloc[idx[0]]['PC 2'], s=100, marker=\"s\", c='y')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digression on MultiIndex: Data Selection\n",
    "\n",
    "- use mulitindex for obtaining validation split\n",
    "\n",
    "[[stackoverflow](https://stackoverflow.com/questions/53927460/select-rows-in-pandas-multiindex-dataframe), [guide](https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html)]\n",
    "\n",
    "- [`xs` method](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.xs.html) or [`pd.IndexSlice`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.IndexSlice.html?highlight=indexslice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_name_1, sample_name_2 = analysis.df_long.sample(2).index.get_level_values(-1).to_list()\n",
    "sample_name_1, sample_name_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.df_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.df_long.loc[pd.IndexSlice[:, sample_name_1], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.df_long.loc[(slice(None), sample_name_2), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with a series the syntax changes slightly (no column) and the indexing behaviour different if a string or a list is passed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = analysis.df_long.squeeze()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.loc[pd.IndexSlice[:, sample_name_2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.loc[pd.IndexSlice[:, [sample_name_2]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(OmegaConf.to_yaml(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(experiment_folder/'data_config.yaml', 'w') as f:\n",
    "    OmegaConf.save(params, f)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a3c193f136b3677f7a0d8b8f2344336d9d9fbcf8449c7b9fd96c2b5d44d3d77c"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
