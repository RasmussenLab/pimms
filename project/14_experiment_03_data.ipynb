{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 03 - Data\n",
    "\n",
    "Create data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import logging\n",
    "import typing\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from src.nb_imports import *\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import vaep.io_images\n",
    "from vaep.pandas import interpolate\n",
    "from vaep.io.datasplits import DataSplits\n",
    "from vaep.sampling import feature_frequency, frequency_by_index, sample_data\n",
    "\n",
    "import src\n",
    "from src.logging import setup_logger\n",
    "logger = setup_logger(logger=logging.getLogger('vaep'))\n",
    "logger.info(\"Experiment 03 - data\")\n",
    "\n",
    "figures = {}  # collection of ax or figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "FN_PEPTIDE_INTENSITIES: str = 'data/df_intensities_N07813_M10000.csv'  # Samples\n",
    "FN_PEPTIDE_FREQ: str = 'data/processed/count_all_peptides.json'\n",
    "M: int = 1000 # M most common features\n",
    "index_col: typing.Union[str,int] = 'Sample ID' # Can be either a string or position (typical 0 for first column)\n",
    "# query expression for subsetting\n",
    "query_subset_meta: str = 'ms_instrument in [\"QE4\", ]'\n",
    "experiment_folder: str = 'data'\n",
    "columns_name: str = 'peptide'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataConfig:\n",
    "    \"\"\"Documentation. Copy pasted arguments to a dataclass.\"\"\"\n",
    "    FN_PEPTIDE_INTENSITIES: str = 'data/df_intensities_N07285_M10000.csv'  # Samples\n",
    "    FN_PEPTIDE_FREQ: str = 'data/processed/count_all_peptides.json'\n",
    "    M: int = 1000 # M most common features\n",
    "    index_col: typing.Union[\n",
    "        str, int\n",
    "    ] = \"Sample ID\"  # Can be either a string or position (typical 0 for first column)\n",
    "    # query expression for subsetting\n",
    "    query_subset_meta: str = 'ms_instrument in [\"QE4\", ]'\n",
    "    experiment_folder: str = \"data\"\n",
    "    columns_name: str = \"peptide\"\n",
    "\n",
    "\n",
    "params = DataConfig(\n",
    "    FN_PEPTIDE_INTENSITIES=FN_PEPTIDE_INTENSITIES,\n",
    "    FN_PEPTIDE_FREQ=FN_PEPTIDE_FREQ,\n",
    "    index_col=index_col,\n",
    "    query_subset_meta=query_subset_meta,\n",
    "    experiment_folder=experiment_folder,\n",
    "    columns_name=columns_name\n",
    ")\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "# OmegaConf.create(cfg)\n",
    "params = OmegaConf.create(params.__dict__)\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "process arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"{FN_PEPTIDE_INTENSITIES = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printable = '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ '\n",
    "\n",
    "\n",
    "def parse_query_expression(s, printable=printable):\n",
    "    return ''.join(filter(lambda x: x in printable, s))\n",
    "\n",
    "\n",
    "if not experiment_folder:\n",
    "    experiment_folder = query_subset_meta.replace('_', ' ')\n",
    "    experiment_folder = parse_query_expression(experiment_folder)\n",
    "    experiment_folder = experiment_folder.strip()\n",
    "    experiment_folder = experiment_folder.replace(' ', '_')\n",
    "    params.experiment_folder\n",
    "experiment_folder = Path(experiment_folder)\n",
    "logger.info(f'Folder for output = {experiment_folder}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select M most common features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "# Use PeptideCounter instead?\n",
    "with open(Path(params.FN_PEPTIDE_FREQ)) as f:\n",
    "    freq_pep_all = Counter(json.load(f)['counter'])\n",
    "    \n",
    "selected_peptides = {k: v for k, v in freq_pep_all.most_common(M)}\n",
    "print(f\"No. of selected features: {len(selected_peptides):,d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "params.usecols = sorted(selected_peptides)\n",
    "if isinstance(params.index_col, str): params.usecols.insert(0, params.index_col)\n",
    "analysis = AnalyzePeptides.from_file(fname=params.FN_PEPTIDE_INTENSITIES,\n",
    "                                     nrows=None,\n",
    "                                     index_col=params.index_col,\n",
    "                                     usecols=params.usecols\n",
    "                                    )\n",
    "analysis.df.columns.name = columns_name\n",
    "\n",
    "analysis.log_transform(np.log2)\n",
    "logger.info(f\"{analysis = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename some samples\n",
    "- [ ] needs to be moved into the data extraction pipeline from the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# some date are not possible in the indices\n",
    "rename_indices_w_wrong_dates = {'20161131_LUMOS1_nLC13_AH_MNT_HeLa_long_03': '20161130_LUMOS1_nLC13_AH_MNT_HeLa_long_03',\n",
    "                                '20180230_QE10_nLC0_MR_QC_MNT_Hela_12': '20180330_QE10_nLC0_MR_QC_MNT_Hela_12',\n",
    "                                '20161131_LUMOS1_nLC13_AH_MNT_HeLa_long_01': '20161130_LUMOS1_nLC13_AH_MNT_HeLa_long_01',\n",
    "                                '20180230_QE10_nLC0_MR_QC_MNT_Hela_11': '20180330_QE10_nLC0_MR_QC_MNT_Hela_11',\n",
    "                                '20161131_LUMOS1_nLC13_AH_MNT_HeLa_long_02': '20161130_LUMOS1_nLC13_AH_MNT_HeLa_long_02'}\n",
    "analysis.df.rename(index=rename_indices_w_wrong_dates, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert analysis.df.index.is_unique, \"Duplicates in index\"\n",
    "analysis.df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.add_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.df_meta['datetime'] = pd.to_datetime(\n",
    "    analysis.df_meta.date, format=\"%Y/%m/%d\")  # persistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "analysis.df_meta.describe(include='all', datetime_is_numeric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.df_meta = analysis.df_meta.query(query_subset_meta)\n",
    "analysis.df_meta.describe(include='all', datetime_is_numeric=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "select proteomics data based on meta data query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.df = analysis.df.loc[analysis.df_meta.index]\n",
    "analysis.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA plot of raw data\n",
    "- biological stock differences in PCA plot. Show differences in models. Only see biological variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = analysis.plot_pca()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaep.io_images._savefig(\n",
    "    fig, f'pca_plot_raw_data_{analysis.fname_stub}', folder=experiment_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatter plots need to become interactive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split: Train, validation and test data\n",
    "\n",
    "- test data is in clinical language often denoted as independent validation cohort\n",
    "- validation data (for model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.splits = DataSplits(is_wide_format=True)\n",
    "splits = analysis.splits\n",
    "print(f\"{analysis.splits = }\")\n",
    "analysis.splits.__annotations__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentiles = (0.8, 0.9)  # change here\n",
    "\n",
    "percent_str = [f'{int(x*100)}%' for x in percentiles]\n",
    "split_at_date = analysis.df_meta['datetime'].describe(\n",
    "    datetime_is_numeric=True, percentiles=(0.8, 0.9)).loc[percent_str]\n",
    "split_at_date = tuple(pd.Timestamp(t.date()) for t in split_at_date)\n",
    "\n",
    "print(f\"{split_at_date[0] = }\", f\"{split_at_date[1] = }\", sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_train = analysis.df_meta['datetime'] < split_at_date[0]\n",
    "analysis.splits.train_X = analysis.df.loc[idx_train]\n",
    "analysis.splits.train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_validation = ((analysis.df_meta['datetime'] >= split_at_date[0]) & (\n",
    "    analysis.df_meta['datetime'] < split_at_date[1]))\n",
    "analysis.splits.val_X = analysis.df.loc[idx_validation]\n",
    "analysis.splits.val_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_test = (analysis.df_meta['datetime'] >= split_at_date[1])\n",
    "# analysis.df_test =\n",
    "analysis.splits.test_X = analysis.df.loc[idx_test]\n",
    "analysis.splits.test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_test_na = analysis.splits.test_X.stack(\n",
    "    dropna=False).loc[splits.test_X.isna().stack()].index\n",
    "print(f\"number of missing values in test data: {len(idx_test_na)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peptide frequency  in training data\n",
    "\n",
    "- higher count, higher probability to be sampled into training data\n",
    "- missing peptides are sampled both into training as well as into validation dataset\n",
    "- everything not in training data is validation data\n",
    "\n",
    "Based on unmodified training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis.splits.to_wide_format()\n",
    "assert analysis.splits is splits, \"Sanity check failed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_per_peptide = feature_frequency(analysis.splits.train_X)\n",
    "freq_per_peptide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = \"Total number of samples in training data split: {}\"\n",
    "print(msg.format(len(analysis.splits.train_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_per_peptide.to_csv(experiment_folder /'data' / 'freq_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conserning sampling with frequency weights:\n",
    "  - larger weight -> higher probablility of being sampled\n",
    "  - weights need to be alignable to index of original DataFrame before grouping (same index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample targets (Fake NAs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add goldstandard targets for valiation and test data\n",
    "- based on same day\n",
    "- same instrument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create some target values by sampling 5% of the validation and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.splits.to_long_format(name_values='intensity') # long format as sample_data uses long-format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits.val_X, splits.val_y = sample_data(splits.val_X, sample_index_to_drop=0, weights=freq_per_peptide)\n",
    "splits.test_X, splits.test_y = sample_data(splits.test_X, sample_index_to_drop=0, weights=freq_per_peptide)\n",
    "\n",
    "for k, s in splits:\n",
    "    s.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Save in long format\n",
    "\n",
    "- Data in long format: (peptide, sample_id, intensity)\n",
    "- no missing values kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = experiment_folder / 'data'# possibly avoid duplication?\n",
    "\n",
    "splits.dump(folder=folder)  # dumps data in long-format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reload from disk\n",
    "# splits = DataSplits.from_folder(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA plot of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ana_train_X = analyzers.AnalyzePeptides(data=splits.train_X, is_wide_format=False, ind_unstack='peptide')\n",
    "figures['pca_train'] = ana_train_X.plot_pca()\n",
    "vaep.savefig(figures['pca_train'], experiment_folder / f'pca_plot_raw_data_{ana_train_X.fname_stub}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to DataSplits a inputs attribute\n",
    "\n",
    "data_dict = {'train': splits.train_X, 'valid': splits.val_X, 'test': splits.test_X}\n",
    "PCs = pd.DataFrame()\n",
    "split_map = pd.Series(dtype='string')\n",
    "for key, df in data_dict.items():\n",
    "    df = df.unstack()\n",
    "    PCs = PCs.append(ana_train_X.calculate_PCs(df))\n",
    "    split_map = split_map.append(pd.Series(key, index=df.index))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,8))\n",
    "ax.legend(title='splits')\n",
    "analyzers.seaborn_scatter(PCs.iloc[:, :2], fig, ax, meta=split_map,\n",
    "                          title='First two principal compements (based on training data PCA)')\n",
    "ax.get_legend().set_title(\"split\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For *Collaborative Filtering*, new samples could be initialized based on a KNN approach in the original sample space or the reduced PCA dimension.\n",
    "  - The sample embeddings of the K neighearst neighbours could be averaged for a new sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Change number of principal components\n",
    "# K = 2\n",
    "# _ = ana_train_X.get_PCA(n_components=K)\n",
    "\n",
    "train_PCs = ana_train_X.calculate_PCs(splits.train_X.unstack())\n",
    "test_PCs = ana_train_X.calculate_PCs(splits.test_X.unstack())\n",
    "nn = NearestNeighbors(n_neighbors=5).fit(train_PCs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select K neareast neighbors for first test data sample from training data. Compare equal distance mean to mean weighted by distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d, idx = nn.kneighbors(test_PCs.iloc[1:2])\n",
    "# test_PCs.iloc[1]\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_PCs.iloc[idx[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = d / d.sum()\n",
    "display(f\"Sample weights based on distances: {w = }\")\n",
    "w.flatten().reshape(5,1) * train_PCs.iloc[idx[0]] # apply weights to values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame( (train_PCs.iloc[idx[0]].mean(), # mean\n",
    "              (w.flatten().reshape(5,1) * train_PCs.iloc[idx[0]]).sum() # sum of weighted samples\n",
    "              ), index=['mean','weighted by distance '])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add visual representation of picked points in the first two principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.scatter(x=test_PCs.iloc[1]['PC 1'], y=test_PCs.iloc[1]['PC 2'], s=100, marker=\"v\", c='r')\n",
    "ax.scatter(x=train_PCs.iloc[idx[0]]['PC 1'], y=train_PCs.iloc[idx[0]]['PC 2'], s=100, marker=\"s\", c='y')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digression on MultiIndex: Data Selection\n",
    "\n",
    "- use mulitindex for obtaining validation split\n",
    "\n",
    "[[stackoverflow](https://stackoverflow.com/questions/53927460/select-rows-in-pandas-multiindex-dataframe), [guide](https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html)]\n",
    "\n",
    "- [`xs` method](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.xs.html) or [`pd.IndexSlice`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.IndexSlice.html?highlight=indexslice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_name_1, sample_name_2 = analysis.df_long.sample(2).index.get_level_values(-1).to_list()\n",
    "sample_name_1, sample_name_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.df_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.df_long.loc[pd.IndexSlice[:, sample_name_1], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.df_long.loc[(slice(None), sample_name_2), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with a series the syntax changes slightly (no column) and the indexing behaviour different if a string or a list is passed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = analysis.df_long.squeeze()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.loc[pd.IndexSlice[:, sample_name_2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.loc[pd.IndexSlice[:, [sample_name_2]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(OmegaConf.to_yaml(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(experiment_folder/'data_config.yaml', 'w') as f:\n",
    "    OmegaConf.save(params, f)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ca718f398b3a596c3df6787ca2afa269ec54c58eb9478d66aeb41db8e6cb8262"
  },
  "kernelspec": {
   "display_name": "vaep",
   "language": "python",
   "name": "vaep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
