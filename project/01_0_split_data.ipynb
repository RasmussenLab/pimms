{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 03 - Data\n",
    "\n",
    "Create data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import List, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import vaep\n",
    "import vaep.io.load\n",
    "from vaep.analyzers import analyzers\n",
    "from vaep.io.datasplits import DataSplits\n",
    "from vaep.sampling import feature_frequency\n",
    "from vaep.sklearn import get_PCA\n",
    "\n",
    "logger = vaep.logging.setup_nb_logger()\n",
    "logger.info(\"Split data and make diagnostic plots\")\n",
    "logging.getLogger('fontTools').setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "def align_meta_data(df: pd.DataFrame, df_meta: pd.DataFrame):\n",
    "    try:\n",
    "        df = df.loc[df_meta.index]\n",
    "    except KeyError as e:\n",
    "        logger.warning(e)\n",
    "        logger.warning(\"Ignore missing samples in quantified samples\")\n",
    "        df = df.loc[df.index.intersection(\n",
    "            df_meta.index)]\n",
    "    return df_meta\n",
    "\n",
    "\n",
    "pd.options.display.max_columns = 32\n",
    "plt.rcParams['figure.figsize'] = [4, 2]\n",
    "\n",
    "vaep.plotting.make_large_descriptors(7)\n",
    "\n",
    "figures = {}  # collection of ax or figures\n",
    "dumps = {}  # collection of data dumps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# catch passed parameters\n",
    "args = None\n",
    "args = dict(globals()).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "FN_INTENSITIES: str = 'data/dev_datasets/HeLa_6070/protein_groups_wide_N50.csv'  # Sample (rows), features (columns)\n",
    "index_col: Union[str, int] = 0  # Can be either a string or position (default 0 for first column), or a list of these.\n",
    "column_names: List[str] = [\"Gene Names\"]  # Manuelly set column names (of Index object in columns)\n",
    "fn_rawfile_metadata: str = 'data/dev_datasets/HeLa_6070/files_selected_metadata_N50.csv'  # metadata for samples (rows)\n",
    "feat_prevalence: Union[int, float] = 0.25  # Minimum number or fraction of feature prevalence across samples to be kept\n",
    "sample_completeness: Union[int, float] = 0.5  # Minimum number or fraction of total requested features per Sample\n",
    "select_N: int = None  # only use latest N samples\n",
    "sample_N: bool = False  # if select_N, sample N randomly instead of using latest N\n",
    "random_state: int = 42  # random state for reproducibility of splits\n",
    "logarithm: str = 'log2'  # Log transformation of initial data (select one of the existing in numpy)\n",
    "folder_experiment: str = 'runs/example'  # folder to save figures and data dumps\n",
    "folder_data: str = ''  # specify special data directory if needed\n",
    "file_format: str = 'csv'  # file format of create splits, default pickle (pkl)\n",
    "use_every_nth_xtick: int = 1  # use every nth xtick in plots (default 1, i.e. every xtick is kept)\n",
    "# metadata -> defaults for metadata extracted from machine data, used for plotting\n",
    "meta_date_col: str = None  # date column in meta data\n",
    "meta_cat_col: str = None  # category column in meta data\n",
    "# train, validation and test data splits\n",
    "frac_non_train: float = 0.1  # fraction of non training data (validation and test split)\n",
    "frac_mnar: float = 0.0  # fraction of missing not at random data, rest: missing completely at random\n",
    "prop_sample_w_sim: float = 1.0  # proportion of samples with simulated missing values\n",
    "feat_name_display: str = None  # display name for feature name (e.g. 'protein group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = vaep.nb.get_params(args, globals=globals())\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = vaep.nb.args_from_dict(args)\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not 0.0 <= args.frac_mnar <= 1.0:\n",
    "    raise ValueError(\"Invalid MNAR float value (should be betw. 0 and 1):\"\n",
    "                     f\" {args.frac_mnar}\")\n",
    "\n",
    "if isinstance(args.index_col, str) or isinstance(args.index_col, int):\n",
    "    args.overwrite_entry('index_col', [args.index_col])\n",
    "args.index_col  # make sure it is an iterable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "process arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"{args.FN_INTENSITIES = }\")\n",
    "\n",
    "\n",
    "FILE_FORMAT_TO_CONSTRUCTOR = {'csv': 'from_csv',\n",
    "                              'pkl': 'from_pickle',\n",
    "                              'pickle': 'from_pickle',\n",
    "                              }\n",
    "\n",
    "FILE_EXT = Path(args.FN_INTENSITIES).suffix[1:]\n",
    "logger.info(\n",
    "    f\"File format (extension): {FILE_EXT}  (!specifies data loading function!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! factor out file reading to a separate module, not class\n",
    "# AnalyzePeptides.from_csv\n",
    "constructor = getattr(vaep.io.load, FILE_FORMAT_TO_CONSTRUCTOR[FILE_EXT])\n",
    "df = constructor(fname=args.FN_INTENSITIES,\n",
    "                 index_col=args.index_col,\n",
    "                 )\n",
    "if args.column_names:\n",
    "    df.columns.names = args.column_names\n",
    "if args.feat_name_display is None:\n",
    "    args.overwrite_entry('feat_name_display', 'features')\n",
    "    if args.column_names:\n",
    "        args.overwrite_entry('feat_name_display', args.column_names[0])\n",
    "\n",
    "\n",
    "if not df.index.name:\n",
    "    logger.warning(\"No sample index name found, setting to 'Sample ID'\")\n",
    "    df.index.name = 'Sample ID'\n",
    "\n",
    "if args.logarithm:\n",
    "    log_fct = getattr(np, args.logarithm)\n",
    "    df = log_fct(df)  # ! potentially add check to increase value by 1 if 0 is present (should be part of preprocessing)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "ax = (df\n",
    "      .notna()\n",
    "      .sum(axis=0)\n",
    "      .to_frame(df.columns.name)\n",
    "      .plot\n",
    "      .box()\n",
    "      )\n",
    "ax.set_ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = args.out_folder / '01_0_data_stats.xlsx'\n",
    "dumps[fname.name] = fname.as_posix()\n",
    "writer = pd.ExcelWriter(fname)\n",
    "\n",
    "notna = df.notna()\n",
    "data_stats_original = pd.concat(\n",
    "    [\n",
    "        notna.sum().describe().rename('feat_stats'),\n",
    "        notna.sum(axis=1).describe().rename('sample_stats')\n",
    "    ],\n",
    "    axis=1)\n",
    "data_stats_original.to_excel(writer, sheet_name='data_stats_original')\n",
    "data_stats_original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case there are multiple features for each intensity values (currenlty: peptide sequence and charge),\n",
    "combine the column names to a single str index.\n",
    "\n",
    "> The Collaborative Modeling approach will need a single feature column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_as_str(seq):\n",
    "    ret = \"_\".join(str(x) for x in seq)\n",
    "    return ret\n",
    "\n",
    "\n",
    "if isinstance(df.columns, pd.MultiIndex):\n",
    "    logger.warning(\"combine MultiIndex columns to one feature column\")\n",
    "    print(df.columns[:10].map(join_as_str))\n",
    "    _new_name = join_as_str(df.columns.names)\n",
    "    df.columns = df.columns.map(join_as_str)\n",
    "    df.columns.name = _new_name\n",
    "    logger.warning(f\"New name: {df.columns.names = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine metadata\n",
    "\n",
    "- read from file using [ThermoRawFileParser](https://github.com/compomics/ThermoRawFileParser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.fn_rawfile_metadata:\n",
    "    df_meta = pd.read_csv(args.fn_rawfile_metadata, index_col=0)\n",
    "else:\n",
    "    logger.warning(\"No metadata for samples provided, create placeholder.\")\n",
    "    if args.meta_date_col:\n",
    "        raise ValueError(\n",
    "            f\"No metadata provided, but data column set: {args.meta_date_col}\")\n",
    "    if args.meta_cat_col:\n",
    "        raise ValueError(\n",
    "            f\"No metadata provided, but data column set: {args.meta_cat_col}\")\n",
    "    df_meta = pd.DataFrame(index=df.index)\n",
    "df_meta = df_meta.loc[df.index.to_list()]  # index is sample index\n",
    "if df_meta.index.name is None:\n",
    "    df_meta.index.name = args.index_col[0]\n",
    "df_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if args.meta_date_col:\n",
    "    df_meta[args.meta_date_col] = pd.to_datetime(\n",
    "        df_meta[args.meta_date_col])\n",
    "else:\n",
    "    args.overwrite_entry('meta_date_col', 'PlaceholderTime')\n",
    "    df_meta[args.meta_date_col] = range(len(df_meta))\n",
    "df_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta.describe(percentiles=np.linspace(0.05, 0.95, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = df_meta.sort_values(args.meta_date_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_stats = df_meta.describe(include='all')\n",
    "meta_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "subset with variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    display(meta_stats.loc[:, (meta_stats.loc['unique']\n",
    "            > 1) | (meta_stats.loc['std'] > 0.1)])\n",
    "except KeyError:\n",
    "    if 'std' in meta_stats.index:\n",
    "        display(meta_stats.loc[:, (meta_stats.loc['std'] > 0.1)])\n",
    "    if 'unique' in meta_stats.index:\n",
    "        display(meta_stats.loc[:, (meta_stats.loc['std'] > 0.1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = align_meta_data(df, df_meta=df_meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure unique indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df.index.is_unique, \"Duplicates in index.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select a subset of samples if specified (reduce the number of samples)\n",
    "\n",
    "- select features if `select_N` is specifed (for now)\n",
    "- for interpolation to make sense, it is best to select a consecutive number of samples:\n",
    "  - take N most recent samples (-> check that this makes sense for your case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.select_N is not None:\n",
    "    args.select_N = min(args.select_N, len(df_meta))\n",
    "    if args.sample_N:\n",
    "        df_meta = df_meta.sample(args.select_N)\n",
    "    else:\n",
    "        df_meta = df_meta.iloc[-args.select_N:]\n",
    "\n",
    "    df = df.loc[df_meta.index].dropna(\n",
    "        how='all', axis=1)\n",
    "    ax = df.T.describe().loc['count'].hist()\n",
    "    _ = ax.set_title('histogram of features for all eligable samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## First Step: Select features by prevalence\n",
    "- `feat_prevalence` across samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! add function\n",
    "freq_per_feature = df.notna().sum()  # on wide format\n",
    "if isinstance(args.feat_prevalence, float):\n",
    "    N_samples = len(df)\n",
    "    logger.info(f\"Current number of samples: {N_samples}\")\n",
    "    logger.info(\n",
    "        f\"Feature has to be present in at least {args.feat_prevalence:.2%} of samples\")\n",
    "    args.overwrite_entry('feat_prevalence', int(\n",
    "        N_samples * args.feat_prevalence))\n",
    "assert isinstance(args.feat_prevalence, int)\n",
    "! check that feature prevalence is greater equal to 3 (otherwise train, val, test split is not possible)\n",
    "logger.info(\n",
    "    f\"Feature has to be present in at least {args.feat_prevalence} of samples\")\n",
    "# select features\n",
    "mask = freq_per_feature >= args.feat_prevalence\n",
    "logger.info(f\"Drop {(~mask).sum()} features\")\n",
    "freq_per_feature = freq_per_feature.loc[mask]\n",
    "df = df.loc[:, mask]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notna = df.notna()\n",
    "data_stats_filtered = pd.concat(\n",
    "    [\n",
    "        notna.sum().describe().rename('feat_stats'),\n",
    "        notna.sum(axis=1).describe().rename('sample_stats')\n",
    "    ],\n",
    "    axis=1)\n",
    "data_stats_filtered.to_excel(writer, sheet_name='data_stats_filtered')\n",
    "data_stats_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second step - Sample selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select samples based on completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(args.sample_completeness, float):\n",
    "    msg = f'Fraction of minimum sample completeness over all features specified with: {args.sample_completeness}\\n'\n",
    "    # assumes df in wide format\n",
    "    args.overwrite_entry('sample_completeness', int(\n",
    "        df.shape[1] * args.sample_completeness))\n",
    "    msg += f'This translates to a minimum number of features per sample (to be included): {args.sample_completeness}'\n",
    "    logger.info(msg)\n",
    "\n",
    "sample_counts = df.notna().sum(axis=1)  # if DataFrame\n",
    "sample_counts.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = sample_counts > args.sample_completeness\n",
    "msg = f'Drop {len(mask) - mask.sum()} of {len(mask)} initial samples.'\n",
    "logger.info(msg)\n",
    "df = df.loc[mask]\n",
    "df = df.dropna(\n",
    "    axis=1, how='all')  # drop now missing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.N, args.M = df.shape  # save data dimensions\n",
    "args.used_samples = df.index.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of features per sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = 1\n",
    "ax = df.notna().sum(axis=1).hist()\n",
    "ax.set_xlabel(f'{args.feat_name_display.capitalize()} per eligable sample')\n",
    "ax.set_ylabel('observations')\n",
    "fname = args.out_figures / f'0_{group}_hist_features_per_sample'\n",
    "figures[fname.stem] = fname\n",
    "vaep.savefig(ax.get_figure(), fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "ax = df.notna().sum(axis=0).sort_values().plot()\n",
    "_new_labels = [l_.get_text().split(';')[0] for l_ in ax.get_xticklabels()]\n",
    "_ = ax.set_xticklabels(_new_labels, rotation=45,\n",
    "                       horizontalalignment='right')\n",
    "ax.set_xlabel(f'{args.feat_name_display.capitalize()} prevalence')\n",
    "ax.set_ylabel('observations')\n",
    "fname = args.out_figures / f'0_{group}_feature_prevalence'\n",
    "figures[fname.stem] = fname\n",
    "vaep.savefig(ax.get_figure(), fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number off observations accross feature value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max = vaep.plotting.data.min_max(df.stack())\n",
    "ax, bins = vaep.plotting.data.plot_histogram_intensities(\n",
    "    df.stack(), min_max=min_max)\n",
    "ax.set_xlabel('Intensity binned')\n",
    "fname = args.out_figures / f'0_{group}_intensity_distribution_overall'\n",
    "\n",
    "figures[fname.stem] = fname\n",
    "vaep.savefig(ax.get_figure(), fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = vaep.plotting.data.plot_feat_median_over_prop_missing(\n",
    "    data=df, type='scatter')\n",
    "fname = args.out_figures / f'0_{group}_intensity_median_vs_prop_missing_scatter'\n",
    "ax.set_xlabel(\n",
    "    f'{args.feat_name_display.capitalize()} binned by their median intensity'\n",
    "    f' (N {args.feat_name_display})')\n",
    "figures[fname.stem] = fname\n",
    "vaep.savefig(ax.get_figure(), fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax, _data_feat_median_over_prop_missing = vaep.plotting.data.plot_feat_median_over_prop_missing(\n",
    "    data=df, type='boxplot', return_plot_data=True)\n",
    "fname = args.out_figures / f'0_{group}_intensity_median_vs_prop_missing_boxplot'\n",
    "ax.set_xlabel(\n",
    "    f'{args.feat_name_display.capitalize()} binned by their median intensity'\n",
    "    f' (N {args.feat_name_display})')\n",
    "figures[fname.stem] = fname\n",
    "vaep.savefig(ax.get_figure(), fname)\n",
    "_data_feat_median_over_prop_missing.to_csv(fname.with_suffix('.csv'))\n",
    "# _data_feat_median_over_prop_missing.to_excel(fname.with_suffix('.xlsx'))\n",
    "del _data_feat_median_over_prop_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive and Single plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_feature_display_name = f'identified {args.feat_name_display}'\n",
    "sample_counts.name = _feature_display_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 2\n",
    "df = df.astype(float)\n",
    "pcs = get_PCA(df, n_components=K)  # should be renamed to get_PCs\n",
    "pcs = pcs.iloc[:, :K].join(df_meta).join(sample_counts)\n",
    "\n",
    "pcs_name = pcs.columns[:2]\n",
    "pcs_index_name = pcs.index.name\n",
    "pcs = pcs.reset_index()\n",
    "pcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs.describe(include='all').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.meta_cat_col:\n",
    "    fig, ax = plt.subplots(figsize=(3, 3))\n",
    "    analyzers.seaborn_scatter(\n",
    "        pcs[pcs_name], ax, meta=pcs[args.meta_cat_col], title=f\"by {args.meta_cat_col}\")\n",
    "    fname = (args.out_figures\n",
    "             / f'0_{group}_pca_sample_by_{\"_\".join(args.meta_cat_col.split())}')\n",
    "    figures[fname.stem] = fname\n",
    "    vaep.savefig(fig, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.meta_date_col != 'PlaceholderTime':\n",
    "    fig, ax = plt.subplots()\n",
    "    analyzers.plot_date_map(\n",
    "        df=pcs[pcs_name], ax=ax, dates=pcs[args.meta_date_col], title=f'by {args.meta_date_col}')\n",
    "    fname = args.out_figures / f'0_{group}_pca_sample_by_date'\n",
    "    figures[fname.stem] = fname\n",
    "    vaep.savefig(fig, fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- size: number of features in a single sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "col_identified_feat = _feature_display_name\n",
    "analyzers.plot_scatter(\n",
    "    pcs[pcs_name],\n",
    "    ax,\n",
    "    pcs[col_identified_feat],\n",
    "    feat_name_display=args.feat_name_display,\n",
    "    size=5,\n",
    ")\n",
    "fname = (args.out_figures\n",
    "         / f'0_{group}_pca_sample_by_{\"_\".join(col_identified_feat.split())}.pdf')\n",
    "figures[fname.stem] = fname\n",
    "vaep.savefig(fig, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! write principal components to excel (if needed)\n",
    "# pcs.set_index([df.index.name])[[*pcs_name, col_identified_feat]].to_excel(fname.with_suffix('.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    pcs, x=pcs_name[0], y=pcs_name[1],\n",
    "    hover_name=pcs_index_name,\n",
    "    # hover_data=analysis.df_meta,\n",
    "    title=f'First two Principal Components of {args.M} {args.feat_name_display} for {pcs.shape[0]} samples',\n",
    "    # color=pcs['Software Version'],\n",
    "    color=col_identified_feat,\n",
    "    template='none',\n",
    "    width=1200,  # 4 inches x 300 dpi\n",
    "    height=600  # 2 inches x 300 dpi\n",
    ")\n",
    "fname = (args.out_figures\n",
    "         / f'0_{group}_pca_sample_by_{\"_\".join(col_identified_feat.split())}_plotly.pdf')\n",
    "figures[fname.stem] = fname\n",
    "fig.write_image(fname)\n",
    "fig  # stays interactive in html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Medians and percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w_date = df.join(df_meta[args.meta_date_col])\n",
    "df_w_date = df_w_date.set_index(args.meta_date_col).sort_index()\n",
    "if not args.meta_date_col == 'PlaceholderTime':\n",
    "    df_w_date.to_period('min')\n",
    "df_w_date = df_w_date.T\n",
    "df_w_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_w_date.plot.box(rot=80,\n",
    "                        figsize=(7, 3),\n",
    "                        fontsize=7,\n",
    "                        showfliers=False,\n",
    "                        showcaps=False,\n",
    "                        boxprops=dict(linewidth=.4, color='darkblue'),\n",
    "                        flierprops=dict(markersize=.4, color='lightblue'),\n",
    "                        )\n",
    "_ = vaep.plotting.select_xticks(ax)\n",
    "fig = ax.get_figure()\n",
    "fname = args.out_figures / f'0_{group}_median_boxplot'\n",
    "df_w_date.to_pickle(fname.with_suffix('.pkl'))\n",
    "figures[fname.stem] = fname\n",
    "vaep.savefig(fig, fname)\n",
    "del df_w_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percentiles of intensities in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.stack().describe(percentiles=np.linspace(0.05, 0.95, 19).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot sample median over time\n",
    "  - check if points are equally spaced (probably QC samples are run in close proximity)\n",
    "  - the machine will be not use for intermediate periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not args.meta_date_col == 'PlaceholderTime':\n",
    "    dates = df_meta[args.meta_date_col].sort_values()\n",
    "    median_sample_intensity = (df\n",
    "                               .median(axis=1)\n",
    "                               .to_frame('median intensity'))\n",
    "    median_sample_intensity = median_sample_intensity.join(dates)\n",
    "\n",
    "    ax = median_sample_intensity.plot.scatter(x=dates.name, y='median intensity',\n",
    "                                              rot=90,\n",
    "                                              #   fontsize=6,\n",
    "                                              figsize=(8, 2),\n",
    "                                              s=5,\n",
    "                                              xticks=vaep.plotting.select_dates(\n",
    "                                                  median_sample_intensity[dates.name])\n",
    "                                              )\n",
    "    fig = ax.get_figure()\n",
    "    fname = args.out_figures / f'0_{group}_median_scatter'\n",
    "    figures[fname.stem] = fname\n",
    "    vaep.savefig(fig, fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the closer the labels are there denser the samples are measured around that time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature frequency  in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "msg = \"Total number of samples in data: {}\"\n",
    "logger.info(msg.format(len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recalculate feature frequency after selecting samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_per_feature = feature_frequency(df)\n",
    "freq_per_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# freq_per_feature.name = 'Gene names freq' # name it differently?\n",
    "# index.name is lost when data is stored\n",
    "fname = args.data / 'freq_features.json'\n",
    "dumps[fname.name] = fname\n",
    "freq_per_feature.to_json(fname)\n",
    "fname = fname.with_suffix('.pkl')\n",
    "dumps[fname.name] = fname\n",
    "freq_per_feature.to_pickle(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split: Train, validation and test data\n",
    "\n",
    "Select features as described in\n",
    "> Lazar, Cosmin, Laurent Gatto, Myriam Ferro, Christophe Bruley, and Thomas Burger. 2016.\n",
    "> “Accounting for the Multiple Natures of Missing Values in Label-Free Quantitative\n",
    "> Proteomics Data Sets to Compare Imputation Strategies.”\n",
    "> Journal of Proteome Research 15 (4): 1116–25.\n",
    "\n",
    "- select `frac_mnar` based on threshold matrix on quantile of overall frac of data to be used\n",
    "  for validation and test data split, e.g. 0.1 = quantile(0.1)\n",
    "- select frac_mnar from intensities selected using threshold matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "splits = DataSplits(is_wide_format=False)\n",
    "logger.info(f\"{splits = }\")\n",
    "splits.__annotations__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create some target values by sampling X% of the validation and test data.\n",
    "Simulated missing values are not used for validation and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long = vaep.io.datasplits.long_format(df)\n",
    "df_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = 2\n",
    "\n",
    "splits, thresholds, fake_na_mcar, fake_na_mnar = vaep.sampling.sample_mnar_mcar(\n",
    "    df_long=df_long,\n",
    "    frac_non_train=args.frac_non_train,\n",
    "    frac_mnar=args.frac_mnar,\n",
    "    random_state=args.random_state,\n",
    ")\n",
    "logger.info(f\"{splits.train_X.shape = } - {splits.val_y.shape = } - {splits.test_y.shape = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "N = len(df_long)\n",
    "N_MCAR = len(fake_na_mcar)\n",
    "N_MNAR = len(fake_na_mnar)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(6, 2))\n",
    "ax = axes[0]\n",
    "plot_histogram_intensities = partial(vaep.plotting.data.plot_histogram_intensities,\n",
    "                                     min_max=min_max,\n",
    "                                     alpha=0.8)\n",
    "plot_histogram_intensities(\n",
    "    df_long.squeeze(),\n",
    "    ax=ax,\n",
    "    label='observed')\n",
    "plot_histogram_intensities(\n",
    "    thresholds,\n",
    "    ax=ax,\n",
    "    label='thresholds')\n",
    "if args.use_every_nth_xtick > 1:\n",
    "    ax.set_xticks(ax.get_xticks()[::2])\n",
    "ax.legend()\n",
    "ax = axes[1]\n",
    "plot_histogram_intensities(\n",
    "    fake_na_mnar.squeeze(),\n",
    "    ax=ax,\n",
    "    label=f'MNAR ({N_MNAR:,d})',\n",
    "    color='C2')\n",
    "plot_histogram_intensities(\n",
    "    fake_na_mcar.squeeze(),\n",
    "    ax=ax,\n",
    "    color='C3',\n",
    "    label=f'MCAR ({N_MCAR:,d})')\n",
    "if args.use_every_nth_xtick > 1:\n",
    "    ax.set_xticks(ax.get_xticks()[::2])\n",
    "ax.legend()\n",
    "fname = args.out_figures / f'0_{group}_mnar_mcar_histograms.pdf'\n",
    "figures[fname.stem] = fname\n",
    "vaep.savefig(fig, fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep simulated samples only in a subset of the samples\n",
    "In case only a subset of the samples should be used for validation and testing,\n",
    "although these samples can be used for fitting the models,\n",
    "the following cell will select samples stratified by the eventually set `meta_cat_col` column.\n",
    "\n",
    "The procedure is experimental and turned off by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0.0 < args.prop_sample_w_sim < 1.0:\n",
    "    to_stratify = None\n",
    "    if args.meta_cat_col and df_meta is not None:\n",
    "        to_stratify = df_meta[args.meta_cat_col].fillna(-1)  # ! fillna with -1 as separate category (sofisticate check)\n",
    "    train_idx, val_test_idx = train_test_split(splits.train_X.index.levels[0],\n",
    "                                               test_size=args.prop_sample_w_sim,\n",
    "                                               stratify=to_stratify,\n",
    "                                               random_state=42)\n",
    "    val_idx, test_idx = train_test_split(val_test_idx,\n",
    "                                         test_size=.5,\n",
    "                                         stratify=to_stratify.loc[val_test_idx] if to_stratify is not None else None,\n",
    "                                         random_state=42)\n",
    "    logger.info(f\"Sample in Train: {len(train_idx):,d} - Validation: {len(val_idx):,d} - Test: {len(test_idx):,d}\")\n",
    "    # reassign some simulated missing values to training data:\n",
    "    splits.train_X = pd.concat(\n",
    "        [splits.train_X,\n",
    "         splits.val_y.loc[train_idx],\n",
    "         splits.test_y.loc[train_idx]\n",
    "         ])\n",
    "    splits.val_y = splits.val_y.loc[val_idx]\n",
    "    splits.test_y = splits.test_y.loc[test_idx]\n",
    "    logger.info(f\"New shapes: {splits.train_X.shape = } - {splits.val_y.shape = } - {splits.test_y.shape = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits.test_y.groupby(level=-1).count().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits.val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits.train_X.groupby(level=-1).count().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that feature indices and sample indicies overlap between splits\n",
    "# -> a single feature cannot be only in the validation or test split\n",
    "# -> single features should be put into the training data\n",
    "# -> or raise error as feature completness treshold is so low that less than 3 samples\n",
    "# per feature are allowd.\n",
    "\n",
    "diff = (splits\n",
    "        .val_y\n",
    "        .index\n",
    "        .levels[-1]\n",
    "        .difference(splits\n",
    "                    .train_X\n",
    "                    .index\n",
    "                    .levels[-1]\n",
    "                    ).to_list())\n",
    "if diff:\n",
    "    to_remove = splits.val_y.loc[pd.IndexSlice[:, diff]]\n",
    "    display(to_remove)\n",
    "    splits.train_X = pd.concat([splits.train_X, to_remove])\n",
    "    splits.val_y = splits.val_y.drop(to_remove.index)\n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = (splits\n",
    "        .test_y\n",
    "        .index\n",
    "        .levels[-1]\n",
    "        .difference(splits\n",
    "                    .train_X\n",
    "                    .index\n",
    "                    .levels[-1]\n",
    "                    ).to_list())\n",
    "if diff:\n",
    "    to_remove = splits.test_y.loc[pd.IndexSlice[:, diff]]\n",
    "    display(to_remove)\n",
    "    splits.train_X = pd.concat([splits.train_X, to_remove])\n",
    "    splits.test_y = splits.test_y.drop(to_remove.index)\n",
    "diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some tools require at least 4 observation in the training data,\n",
    "which is a good requirment. Due to \"MNAR\" sampling, most measurments\n",
    "of a features can end up in the validation or test data.\n",
    "\n",
    "In that case: Move the validation measurments back to the training data.\n",
    "If after this procedure the condition is still not met, a value error is raised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_min_4_measurments = splits.train_X.groupby(level=1).count() < 4\n",
    "if mask_min_4_measurments.any():\n",
    "    idx = mask_min_4_measurments.loc[mask_min_4_measurments].index\n",
    "    logger.warning(f\"Features with less than 4 measurments in training data: {idx.to_list()}\")\n",
    "    to_remove = splits.val_y.loc[pd.IndexSlice[:, idx]]\n",
    "    logger.info(\"To remove from validation data: \")\n",
    "    display(to_remove)\n",
    "    splits.train_X = pd.concat([splits.train_X, to_remove])\n",
    "    splits.val_y = splits.val_y.drop(to_remove.index)\n",
    "    # check condition again\n",
    "    mask_min_4_measurments = splits.train_X.groupby(level=1).count() < 4\n",
    "    if mask_min_4_measurments.any():\n",
    "        idx = mask_min_4_measurments.loc[mask_min_4_measurments].index\n",
    "        raise ValueError(\"Some features still have less than 4 measurments in training data\"\n",
    "                         f\" after removing the features from the validation data: {idx.to_list()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save in long format\n",
    "\n",
    "- Data in long format: (peptide, sample_id, intensity)\n",
    "- no missing values kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dumps data in long-format\n",
    "splits_dumped = splits.dump(folder=args.data, file_format=args.file_format)\n",
    "dumps.update(splits_dumped)\n",
    "splits_dumped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = DataSplits.from_folder(args.data, file_format=args.file_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot distribution of splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_df = pd.DataFrame(index=df_long.index)\n",
    "splits_df['train'] = splits.train_X\n",
    "splits_df['val'] = splits.val_y\n",
    "splits_df['test'] = splits.test_y\n",
    "stats_splits = splits_df.describe()\n",
    "stats_splits.to_excel(writer, 'stats_splits', float_format='%.3f')\n",
    "stats_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whitespaces in legends are not displayed correctly...\n",
    "# max_int_len   = len(str(int(stats_splits.loc['count'].max()))) +1\n",
    "# _legend = [\n",
    "#     f'{s:<5} (N={int(stats_splits.loc[\"count\", s]):>{max_int_len},d})'.replace(\n",
    "#         ' ', '\\u00A0')\n",
    "#     for s in ('train', 'val', 'test')]\n",
    "_legend = [\n",
    "    f'{s:<5} (N={int(stats_splits.loc[\"count\", s]):,d})'\n",
    "    for s in ('train', 'val', 'test')]\n",
    "print(_legend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = 3\n",
    "ax = (splits\n",
    "      .train_X\n",
    "      .plot\n",
    "      .hist(\n",
    "          bins=bins,\n",
    "          ax=None,\n",
    "          color='C0',\n",
    "      ))\n",
    "_ = (splits\n",
    "     .val_y\n",
    "     .plot\n",
    "     .hist(bins=bins,\n",
    "           xticks=list(bins),\n",
    "           ax=ax,\n",
    "           color='C2',\n",
    "           legend=True)\n",
    "     )\n",
    "ax.legend(_legend[:-1])\n",
    "if args.use_every_nth_xtick > 1:\n",
    "    ax.set_xticks(ax.get_xticks()[::2])\n",
    "ax.set_xlabel('Intensity bins')\n",
    "fname = args.out_figures / f'0_{group}_val_over_train_split.pdf'\n",
    "figures[fname.name] = fname\n",
    "vaep.savefig(ax.get_figure(), fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_bin, max_bin = vaep.plotting.data.min_max(splits.val_y)\n",
    "bins = range(int(min_bin), int(max_bin), 1)\n",
    "ax = splits_df.plot.hist(bins=bins,\n",
    "                         xticks=list(bins),\n",
    "                         legend=False,\n",
    "                         stacked=True,\n",
    "                         color=['C0', 'C1', 'C2'],\n",
    "                         )\n",
    "if args.use_every_nth_xtick > 1:\n",
    "    ax.set_xticks(ax.get_xticks()[::2])\n",
    "ax.legend(_legend)\n",
    "ax.set_xlabel('Intensity bins')\n",
    "ax.yaxis.set_major_formatter(\"{x:,.0f}\")\n",
    "fname = args.out_figures / f'0_{group}_splits_freq_stacked.pdf'\n",
    "figures[fname.name] = fname\n",
    "vaep.savefig(ax.get_figure(), fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_per_bin = vaep.pandas.get_counts_per_bin(df=splits_df, bins=bins)\n",
    "counts_per_bin.to_excel(fname.with_suffix('.xlsx'))\n",
    "counts_per_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = splits_df.drop('train', axis=1).plot.hist(bins=bins,\n",
    "                                               xticks=list(bins),\n",
    "                                               color=['C1', 'C2'],\n",
    "                                               legend=False,\n",
    "                                               stacked=True,\n",
    "                                               )\n",
    "if args.use_every_nth_xtick > 1:\n",
    "    ax.set_xticks(ax.get_xticks()[::2])\n",
    "ax.legend(_legend[1:])\n",
    "ax.set_xlabel('Intensity bins')\n",
    "ax.yaxis.set_major_formatter(\"{x:,.0f}\")\n",
    "fname = args.out_figures / f'0_{group}_val_test_split_freq_stacked_.pdf'\n",
    "figures[fname.name] = fname\n",
    "vaep.savefig(ax.get_figure(), fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save binned counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_per_bin = dict()\n",
    "for col in splits_df.columns:\n",
    "    _series = (pd.cut(splits_df[col], bins=bins)\n",
    "               .to_frame()\n",
    "               .groupby(col)\n",
    "               .size())\n",
    "    _series.index.name = 'bin'\n",
    "    counts_per_bin[col] = _series\n",
    "counts_per_bin = pd.DataFrame(counts_per_bin)\n",
    "counts_per_bin.to_excel(fname.with_suffix('.xlsx'))\n",
    "counts_per_bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot training data missing plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits.to_wide_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = vaep.plotting.data.plot_feat_median_over_prop_missing(\n",
    "    data=splits.train_X, type='scatter')\n",
    "fname = args.out_figures / f'0_{group}_intensity_median_vs_prop_missing_scatter_train'\n",
    "figures[fname.stem] = fname\n",
    "vaep.savefig(ax.get_figure(), fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = vaep.plotting.data.plot_feat_median_over_prop_missing(\n",
    "    data=splits.train_X, type='boxplot')\n",
    "fname = args.out_figures / f'0_{group}_intensity_median_vs_prop_missing_boxplot_train'\n",
    "figures[fname.stem] = fname\n",
    "vaep.savefig(ax.get_figure(), fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medians = (splits\n",
    "           .train_X\n",
    "           .median()\n",
    "           .astype(int)\n",
    "           ).to_frame('median_floor')\n",
    "\n",
    "feat_with_median = medians.groupby('median_floor').size().rename('M feat')\n",
    "medians = medians.join(feat_with_median, on='median_floor')\n",
    "medians = medians.apply(lambda s: \"{:02,d} (N={:3,d})\".format(*s), axis=1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 2))\n",
    "s = 1\n",
    "s_axes = pd.DataFrame({'medians': medians,\n",
    "                       'Validation split': splits.val_y.notna().sum(),\n",
    "                       'Training split': splits.train_X.notna().sum()}\n",
    "                      ).plot.box(by='medians',\n",
    "                                 boxprops=dict(linewidth=s),\n",
    "                                 flierprops=dict(markersize=s),\n",
    "                                 ax=ax)\n",
    "for ax in s_axes:\n",
    "    _ = ax.set_xticklabels(ax.get_xticklabels(),\n",
    "                           rotation=45,\n",
    "                           horizontalalignment='right')\n",
    "    ax.set_xlabel(f'{args.feat_name_display.capitalize()} binned by their median intensity '\n",
    "                  f'(N {args.feat_name_display})')\n",
    "    _ = ax.set_ylabel('Frequency')\n",
    "fname = args.out_figures / f'0_{group}_intensity_median_vs_prop_missing_boxplot_val_train'\n",
    "figures[fname.stem] = fname\n",
    "vaep.savefig(ax.get_figure(), fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = args.folder_experiment / 'data_config.yaml'\n",
    "args.dump(fname)\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saved Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved figures\n",
    "figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saved dumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()\n",
    "dumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
