{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 03 - Data\n",
    "\n",
    "Create data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.options.display.max_columns = 32\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import vaep\n",
    "from vaep.pandas import interpolate, parse_query_expression\n",
    "from vaep.io.datasplits import DataSplits\n",
    "from vaep.io import thermo_raw_files\n",
    "from vaep.sampling import feature_frequency, frequency_by_index, sample_data\n",
    "\n",
    "from vaep.analyzers import analyzers\n",
    "from vaep.analyzers.analyzers import  AnalyzePeptides\n",
    "\n",
    "from vaep.logging import setup_logger\n",
    "logger = vaep.logging.setup_nb_logger()\n",
    "logger.info(\"Split data and make diagnostic plots\")\n",
    "\n",
    "figures = {}  # collection of ax or figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# catch passed parameters\n",
    "args = None\n",
    "args = dict(globals()).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "FN_INTENSITIES: str =  'data/single_datasets/df_intensities_proteinGroups_long_2017_2018_2019_2020_N05015_M04547/Q_Exactive_HF_X_Orbitrap_Exactive_Series_slot_#6070.pkl'  # Intensities for feature\n",
    "# FN_PEPTIDE_FREQ: str = 'data/processed/count_all_peptides.json' # Peptide counts for all parsed files on erda (for data selection)\n",
    "fn_rawfile_metadata: str = 'data/files_selected_metadata.csv' # Machine parsed metadata from rawfile workflow\n",
    "# M: int = 5000 # M most common features\n",
    "feat_prevalence: Union[int, float] = 0.25 # Minum number or fraction of feature prevalence across samples to be kept\n",
    "sample_completeness: Union[int, float] = 0.5 # Minimum number or fraction of total requested features per Sample\n",
    "select_N = None # sample a certain number of samples\n",
    "min_RT_time: Union[int, float] = 120 # Minum retention time (RT) in minutes\n",
    "index_col: Union[str,int] = ['Sample ID', 'Gene names'] # Can be either a string or position (typical 0 for first column)\n",
    "# query expression for subsetting\n",
    "# query_subset_meta: str = \"`instrument serial number` in ['Exactive Series slot #6070',]\" # query for metadata, see files_selected_per_instrument_counts.csv for options\n",
    "logarithm: str = 'log2' # Log transformation of initial data (select one of the existing in numpy)\n",
    "folder_experiment: str = f'runs/experiment_03/{Path(FN_INTENSITIES).parent.name}/{Path(FN_INTENSITIES).stem}'\n",
    "column_names: List = None # Manuelly set column names\n",
    "# metadata -> defaults for metadata extracted from machine data\n",
    "meta_date_col = 'Content Creation Date'\n",
    "meta_cat_col = 'Thermo Scientific instrument model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select_N = 50\n",
    "# fn_rawfile_metadata = ''\n",
    "# meta_date_col = ''\n",
    "# meta_cat_col = ''\n",
    "# min_RT_time = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# # peptides\n",
    "# FN_INTENSITIES: str = 'data/single_datasets/df_intensities_peptides_long_2017_2018_2019_2020_N05011_M42725/Q_Exactive_HF_X_Orbitrap_Exactive_Series_slot_#6070.pkl'  # Intensities for feature\n",
    "# index_col: Union[str,int] = ['Sample ID', 'peptide'] # Can be either a string or position (typical 0 for first column)\n",
    "# folder_experiment: str = f'runs/experiment_03/{Path(FN_INTENSITIES).parent.name}/{Path(FN_INTENSITIES).stem}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# # # evidence\n",
    "# FN_INTENSITIES: str = 'data/single_datasets/df_intensities_evidence_long_2017_2018_2019_2020_N05015_M49321/Q_Exactive_HF_X_Orbitrap_Exactive_Series_slot_#6075.pkl'  # Intensities for feature\n",
    "# index_col: Union[str,int] = ['Sample ID', 'Sequence', 'Charge'] # Can be either a string or position (typical 0 for first column)\n",
    "# folder_experiment: str = f'runs/experiment_03/{Path(FN_INTENSITIES).parent.name}/{Path(FN_INTENSITIES).stem}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {k: v for k, v in globals().items() if k not in args and k[0] != '_'}\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There must be a better way...\n",
    "@dataclass\n",
    "class DataConfig:\n",
    "    \"\"\"Documentation. Copy parameters one-to-one to a dataclass.\"\"\"\n",
    "    FN_INTENSITIES: str  # Samples metadata extraced from erda\n",
    "    # file_ext: str # file extension\n",
    "    # FN_PEPTIDE_FREQ: str # Peptide counts for all parsed files on erda (for data selection)\n",
    "    fn_rawfile_metadata: str  # Machine parsed metadata from rawfile workflow\n",
    "    # M: int # M most common features\n",
    "    feat_prevalence: Union[int, float] = 0.25 # Minum number or fraction of feature prevalence across samples to be kept\n",
    "    sample_completeness: Union[int, float] = 0.5 # Minimum number or fraction of total requested features per Sample\n",
    "    select_N:int = None # sample a certain number of samples\n",
    "    min_RT_time: Union[int, float] = None # set to None per default\n",
    "    index_col: Union[\n",
    "        str, int\n",
    "    ] = \"Sample ID\"  # Can be either a string or position (typical 0 for first column)\n",
    "    # query expression for subsetting\n",
    "    # query_subset_meta: str = \"`instrument serial number` in ['Exactive Series slot #6070',]\" # query for metadata, see files_selected_per_instrument_counts.csv for options\n",
    "    logarithm: str = 'log2' # Log transformation of initial data (select one of the existing in numpy)\n",
    "    folder_experiment: str = 'runs/experiment_03'\n",
    "    column_names: str = None # Manuelly set column names\n",
    "    # metadata -> defaults for metadata extracted from machine data\n",
    "    meta_date_col: str = None\n",
    "    meta_cat_col: str = None\n",
    "\n",
    "params = DataConfig(**args) # catches if non-specified arguments were passed\n",
    "\n",
    "params = OmegaConf.create(params.__dict__)\n",
    "dict(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_experiment = Path(folder_experiment)\n",
    "folder_experiment.mkdir(exist_ok=True, parents=True)\n",
    "logger.info(f'Folder for output = {folder_experiment}')\n",
    "\n",
    "folder_data = folder_experiment / 'data'\n",
    "folder_data.mkdir(exist_ok=True)\n",
    "logger.info(f'Folder for data: {folder_data = }')\n",
    "\n",
    "folder_figures = folder_experiment / 'figures'\n",
    "folder_figures.mkdir(exist_ok=True)\n",
    "logger.info(f'Folder for figures: {folder_figures = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "process arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"{FN_INTENSITIES = }\")\n",
    "\n",
    "\n",
    "FILE_FORMAT_TO_CONSTRUCTOR = {'csv': 'from_csv',\n",
    "                              'pkl': 'from_pickle',\n",
    "                              'pickle': 'from_pickle',\n",
    "                              }\n",
    "\n",
    "FILE_EXT = Path(FN_INTENSITIES).suffix[1:]\n",
    "logger.info(f\"File format (extension): {FILE_EXT}  (!specifies data loading function!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "params.used_features = None # sorted(selected_peptides)\n",
    "if isinstance(params.index_col, str) and params.used_features: params.used_features.insert(0, params.index_col)\n",
    "constructor = getattr(AnalyzePeptides, FILE_FORMAT_TO_CONSTRUCTOR[FILE_EXT]) #AnalyzePeptides.from_csv \n",
    "analysis = constructor(fname=params.FN_INTENSITIES,\n",
    "                                     index_col=index_col,\n",
    "                                     usecols=params.used_features\n",
    "                                    )\n",
    "\n",
    "if params.column_names:\n",
    "    analysis.df.columns.names = params.column_names\n",
    "\n",
    "\n",
    "log_fct = getattr(np, params.logarithm)\n",
    "analysis.log_transform(log_fct)\n",
    "logger.info(f\"{analysis = }\")\n",
    "analysis.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = analysis.df.notna().sum(axis=0).to_frame(analysis.df.columns.name).plot.box()\n",
    "ax.set_ylabel('number of observation across samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case there are multiple features for each intensity values (currenlty: peptide sequence and charge), combine the column names to a single str index.\n",
    "\n",
    "> The Collaborative Modeling approach will need a single feature column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_as_str(seq):\n",
    "    ret = \"_\".join(str(x) for x in seq)\n",
    "    return ret\n",
    "    \n",
    "# if hasattr(analysis.df.columns, \"levels\"):\n",
    "if isinstance(analysis.df.columns, pd.MultiIndex):\n",
    "    logger.warning(\"combine MultiIndex columns to one feature column\")\n",
    "    print(analysis.df.columns[:10].map(join_as_str))\n",
    "    _new_name = join_as_str(analysis.df.columns.names)\n",
    "    analysis.df.columns = analysis.df.columns.map(join_as_str)\n",
    "    analysis.df.columns.name = _new_name\n",
    "    logger.warning(f\"New name: {analysis.df.columns.names = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Machine metadata\n",
    "\n",
    "- read from file using [ThermoRawFileParser](https://github.com/compomics/ThermoRawFileParser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params.fn_rawfile_metadata:\n",
    "    df_meta = pd.read_csv(params.fn_rawfile_metadata, index_col=0)\n",
    "else:\n",
    "    logger.warning(f\"No metadata for samples provided, create placeholder.\")\n",
    "    if params.meta_date_col:\n",
    "        raise ValueError(f\"No metadata provided, but data column set: {params.meta_date_col}\")\n",
    "    if params.meta_cat_col:\n",
    "        raise ValueError(f\"No metadata provided, but data column set: {params.meta_cat_col}\")\n",
    "    df_meta = pd.DataFrame(index=analysis.df.index)\n",
    "df_meta = df_meta.loc[analysis.df.index.to_list()] # index is sample index\n",
    "if df_meta.index.name is None:\n",
    "    df_meta.index.name = params.index_col[0]\n",
    "df_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params.meta_date_col:\n",
    "    df_meta[params.meta_date_col] = pd.to_datetime(df_meta[params.meta_date_col])\n",
    "else:\n",
    "    params.meta_date_col = 'PlaceholderTime'\n",
    "    df_meta[params.meta_date_col] = range(len(df_meta))\n",
    "df_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_meta.columns.isin(thermo_raw_files.cols_instrument).sum() == len(thermo_raw_files.cols_instrument): \n",
    "    display(df_meta.groupby(thermo_raw_files.cols_instrument)[params.meta_date_col].agg(['count','min','max']))\n",
    "else:\n",
    "    logger.info(f\"Instrument column not found: {thermo_raw_files.cols_instrument}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta.describe(datetime_is_numeric=True, percentiles=np.linspace(0.05, 0.95, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "select samples with a minimum retention time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params.min_RT_time:\n",
    "    logger.info(\"Metadata should have 'MS max RT' entry from ThermoRawFileParser\")\n",
    "    msg = f\"Minimum RT time maxiumum is set to {params.min_RT_time} minutes (to exclude too short runs, which are potentially fractions).\"\n",
    "    mask_RT = df_meta['MS max RT'] >= params.min_RT_time # can be integrated into query string\n",
    "    msg += f\" Total number of samples retained: {int(mask_RT.sum())}\"\n",
    "    msg += f\" ({int(len(mask_RT) - mask_RT.sum())} excluded).\"\n",
    "    logger.info(msg)\n",
    "    df_meta = df_meta.loc[mask_RT]\n",
    "else:\n",
    "    logger.warning(f\"Retention time filtering deactivated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = df_meta.sort_values(params.meta_date_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_stats = df_meta.describe(include='all', datetime_is_numeric=True)\n",
    "meta_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "subset with variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    display(meta_stats.loc[:, (meta_stats.loc['unique'] > 1) |  (meta_stats.loc['std'] > 0.1)])\n",
    "except KeyError:\n",
    "    if 'std' in meta_stats.index:\n",
    "        display(meta_stats.loc[:, (meta_stats.loc['std'] > 0.1)])\n",
    "    if 'unique' in meta_stats.index:\n",
    "        display(meta_stats.loc[:, (meta_stats.loc['std'] > 0.1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional, if using ThermoRawFileParser: check some columns describing settings\n",
    "  - software can be updated: `Software Version`\n",
    "  - `mass resolution` setting for instrument\n",
    "  - colision type for MS2: `beam-type collision-induced dissocation`\n",
    "  - missing `dilution factor` \n",
    "  - omit (uncomment if needed):\n",
    "    - quite some variation due to `MS max charge`: omit\n",
    "    - variation by `injection volume setting` and instrument over time\n",
    "        - 500ng of peptides should be injected, based on concentration of peptides this setting is adjusted to get it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_raw_settings = [\n",
    " 'Thermo Scientific instrument model',\n",
    " 'instrument serial number',\n",
    " 'Software Version', \n",
    " # 'MS max charge',\n",
    " 'mass resolution',\n",
    " 'beam-type collision-induced dissociation', \n",
    " # 'injection volume setting',\n",
    " 'dilution factor',\n",
    "]\n",
    "\n",
    "if df_meta.columns.isin(meta_raw_settings).sum() == len(meta_raw_settings):\n",
    "    display(\n",
    "        df_meta[meta_raw_settings].drop_duplicates() # index gives first example with this combination\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- check for variation in `software Version` and `injection volume setting`\n",
    "\n",
    "\n",
    "Update selection of samples based on metadata (e.g. minimal retention time)\n",
    "- sort data the same as sorted meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_meta_data(analysis: AnalyzePeptides, df_meta:pd.DataFrame):\n",
    "    try:\n",
    "        analysis.df = analysis.df.loc[df_meta.index]\n",
    "    except KeyError as e:\n",
    "        logger.warning(e)\n",
    "        logger.warning(\"Ignore missing samples in quantified samples\")\n",
    "        analysis.df = analysis.df.loc[analysis.df.index.intersection(df_meta.index)]\n",
    "\n",
    "    analysis.df_meta = df_meta\n",
    "    return analysis\n",
    "\n",
    "analysis = add_meta_data(analysis, df_meta=df_meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "incorrectly_encoded_metadata": "tags=[] jp-MarkdownHeadingCollapsed=true",
    "tags": []
   },
   "source": [
    "Ensure unique indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert analysis.df.index.is_unique, \"Duplicates in index\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select a subset of samples if specified (reduce the number of samples)\n",
    "\n",
    "- select features if `select_N` is specifed (for now)\n",
    "- for interpolation to make sense, it is best to select a consecutive number of samples:\n",
    "  - take N most recent samples (-> check that this makes sense for your case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params.select_N is not None:\n",
    "    params.select_N = min(params.select_N, len(analysis.df_meta))\n",
    "                   \n",
    "    analysis.df_meta = analysis.df_meta.iloc[-params.select_N:]\n",
    "    \n",
    "    analysis.df = analysis.df.loc[analysis.df_meta.index].dropna(how='all', axis=1)\n",
    "    ax = analysis.df.T.describe().loc['count'].hist()\n",
    "    _ = ax.set_title('histogram of features for all eligable samples')\n",
    "    \n",
    "    # updates\n",
    "    sample_counts = analysis.df.notna().sum(axis=1) # if DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export Pathname captured by ThermoRawFileParser\n",
    "# analysis.df_meta['Pathname'].to_json(folder_experiment / 'config_rawfile_paths.json', indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Step: Select features by prevalence\n",
    "- `feat_prevalence` across samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "freq_per_feature = analysis.df.notna().sum() # on wide format\n",
    "if isinstance(params.feat_prevalence, float):\n",
    "    N_samples = len(analysis.df_meta)\n",
    "    logger.info(f\"Current number of samples: {N_samples}\")\n",
    "    logger.info(f\"Feature has to be present in at least {params.feat_prevalence:.2%} of samples\")\n",
    "    params.feat_prevalence = int(N_samples * params.feat_prevalence)\n",
    "assert isinstance(params.feat_prevalence, int)\n",
    "logger.info(f\"Feature has to be present in at least {params.feat_prevalence} of samples\")                \n",
    "# select features\n",
    "mask = freq_per_feature >= params.feat_prevalence\n",
    "logger.info(f\"Drop {(~mask).sum()} features\")\n",
    "freq_per_feature = freq_per_feature.loc[mask]\n",
    "analysis.df = analysis.df.loc[:, mask]\n",
    "analysis.N, analysis.M = analysis.df.shape\n",
    "\n",
    "# # potentially create freq based on DataFrame\n",
    "analysis.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "incorrectly_encoded_metadata": "tags=[] jp-MarkdownHeadingCollapsed=true",
    "tags": []
   },
   "source": [
    "## Second step - Sample selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select samples based on completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(params.sample_completeness, float):\n",
    "    msg = f'Fraction of minimum sample completeness over all features specified with: {params.sample_completeness}\\n'\n",
    "    # assumes df in wide format\n",
    "    params.sample_completeness = int(analysis.df.shape[1] * params.sample_completeness)\n",
    "    msg += f'This translates to a minimum number of features per sample (to be included): {params.sample_completeness}'\n",
    "    logger.info(msg)\n",
    "\n",
    "sample_counts = analysis.df.notna().sum(axis=1) # if DataFrame\n",
    "    \n",
    "mask = sample_counts > params.sample_completeness\n",
    "msg = f'Drop {len(mask) - mask.sum()} of {len(mask)} initial samples.'\n",
    "print(msg)\n",
    "analysis.df = analysis.df.loc[mask]\n",
    "analysis.df = analysis.df.dropna(axis=1, how='all') # drop now missing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.N, params.M = analysis.df.shape # save data dimensions\n",
    "params.used_samples = analysis.df.index.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of features per sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = analysis.df.notna().sum(axis=1).hist()\n",
    "ax.set_xlabel('features per eligable sample')\n",
    "ax.set_ylabel('observations')\n",
    "fname = folder_figures / 'hist_features_per_sample'\n",
    "figures[fname.stem] = fname\n",
    "vaep.savefig(ax.get_figure(), fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "ax = analysis.df.notna().sum(axis=0).sort_values().plot()\n",
    "ax.set_xlabel('feature prevalence')\n",
    "ax.set_ylabel('observations')\n",
    "fname = folder_figures / 'feature_prevalence'\n",
    "figures[fname.stem] = fname\n",
    "vaep.savefig(ax.get_figure(), fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number off observations accross feature value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max(s: pd.Series):\n",
    "    min_bin, max_bin = (int(s.min()), (int(s.max())+1))\n",
    "    return min_bin, max_bin\n",
    "\n",
    "\n",
    "def plot_histogram_intensites(s: pd.Series, interval_bins=1, min_max=(15, 40), ax=None):\n",
    "\n",
    "    min_bin, max_bin = min_max\n",
    "    bins = range(min_bin, int(max_bin), 1)\n",
    "    ax = s.plot.hist(bins=bins, ax=ax)\n",
    "    return ax, bins\n",
    "\n",
    "\n",
    "min_intensity, max_intensity = min_max(analysis.df.stack())\n",
    "ax, bins = plot_histogram_intensites(\n",
    "    analysis.df.stack(), min_max=(min_intensity, max_intensity))\n",
    "ax.locator_params(axis='x', integer=True)\n",
    "\n",
    "fname = folder_figures / 'intensity_distribution_overall'\n",
    "figures[fname.stem] = fname\n",
    "vaep.savefig(ax.get_figure(), fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_by_median = {'median feat value': analysis.df.median(\n",
    "), 'prop. missing': analysis.df.isna().mean()}\n",
    "missing_by_median = pd.DataFrame(missing_by_median)\n",
    "x_col, y_col = missing_by_median.columns\n",
    "\n",
    "bins = range(*min_max(missing_by_median['median feat value']), 1)\n",
    "\n",
    "missing_by_median['bins'] = pd.cut(\n",
    "    missing_by_median['median feat value'], bins=bins)\n",
    "missing_by_median['median feat value (rounded)'] = missing_by_median['median feat value'].round(decimals=0).astype(int)\n",
    "_counts = missing_by_median.groupby('median feat value (rounded)')['median feat value'].count().rename('count')\n",
    "missing_by_median = missing_by_median.join(_counts, on='median feat value (rounded)')\n",
    "missing_by_median['Intensity rounded (based on N observations)'] = missing_by_median.iloc[:,-2:].apply(lambda s: \"{}  (N={:3,d})\".format(*s), axis=1)\n",
    "\n",
    "ax = missing_by_median.plot.scatter(x_col, y_col, ylim=(0, 1))\n",
    "\n",
    "\n",
    "fname = folder_figures / 'intensity_median_vs_prop_missing_scatter'\n",
    "figures[fname.stem] = fname\n",
    "vaep.savefig(ax.get_figure(), fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_col = 'prop. missing'\n",
    "x_col = 'Intensity rounded (based on N observations)'\n",
    "ax = missing_by_median[[x_col, y_col]].plot.box(by=x_col)\n",
    "_ = ax.set_title('')\n",
    "_ = ax.set_ylabel(y_col)\n",
    "_ = ax.set_xlabel(x_col)\n",
    "_ = ax.set_xticklabels(ax.get_xticklabels(), rotation=45,\n",
    "                       horizontalalignment='right')\n",
    "\n",
    "fname = folder_figures / 'intensity_median_vs_prop_missing_boxplot'\n",
    "figures[fname.stem] = fname\n",
    "vaep.savefig(ax.get_figure(), fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive and Single plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatter plots need to become interactive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_counts.name = 'identified features'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 2\n",
    "analysis.df = analysis.df.astype(float)\n",
    "pcs = analysis.get_PCA(n_components=K) # should be renamed to get_PCs\n",
    "pcs = pcs.iloc[:,:K].join(analysis.df_meta).join(sample_counts)\n",
    "\n",
    "pcs_name = pcs.columns[:2]\n",
    "pcs = pcs.reset_index()\n",
    "pcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs.describe(include='all', datetime_is_numeric=True).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params.meta_cat_col:\n",
    "    fig, ax = plt.subplots(figsize=(18,10))\n",
    "    analyzers.seaborn_scatter(pcs[pcs_name], fig, ax, meta=pcs[params.meta_cat_col], title=f\"by {params.meta_cat_col}\")\n",
    "    fname = folder_figures / f'pca_sample_by_{\"_\".join(params.meta_cat_col.split())}'\n",
    "    figures[fname.stem] = fname\n",
    "    vaep.savefig(fig, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params.meta_date_col != 'PlaceholderTime':\n",
    "    fig, ax = plt.subplots(figsize=(23, 10))\n",
    "    analyzers.plot_date_map(pcs[pcs_name], fig, ax, pcs[params.meta_date_col], title=f'by {params.meta_date_col}')\n",
    "    fname = folder_figures / 'pca_sample_by_date'\n",
    "    figures[fname.stem] = fname\n",
    "    vaep.savefig(fig, fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- software version: Does it make a difference?\n",
    "- size: number of features in a single sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    pcs, x=pcs_name[0], y=pcs_name[1],\n",
    "    hover_name=params.index_col[0],\n",
    "    # hover_data=analysis.df_meta,\n",
    "    title=f'First two Principal Components of {analysis.M} most abundant peptides for {pcs.shape[0]} samples',\n",
    "    # color=pcs['Software Version'],\n",
    "    color='identified features',\n",
    "    width=1200,\n",
    "    height=600\n",
    ")\n",
    "fname = folder_figures / 'pca_identified_features.png'\n",
    "figures[fname.stem] =  fname\n",
    "fig.write_image(fname)\n",
    "fig # stays interactive in html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Medians and percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = analysis.df\n",
    "df = df.join(df_meta[params.meta_date_col])\n",
    "df = df.set_index(params.meta_date_col).sort_index()\n",
    "if not params.meta_date_col == 'PlaceholderTime':\n",
    "    df.to_period('min')\n",
    "df = df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "ax = df.boxplot(rot=80, figsize=(20, 10), fontsize='large', showfliers=False, showcaps=False)\n",
    "_ = vaep.plotting.select_xticks(ax)\n",
    "fig = ax.get_figure()\n",
    "fname = folder_figures / 'median_boxplot'\n",
    "figures[fname.stem] =  fname\n",
    "vaep.savefig(fig, fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percentiles of intensities in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.stack().describe(percentiles=np.linspace(0.05, 0.95, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot sample median over time\n",
    "  - check if points are equally spaced (probably QC samples are run in close proximity)\n",
    "  - the machine will be not use for intermediate periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not params.meta_date_col == 'PlaceholderTime':\n",
    "    dates = df_meta[params.meta_date_col].sort_values()\n",
    "    # dates.name = 'date'\n",
    "    median_sample_intensity = (analysis.df\n",
    "                               .median(axis=1)\n",
    "                               .to_frame('median intensity'))\n",
    "    median_sample_intensity = median_sample_intensity.join(dates)\n",
    "\n",
    "    ax = median_sample_intensity.plot.scatter(x=dates.name, y='median intensity',\n",
    "                                              rot=90,\n",
    "                                              fontsize='large',\n",
    "                                              figsize=(20, 10),\n",
    "                                              xticks=vaep.plotting.select_dates(\n",
    "                                                  median_sample_intensity[dates.name])\n",
    "                                              )\n",
    "    fig = ax.get_figure()\n",
    "    figures['median_scatter'] = folder_figures / 'median_scatter'\n",
    "    vaep.savefig(fig, figures['median_scatter'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the closer the labels are there denser the samples are measured around that time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature frequency  in data\n",
    "\n",
    "- higher count, higher probability to be sampled into training data\n",
    "- missing peptides are sampled both into training as well as into validation dataset\n",
    "- everything not in training data is validation data\n",
    "\n",
    "Based on unmodified training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = \"Total number of samples in training data split: {}\"\n",
    "print(msg.format(len(analysis.df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # analysis.splits.to_wide_format()\n",
    "# assert analysis.splits is splits, \"Sanity check failed.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recalculate feature frequency after selecting some samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_per_feature = feature_frequency(analysis.df)\n",
    "freq_per_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq_per_feature.name = 'Gene names freq' # name it differently?\n",
    "freq_per_feature.to_json(folder_data / 'freq_features.json') # index.name is lost when data is stored\n",
    "freq_per_feature.to_pickle(folder_data / 'freq_features.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conserning sampling with frequency weights:\n",
    "  - larger weight -> higher probablility of being sampled\n",
    "  - weights need to be alignable to index of original DataFrame before grouping (same index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split: Train, validation and test data\n",
    "\n",
    "- test data is in clinical language often denoted as independent validation cohort\n",
    "- validation data (for model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.splits = DataSplits(is_wide_format=True)\n",
    "splits = analysis.splits\n",
    "print(f\"{analysis.splits = }\")\n",
    "analysis.splits.__annotations__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample targets (Fake NAs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add goldstandard targets for valiation and test data\n",
    "- based on same day\n",
    "- same instrument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create some target values by sampling 5% of the validation and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.to_long_format(inplace=True)\n",
    "analysis.df_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_na, splits.train_X = sample_data(analysis.df_long.squeeze(), sample_index_to_drop=0, weights=freq_per_feature, frac=0.1)\n",
    "assert len(splits.train_X) > len(fake_na)\n",
    "splits.val_y = fake_na.sample(frac=0.5).sort_index()\n",
    "splits.test_y = fake_na.loc[fake_na.index.difference(splits.val_y.index)]\n",
    "# splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits.test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits.val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits.train_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Save in long format\n",
    "\n",
    "- Data in long format: (peptide, sample_id, intensity)\n",
    "- no missing values kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits.dump(folder=folder_data, file_format=FILE_EXT)  # dumps data in long-format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reload from disk\n",
    "splits = DataSplits.from_folder(folder_data, file_format=FILE_EXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(OmegaConf.to_yaml(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = folder_experiment/'data_config.yaml'\n",
    "with open(fname, 'w') as f:\n",
    "    OmegaConf.save(params, f)\n",
    "fname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saved Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved figures\n",
    "figures"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cf83e9cb890c7f96eb0ae04f39a82254555f56a1a0ed2f03b23a8b40fe6cd31c"
  },
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
