{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split up data into single datasets\n",
    "\n",
    "- create datasets per (set of) instruments for a specific experiments\n",
    "- drop some samples based on quality criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from vaep.io import thermo_raw_files\n",
    "from vaep.analyzers import analyzers\n",
    "\n",
    "from config import erda_dumps\n",
    "from config import defaults\n",
    "\n",
    "import vaep\n",
    "from vaep.logging import setup_nb_logger\n",
    "\n",
    "logger = setup_nb_logger()\n",
    "\n",
    "FOLDER_DATA = defaults.FOLDER_DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DUMP: str = erda_dumps.FN_PROTEIN_GROUPS\n",
    "DUMP: str = erda_dumps.FN_PEPTIDES\n",
    "FOLDER_DATASETS: str = f'single_datasets/{DUMP.stem}'\n",
    "FILE_EXT = 'pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure output folder exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "root - INFO     Folder for datasets to be created: c:\\Users\\kzl465\\Documents\\repos\\vaep\\project\\data\\single_datasets\\df_intensities_peptides_long_2017_2018_2019_2020_N05011_M42725\n"
     ]
    }
   ],
   "source": [
    "FOLDER_DATASETS = defaults.FOLDER_DATA / FOLDER_DATASETS\n",
    "FOLDER_DATASETS.mkdir(exist_ok=True, parents=True)\n",
    "logger.info(f\"Folder for datasets to be created: {FOLDER_DATASETS.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dumps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- load dumps\n",
    "- load file to machine mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In case it is a DataFrame, not a series (-> leads to MultiIndex)\n",
    "# dump = analyzers.AnalyzePeptides.from_pickle(DUMP)\n",
    "# dump.df = dump.df.squeeze()\n",
    "# dump.df.to_pickle(DUMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump = analysis = analyzers.AnalyzePeptides.from_pickle(\n",
    "    DUMP,\n",
    "    is_wide_format=False,\n",
    "    ind_unstack=-1)\n",
    "dump.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter for odd samples\n",
    "\n",
    "- fractionated samples\n",
    "- GPF - Gas phase fractionation # Faims? DIA? \n",
    "- DIA\n",
    "- CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see 02_data_exploration_peptides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta Data\n",
    "\n",
    "- based on ThermoRawFileParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = pd.read_csv('data/files_selected_metadata.csv', index_col=0)\n",
    "date_col = 'Content Creation Date'\n",
    "df_meta[date_col] = pd.to_datetime(df_meta[date_col])\n",
    "df_meta = df_meta.loc[dump.df.index]\n",
    "df_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Available instruments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_instrument = df_meta.groupby(thermo_raw_files.cols_instrument)[date_col].agg(\n",
    "    ['count', 'min', 'max']).sort_values(by=thermo_raw_files.cols_instrument[:2] + ['count'], ascending=False)\n",
    "counts_instrument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_MIN_INSTRUMENT = 300\n",
    "selected_instruments = counts_instrument.query(f\"count >= {N_MIN_INSTRUMENT}\")\n",
    "fname = FOLDER_DATASETS/'dataset_info'\n",
    "selected_instruments.to_json(f\"{fname}.json\", indent=4)\n",
    "selected_instruments.to_latex(f\"{fname}.tex\")\n",
    "selected_instruments.to_excel(f\"{fname}.xlsx\")\n",
    "logger.info(f\"Save Information to: {fname} (as json, tex)\")\n",
    "selected_instruments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# import vaep.pandas\n",
    "\n",
    "# def get_fname_from_keys(keys, folder=Path('.'), file_ext='.pkl', remove_duplicates=True):\n",
    "#     if remove_duplicates:\n",
    "#         # https://stackoverflow.com/a/53657523/9684872\n",
    "#         keys = list(dict.fromkeys(keys))\n",
    "#     folder.mkdir(exist_ok=True, parents=True)\n",
    "#     fname_dataset = folder /  '{}{}'.format(vaep.pandas.replace_with(' '.join(keys), replace='- ', replace_with='_'), file_ext)\n",
    "#     return fname_dataset\n",
    "\n",
    "vaep.io.get_fname_from_keys(\n",
    "    selected_instruments.index[0], folder=FOLDER_DATASETS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = selected_instruments.index.names\n",
    "\n",
    "file_formats = {'pkl': 'to_pickle',\n",
    "                'pickle': 'to_pickle',\n",
    "                'csv': 'to_csv'}\n",
    "\n",
    "\n",
    "for values in selected_instruments.index:\n",
    "    mask = df_meta[cols] == values\n",
    "    mask.all(axis=1).sum()\n",
    "    sample_ids = df_meta.loc[mask.all(axis=1)].index\n",
    "    dataset = dump.df.loc[sample_ids]\n",
    "    fname_dataset = vaep.io.get_fname_from_keys(values,\n",
    "                                                folder=FOLDER_DATASETS,\n",
    "                                                file_ext=f\".{FILE_EXT}\")\n",
    "\n",
    "    logger.info(f'Dump dataset with N = {len(dataset)} to {fname_dataset}')\n",
    "    _to_file_format = getattr(dataset, file_formats[FILE_EXT])\n",
    "    _to_file_format(fname_dataset)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a3c193f136b3677f7a0d8b8f2344336d9d9fbcf8449c7b9fd96c2b5d44d3d77c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('vaep')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
