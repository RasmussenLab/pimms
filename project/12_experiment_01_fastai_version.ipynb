{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FASTAI implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from src import config\n",
    "from src.analyzers import *\n",
    "from vaep.transform import StandardScaler, get_df_fitted_mean_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from src.logging import setup_logger\n",
    "\n",
    "logger = logging.getLogger()  # returns root-logger\n",
    "logger.setLevel(logging.CRITICAL)  # silence for everything else\n",
    "logger.handlers = []\n",
    "\n",
    "\n",
    "logger = setup_logger(logger=logging.getLogger('vaep'))\n",
    "logger.info(\"Experiment 01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "- 1000 features (most abundant peptides)\n",
    "- later a subset of samples is selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES_TO_LOAD = None\n",
    "FN_PEPTIDE_INTENSITIES = config.FOLDER_DATA / 'df_intensities_N07813_M01000'\n",
    "analysis = AnalyzePeptides(\n",
    "    fname=FN_PEPTIDE_INTENSITIES, nrows=N_SAMPLES_TO_LOAD, index_col=0)\n",
    "analysis.df = analysis.df.sort_index()  # sort by date\n",
    "assert analysis.df.index.is_unique, \"Non-unique training samples\"\n",
    "analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select consecutives samples for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "N_SAMPLES = 1000\n",
    "logger.info(f\"Selected {N_SAMPLES}\")\n",
    "analysis.N_SAMPLES = N_SAMPLES\n",
    "\n",
    "\n",
    "def get_consecutive_data_indices(index, n_samples=N_SAMPLES):\n",
    "    start_sample = len(index) - n_samples\n",
    "    start_sample = random.randint(0, start_sample)\n",
    "    return index[start_sample:start_sample+n_samples]\n",
    "\n",
    "\n",
    "indices_selected = get_consecutive_data_indices(analysis.df.index)\n",
    "analysis.samples = indices_selected\n",
    "analysis.df = analysis.df.loc[indices_selected]\n",
    "\n",
    "FRACTION = 0.9\n",
    "\n",
    "class Indices(SimpleNamespace):\n",
    "    pass\n",
    "\n",
    "indices = Indices()\n",
    "indices.train, indices.valid = indices_selected[:int(\n",
    "    FRACTION*N_SAMPLES)], indices_selected[int(FRACTION*N_SAMPLES):]\n",
    "analysis.indices = indices\n",
    "\n",
    "analysis.df_train = analysis.df.loc[indices.train]\n",
    "analysis.df_valid = analysis.df.loc[indices.valid]\n",
    "\n",
    "analysis.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fastai Dataloader\n",
    "\n",
    "> fastai includes a replacement for Pytorch's DataLoader which is largely API-compatible, and adds a lot of useful functionality and flexibility. Before we look at the class, there are a couple of helpers we'll need to define. [[link](https://docs.fast.ai/data.load.html)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fastai.tabular.all as tab\n",
    "from fastcore.transform import Transform\n",
    "\n",
    "from fastai.tabular.data import TabularDataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataloaders using an appropriate factory method from `TabularDataLoaders` class, here [`from_df`](https://docs.fast.ai/tabular.data.html#TabularDataLoaders.from_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame is shuffled\n",
    "N_VAL = 100\n",
    "valid_idx = list(range(N_VAL))\n",
    "dls = TabularDataLoaders.from_df(df=analysis.df, valid_idx=valid_idx, bs=64, \n",
    "                                 cat_names=None,\n",
    "                                 cont_names=list(analysis.df.columns),\n",
    "                                 y_names=None,\n",
    "                                 procs=None, # add options                                 \n",
    "                                )\n",
    "analysis.dls = dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.show_batch()  # loses object index attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.valid.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dls.train:\n",
    "    print(batch) # cat_names, cont_names, y_names\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms\n",
    "\n",
    "- procs ?\n",
    "- can be applied to an `pd.DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize(Transform):\n",
    "    def setup(self, array):\n",
    "        self.mean = array.mean()  # this assumes tensor, numpy arrays and alike\n",
    "        # should be applied along axis 0 (over the samples)\n",
    "        self.std = array.std()  # ddof=0 in scikit-learn\n",
    "\n",
    "    def encodes(self, x):\n",
    "        x_enc = (x - self.mean) / self.std\n",
    "        return x_enc\n",
    "\n",
    "    def decodes(self, x_enc):\n",
    "        x = (self.std * x_enc) + self.mean\n",
    "        return x\n",
    "\n",
    "\n",
    "tf_norm = Normalize()\n",
    "tf_norm.setup(analysis.df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare results to scikit learn implementation of [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html).\n",
    "\n",
    "Differences seem to arrive due to iterative computation of mean and standard-deviation in scikit-learn, see [`_incremental_mean_and_var`](https://github.com/scikit-learn/scikit-learn/blob/15a949460dbf19e5e196b8ef48f9712b72a3b3c3/sklearn/utils/extmath.py#L792)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 5\n",
    "scaler = StandardScaler().fit(analysis.df_train)\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        ('Transform', 'mean'): tf_norm.mean[:M],\n",
    "        ('Transform', 'std'): tf_norm.std[:M],\n",
    "        ('StandardScaler', 'mean'): scaler.mean_[:M],\n",
    "        ('StandardScaler', 'std'): scaler.scale_[:M]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "tf_norm(analysis.df_train.iloc[:N]).iloc[:, :M]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.transform(analysis.df_train.iloc[:N]).iloc[:, :M]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TabularDataLoader with procs\n",
    "\n",
    "- ToDo: Replace Normalize with custom Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.tabular.core import Normalize, FillMissing\n",
    "Normalize, FillMissing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame is shuffled\n",
    "N_VAL = 100\n",
    "valid_idx = list(range(N_VAL))\n",
    "dls = TabularDataLoaders.from_df(df=analysis.df, valid_idx=valid_idx, bs=64, \n",
    "                                 cat_names=None,\n",
    "                                 cont_names=list(analysis.df.columns),\n",
    "                                 y_names=None,\n",
    "                                 procs=[Normalize, FillMissing], # add options  \n",
    "                                )\n",
    "# analysis.dls = dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dls.one_batch()\n",
    "sample # cat, cont, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dls.decode(sample) # DataLoader with decoded\n",
    "sample.cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.conts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation in Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "procs = dls.procs\n",
    "type(procs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(procs.normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initalized  procs/transforms in `fs` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[type(x) for x in procs.fs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now the `loss_func` signature and the `NN_Module` forward path have to be adapted. Unsure how to do this in plain PyTorch yet. So we only use the dataloader for now.\n",
    "\n",
    "- Callback needed to set `xb` to `yb`, see [callback-attributes](https://docs.fast.ai/callback.core.html#Attributes-available-to-callbacks) and [example](https://github.com/dhuynh95/fastai_autoencoder/blob/bc357927f26273d676dca9a41018411408b97430/fastai_autoencoder/callback.py#L16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_function(recon_x=batch_recon, x=batch, mask=mask, mu=mu, logvar=logvar)\n",
    "# learn = Learner(dls, NN_Module, opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrap Code\n",
    "\n",
    "- understand the many ways to interact with data in FastAi's library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `ItemTransform` to is performed of some data in a DataFrame? (\"to dataloaders\", \"to datasets\" name to signal intermediate ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #source: https://nbviewer.jupyter.org/github/EtienneT/TabularVAE/blob/master/TabularAE.ipynb\n",
    "# from fastai.tabular.all import *\n",
    "\n",
    "# class ReadTabBatchIdentity(ItemTransform):\n",
    "#     \"Read a batch of data and return the inputs as both `x` and `y`\"\n",
    "#     def __init__(self, to): store_attr()\n",
    "\n",
    "#     def encodes(self, to):\n",
    "#         if not to.with_cont: res = (tensor(to.cats).long(),) + (tensor(to.cats).long(),)\n",
    "#         else: res = (tensor(to.cats).long(),tensor(to.conts).float()) + (tensor(to.cats).long(), tensor(to.conts).float())\n",
    "#         if to.device is not None: res = to_device(res, to.device)\n",
    "#         return res\n",
    "    \n",
    "# class TabularPandasIdentity(TabularPandas): pass\n",
    "\n",
    "# @delegates()\n",
    "# class TabDataLoaderIdentity(TabDataLoader):\n",
    "#     \"A transformed `DataLoader` for AutoEncoder problems with Tabular data\"\n",
    "#     do_item = noops\n",
    "#     def __init__(self, dataset, bs=16, shuffle=False, after_batch=None, num_workers=0, **kwargs):\n",
    "#         if after_batch is None: after_batch = L(TransformBlock().batch_tfms)+ReadTabBatchIdentity(dataset)\n",
    "#         super().__init__(dataset, bs=bs, shuffle=shuffle, after_batch=after_batch, num_workers=num_workers, **kwargs)\n",
    "\n",
    "#     def create_batch(self, i): return self.dataset.iloc[i]\n",
    "\n",
    "# TabularPandasIdentity._dl_type = TabDataLoaderIdentity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Tabular dataset in numpy](https://muellerzr.github.io/fastblog/2020/04/22/TabularNumpy.html#Bringing-in-NumPy)\n",
    "  - `DataSet`: PyTorch interface\n",
    "  - `DataLoader` customization\n",
    "      1. `create_item`\n",
    "      2. `create_batch`\n",
    "      3. `get_idxs`\n",
    "      4. `shuffle_ds`\n",
    "\n",
    "The example is build on top of `TabularPandas` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom [`TransformBlock`](https://github.com/fastai/fastai/blob/master/fastai/data/block.py#L13)\n",
    "\n",
    "```python\n",
    "class TransformBlock():\n",
    "    \"A basic wrapper that links defaults transforms for the data block API\"\n",
    "    def __init__(self, type_tfms=None, item_tfms=None, batch_tfms=None, dl_type=None, dls_kwargs=None):\n",
    "        self.type_tfms  =            L(type_tfms)\n",
    "        self.item_tfms  = ToTensor + L(item_tfms)\n",
    "        self.batch_tfms =            L(batch_tfms)\n",
    "        self.dl_type,self.dls_kwargs = dl_type,({} if dls_kwargs is None else dls_kwargs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TabularPandas - Applying transforms\n",
    "\n",
    "1. Transform are applied in place\n",
    "2. Order of procs is ignored. Instead a pre-defined order is applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fastai.tabular.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.rand(100, 10)* 2 + 24 )\n",
    "df = df.mask(cond=pd.DataFrame(np.random.rand(100,10)>0.9))\n",
    "df.columns = list('ABCDEFGIJH')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "procs = [Normalize, FillMissing(add_col=True)]\n",
    "cont_names = list(df.columns)\n",
    "\n",
    "to = TabularPandas(df, procs=procs, cont_names=cont_names)\n",
    "print(\"Tabular object:\", type(to))\n",
    "\n",
    "to.items # items reveals data in DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_names = list(df.columns)\n",
    "to_manuel = to = TabularPandas(df, cont_names=cont_names)\n",
    "procs = [Normalize().setups(to_manuel), FillMissing(add_col=True).setup(to_manuel)]\n",
    "to_manuel.items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_manuel.items.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_names = list(df.columns)\n",
    "to_manuel = to = TabularPandas(df, cont_names=cont_names)\n",
    "procs = [FillMissing(add_col=True).setup(to_manuel), Normalize().setups(to_manuel)]\n",
    "to_manuel.items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_manuel.items.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vaep",
   "language": "python",
   "name": "vaep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
