{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proof of Concept - VAEP \n",
    "Variational Autoencoder of the Proteome (VAEP), reconstructiong samples on the peptide level using `log`-transformed on peptide intensities. This is the proof of concept (POC) for later use. \n",
    "\n",
    "- Fit VAE to Hela-Sample data (3 samples) and overfit. (Functional test of code)\n",
    "- Fit \n",
    "\n",
    "### Handling Missing values\n",
    "In this semi-supervised setting, where the samples are both input and target, missing values have to be imputed in the sample for the input space, but these values should not be considered for the loss function as their truth is unkown. \n",
    "\n",
    "### Alternatives\n",
    "\n",
    "- [`sklearn.imputer.IterativeImputer`](https://scikit-learn.org/stable/modules/impute.html#iterative-imputer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import vaep\n",
    "from vaep.transform import log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data\\\\sample_peptides_n4.pkl'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import src.file_utils as io_\n",
    "FOLDER_DATA = 'data'\n",
    "files = io_.search_files(path=FOLDER_DATA, query='.pkl')\n",
    "file = io_.check_for_key(files, 'peptides_n4') # ToDo: check for more than one key behaviour\n",
    "file # sample_peptides.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "peptides = pd.read_pickle(file)\n",
    "peptides = peptides.apply(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Sequence</th>\n",
       "      <th>AAAAAAAAAPAAAATAPTTAATTAATAAQ</th>\n",
       "      <th>AAAAAAALQAK</th>\n",
       "      <th>AAAAAAGAASGLPGPVAQGLK</th>\n",
       "      <th>AAAAAAGAGPEMVR</th>\n",
       "      <th>AAAAAGTATSQRFFQSFSDALIDEDPQAALEELTK</th>\n",
       "      <th>AAAAASRGVGAK</th>\n",
       "      <th>AAAAECDVVMAATEPELLDDQEAK</th>\n",
       "      <th>AAAAVVVPAEWIK</th>\n",
       "      <th>AAAEVAGQFVIK</th>\n",
       "      <th>AAAFEEQENETVVVK</th>\n",
       "      <th>...</th>\n",
       "      <th>YWSQQIEESTTVVTTQSAEVGAAETTLTELRR</th>\n",
       "      <th>YYALCGFGGVLSCGLTHTAVVPLDLVK</th>\n",
       "      <th>YYDQICSIEPK</th>\n",
       "      <th>YYDVMSDEEIER</th>\n",
       "      <th>YYNSDVHR</th>\n",
       "      <th>YYRVCTLAIIDPGDSDIIR</th>\n",
       "      <th>YYTEFPTVLDITAEDPSK</th>\n",
       "      <th>YYTPVPCESATAK</th>\n",
       "      <th>YYTSASGDEMVSLK</th>\n",
       "      <th>YYVTIIDAPGHR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MQ1.6.0.1_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_Hela_01_200327</th>\n",
       "      <td>20.709441</td>\n",
       "      <td>21.698618</td>\n",
       "      <td>18.533920</td>\n",
       "      <td>17.479482</td>\n",
       "      <td>17.200782</td>\n",
       "      <td>18.421680</td>\n",
       "      <td>18.269334</td>\n",
       "      <td>18.525491</td>\n",
       "      <td>18.562701</td>\n",
       "      <td>17.958788</td>\n",
       "      <td>...</td>\n",
       "      <td>17.820660</td>\n",
       "      <td>19.497933</td>\n",
       "      <td>18.332658</td>\n",
       "      <td>17.023568</td>\n",
       "      <td>17.797601</td>\n",
       "      <td>19.440322</td>\n",
       "      <td>18.273062</td>\n",
       "      <td>17.127330</td>\n",
       "      <td>19.731605</td>\n",
       "      <td>19.866894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_Hela_01_200330</th>\n",
       "      <td>20.626997</td>\n",
       "      <td>21.698618</td>\n",
       "      <td>18.533920</td>\n",
       "      <td>17.479482</td>\n",
       "      <td>17.200782</td>\n",
       "      <td>18.421680</td>\n",
       "      <td>18.269334</td>\n",
       "      <td>18.525491</td>\n",
       "      <td>18.395158</td>\n",
       "      <td>17.958788</td>\n",
       "      <td>...</td>\n",
       "      <td>17.820660</td>\n",
       "      <td>19.497933</td>\n",
       "      <td>18.332658</td>\n",
       "      <td>17.023568</td>\n",
       "      <td>17.730370</td>\n",
       "      <td>19.440322</td>\n",
       "      <td>18.273062</td>\n",
       "      <td>17.127330</td>\n",
       "      <td>19.731605</td>\n",
       "      <td>19.866894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_Hela_01_20190104110509_200331</th>\n",
       "      <td>20.922936</td>\n",
       "      <td>21.616247</td>\n",
       "      <td>19.263712</td>\n",
       "      <td>17.635352</td>\n",
       "      <td>18.393957</td>\n",
       "      <td>18.658673</td>\n",
       "      <td>18.543252</td>\n",
       "      <td>18.802195</td>\n",
       "      <td>17.974722</td>\n",
       "      <td>17.683394</td>\n",
       "      <td>...</td>\n",
       "      <td>20.778795</td>\n",
       "      <td>21.700765</td>\n",
       "      <td>18.648215</td>\n",
       "      <td>17.991343</td>\n",
       "      <td>19.239403</td>\n",
       "      <td>21.242119</td>\n",
       "      <td>18.540151</td>\n",
       "      <td>17.254261</td>\n",
       "      <td>20.077269</td>\n",
       "      <td>18.575717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_Hela_02_200331</th>\n",
       "      <td>20.771484</td>\n",
       "      <td>21.870383</td>\n",
       "      <td>18.323883</td>\n",
       "      <td>17.344691</td>\n",
       "      <td>19.156265</td>\n",
       "      <td>18.506767</td>\n",
       "      <td>18.433498</td>\n",
       "      <td>18.377303</td>\n",
       "      <td>18.511435</td>\n",
       "      <td>18.089896</td>\n",
       "      <td>...</td>\n",
       "      <td>20.136207</td>\n",
       "      <td>21.329656</td>\n",
       "      <td>18.422878</td>\n",
       "      <td>17.171745</td>\n",
       "      <td>19.026798</td>\n",
       "      <td>19.965857</td>\n",
       "      <td>18.210873</td>\n",
       "      <td>17.451912</td>\n",
       "      <td>19.815315</td>\n",
       "      <td>18.134424</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 5732 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Sequence                                            AAAAAAAAAPAAAATAPTTAATTAATAAQ  \\\n",
       "MQ1.6.0.1_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_He...                      20.709441   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...                      20.626997   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...                      20.922936   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...                      20.771484   \n",
       "\n",
       "Sequence                                            AAAAAAALQAK  \\\n",
       "MQ1.6.0.1_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_He...    21.698618   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...    21.698618   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...    21.616247   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...    21.870383   \n",
       "\n",
       "Sequence                                            AAAAAAGAASGLPGPVAQGLK  \\\n",
       "MQ1.6.0.1_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_He...              18.533920   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...              18.533920   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...              19.263712   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...              18.323883   \n",
       "\n",
       "Sequence                                            AAAAAAGAGPEMVR  \\\n",
       "MQ1.6.0.1_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_He...       17.479482   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...       17.479482   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...       17.635352   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...       17.344691   \n",
       "\n",
       "Sequence                                            AAAAAGTATSQRFFQSFSDALIDEDPQAALEELTK  \\\n",
       "MQ1.6.0.1_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_He...                            17.200782   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...                            17.200782   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...                            18.393957   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...                            19.156265   \n",
       "\n",
       "Sequence                                            AAAAASRGVGAK  \\\n",
       "MQ1.6.0.1_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_He...     18.421680   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...     18.421680   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...     18.658673   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...     18.506767   \n",
       "\n",
       "Sequence                                            AAAAECDVVMAATEPELLDDQEAK  \\\n",
       "MQ1.6.0.1_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_He...                 18.269334   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...                 18.269334   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...                 18.543252   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...                 18.433498   \n",
       "\n",
       "Sequence                                            AAAAVVVPAEWIK  \\\n",
       "MQ1.6.0.1_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_He...      18.525491   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...      18.525491   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...      18.802195   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...      18.377303   \n",
       "\n",
       "Sequence                                            AAAEVAGQFVIK  \\\n",
       "MQ1.6.0.1_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_He...     18.562701   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...     18.395158   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...     17.974722   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...     18.511435   \n",
       "\n",
       "Sequence                                            AAAFEEQENETVVVK  ...  \\\n",
       "MQ1.6.0.1_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_He...        17.958788  ...   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...        17.958788  ...   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...        17.683394  ...   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...        18.089896  ...   \n",
       "\n",
       "Sequence                                            YWSQQIEESTTVVTTQSAEVGAAETTLTELRR  \\\n",
       "MQ1.6.0.1_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_He...                         17.820660   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...                         17.820660   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...                         20.778795   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...                         20.136207   \n",
       "\n",
       "Sequence                                            YYALCGFGGVLSCGLTHTAVVPLDLVK  \\\n",
       "MQ1.6.0.1_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_He...                    19.497933   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...                    19.497933   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...                    21.700765   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...                    21.329656   \n",
       "\n",
       "Sequence                                            YYDQICSIEPK  YYDVMSDEEIER  \\\n",
       "MQ1.6.0.1_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_He...    18.332658     17.023568   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...    18.332658     17.023568   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...    18.648215     17.991343   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...    18.422878     17.171745   \n",
       "\n",
       "Sequence                                             YYNSDVHR  \\\n",
       "MQ1.6.0.1_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_He...  17.797601   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...  17.730370   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...  19.239403   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...  19.026798   \n",
       "\n",
       "Sequence                                            YYRVCTLAIIDPGDSDIIR  \\\n",
       "MQ1.6.0.1_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_He...            19.440322   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...            19.440322   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...            21.242119   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...            19.965857   \n",
       "\n",
       "Sequence                                            YYTEFPTVLDITAEDPSK  \\\n",
       "MQ1.6.0.1_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_He...           18.273062   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...           18.273062   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...           18.540151   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...           18.210873   \n",
       "\n",
       "Sequence                                            YYTPVPCESATAK  \\\n",
       "MQ1.6.0.1_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_He...      17.127330   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...      17.127330   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...      17.254261   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...      17.451912   \n",
       "\n",
       "Sequence                                            YYTSASGDEMVSLK  \\\n",
       "MQ1.6.0.1_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_He...       19.731605   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...       19.731605   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...       20.077269   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...       19.815315   \n",
       "\n",
       "Sequence                                            YYVTIIDAPGHR  \n",
       "MQ1.6.0.1_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_He...     19.866894  \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...     19.866894  \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...     18.575717  \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...     18.134424  \n",
       "\n",
       "[4 rows x 5732 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REMOVE_MISSING = True\n",
    "if REMOVE_MISSING:\n",
    "    mask = peptides.isna().sum() == 0\n",
    "    peptides = peptides.loc[:,mask]\n",
    "peptides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMPUTE = False\n",
    "if IMPUTE:\n",
    "    from vaep.imputation import imputation_normal_distribution\n",
    "    imputed = peptides.iloc[:,:10].apply(imputation_normal_distribution)\n",
    "    imputed    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_features = peptides.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impute missing values as 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detection_limit = float(int(peptides.min().min()))\n",
    "detection_limit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "peptides.fillna(detection_limit, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[20.7094, 21.6986, 18.5339,  ..., 17.1273, 19.7316, 19.8669],\n",
       "        [20.6270, 21.6986, 18.5339,  ..., 17.1273, 19.7316, 19.8669],\n",
       "        [20.9229, 21.6162, 19.2637,  ..., 17.2543, 20.0773, 18.5757],\n",
       "        [20.7715, 21.8704, 18.3239,  ..., 17.4519, 19.8153, 18.1344]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_in_memory = peptides.values\n",
    "dataset_in_memory = torch.from_numpy(dataset_in_memory)\n",
    "dataset_in_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Dataset needs a the methods `__len__` and `__getitem__, so it can be feed to a `DataLoader`, this mean the following has to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_in_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([20.6270, 21.6986, 18.5339,  ..., 17.1273, 19.7316, 19.8669],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_in_memory[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Implementation of VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Command Line Arguments\n",
    "- later parameters will be passed a final program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(batch_size=2, cuda=False, epochs=300, log_interval=10, no_cuda=True, seed=43)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vaep.cmd import parse_args\n",
    "args = parse_args(['--batch-size', '2', '--no-cuda', '--seed', '43', '--epochs', '300'])\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a DataLoader instance\n",
    "Passing the DataSet instance in memory to the DataLoader creates a generator for training which shuffles the data on training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if args.cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=dataset_in_memory,\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "# test_loader = torch.utils.data.DataLoader(\n",
    "#     datasets.MNIST('../data', train=False, transform=transforms.ToTensor()),\n",
    "#     batch_size=args.batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nummber of samples in mini-batch: 2 \tObject-Type: <class 'torch.Tensor'>\n",
      "Nummber of samples in mini-batch: 2 \tObject-Type: <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "for data in train_loader:\n",
    "    print(\"Nummber of samples in mini-batch: {}\".format(len(data)),\n",
    "          \"\\tObject-Type: {}\".format(type(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mean'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "mse_loss(input, target, size_average=None, reduce=None, reduction='mean') -> Tensor\n",
       "\n",
       "Measures the element-wise mean squared error.\n",
       "\n",
       "See :class:`~torch.nn.MSELoss` for details.\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\kzl465\\anaconda3\\envs\\vaep\\lib\\site-packages\\torch\\nn\\functional.py\n",
       "\u001b[1;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "F.mse_loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mean'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mSource:\u001b[0m   \n",
       "\u001b[1;32mdef\u001b[0m \u001b[0mmse_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mean'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# type: (Tensor, Tensor, Optional[bool], Optional[bool], str) -> Tensor\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34mr\"\"\"mse_loss(input, target, size_average=None, reduce=None, reduction='mean') -> Tensor\n",
       "\n",
       "    Measures the element-wise mean squared error.\n",
       "\n",
       "    See :class:`~torch.nn.MSELoss` for details.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Using a target size ({}) that is different to the input size ({}). \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                      \u001b[1;34m\"This will likely lead to incorrect results due to broadcasting. \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                      \u001b[1;34m\"Please ensure they have the same size.\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                      \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'none'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'mean'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mexpanded_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\kzl465\\anaconda3\\envs\\vaep\\lib\\site-packages\\torch\\nn\\functional.py\n",
       "\u001b[1;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "F.mse_loss??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        n_neurons = 1000\n",
    "\n",
    "        self.fc1 = nn.Linear(n_features, n_neurons)\n",
    "        self.fc21 = nn.Linear(n_neurons, 50)\n",
    "        self.fc22 = nn.Linear(n_neurons, 50)\n",
    "        self.fc3 = nn.Linear(50, n_neurons)\n",
    "        self.fc4 = nn.Linear(n_neurons, n_features)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "#         print(\"std: \", std)\n",
    "#         print(\"eps: \", eps)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return self.fc4(h3)\n",
    "#         return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, n_features))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "\n",
    "model = VAE().to(device)\n",
    "model.double()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    #BCE = F.binary_cross_entropy(recon_x, x.view(-1, n_features), reduction='sum')\n",
    "    MSE = F.mse_loss(input=recon_x, target=x.view(-1, n_features), reduction='sum')\n",
    "    \n",
    "#     mask = x != detection_limit\n",
    "#     MSE = torch.sum(((recon_x-x)*mask)**2.0)  / torch.sum(mask)\n",
    "    # debug_here()\n",
    "    # import pdb; pdb.set_trace()\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return 0.9*MSE + 0.1*KLD\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "#     for batch_idx, (data, _) in enumerate(train_loader):\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        print(\"Mean of mu: \", mu.mean())\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "#             if i == 0:\n",
    "#                 n = min(data.size(0), 8)\n",
    "#                 comparison = torch.cat([data[:n],\n",
    "#                                       recon_batch.view(args.batch_size, 1, 28, 28)[:n]])\n",
    "#                 save_image(comparison.cpu(),\n",
    "#                          'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of mu:  tensor(-0.1897, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 1 [0/4 (0%)]\tLoss: 4110417.886674\n",
      "Mean of mu:  tensor(-0.2156, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 1 Average loss: 2982663.4148\n",
      "Mean of mu:  tensor(-0.2573, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 2 [0/4 (0%)]\tLoss: 1922235.542813\n",
      "Mean of mu:  tensor(-0.2450, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 2 Average loss: 1906956.2212\n",
      "Mean of mu:  tensor(-0.2525, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 3 [0/4 (0%)]\tLoss: 1859422.540527\n",
      "Mean of mu:  tensor(-0.2877, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 3 Average loss: 1862520.6568\n",
      "Mean of mu:  tensor(-0.2700, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 4 [0/4 (0%)]\tLoss: 1846969.778305\n",
      "Mean of mu:  tensor(-0.3002, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 4 Average loss: 1857288.3679\n",
      "Mean of mu:  tensor(-0.3063, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 5 [0/4 (0%)]\tLoss: 1870072.036328\n",
      "Mean of mu:  tensor(-0.2926, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 5 Average loss: 1852205.1566\n",
      "Mean of mu:  tensor(-0.2971, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 6 [0/4 (0%)]\tLoss: 1835550.912376\n",
      "Mean of mu:  tensor(-0.3318, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 6 Average loss: 1843346.9338\n",
      "Mean of mu:  tensor(-0.3140, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 7 [0/4 (0%)]\tLoss: 1834686.572450\n",
      "Mean of mu:  tensor(-0.3454, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 7 Average loss: 1844388.3634\n",
      "Mean of mu:  tensor(-0.3326, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 8 [0/4 (0%)]\tLoss: 1794656.483894\n",
      "Mean of mu:  tensor(-0.3517, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 8 Average loss: 1836230.2780\n",
      "Mean of mu:  tensor(-0.3445, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 9 [0/4 (0%)]\tLoss: 1792962.093338\n",
      "Mean of mu:  tensor(-0.3616, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 9 Average loss: 1830733.9212\n",
      "Mean of mu:  tensor(-0.3504, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 10 [0/4 (0%)]\tLoss: 1824514.218736\n",
      "Mean of mu:  tensor(-0.3731, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 10 Average loss: 1830648.6163\n",
      "Mean of mu:  tensor(-0.3581, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 11 [0/4 (0%)]\tLoss: 1816139.099677\n",
      "Mean of mu:  tensor(-0.3795, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 11 Average loss: 1825884.4572\n",
      "Mean of mu:  tensor(-0.3801, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 12 [0/4 (0%)]\tLoss: 1860616.291053\n",
      "Mean of mu:  tensor(-0.3729, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 12 Average loss: 1819560.9037\n",
      "Mean of mu:  tensor(-0.3744, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 13 [0/4 (0%)]\tLoss: 1811736.622769\n",
      "Mean of mu:  tensor(-0.3944, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 13 Average loss: 1820855.4151\n",
      "Mean of mu:  tensor(-0.3821, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 14 [0/4 (0%)]\tLoss: 1804845.520378\n",
      "Mean of mu:  tensor(-0.4012, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 14 Average loss: 1814618.2163\n",
      "Mean of mu:  tensor(-0.3866, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 15 [0/4 (0%)]\tLoss: 1801815.864287\n",
      "Mean of mu:  tensor(-0.4086, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 15 Average loss: 1809707.2302\n",
      "Mean of mu:  tensor(-0.4116, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 16 [0/4 (0%)]\tLoss: 1821977.100214\n",
      "Mean of mu:  tensor(-0.3950, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 16 Average loss: 1809468.8358\n",
      "Mean of mu:  tensor(-0.3969, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 17 [0/4 (0%)]\tLoss: 1792712.703402\n",
      "Mean of mu:  tensor(-0.4179, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 17 Average loss: 1800913.5019\n",
      "Mean of mu:  tensor(-0.4017, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 18 [0/4 (0%)]\tLoss: 1789409.031084\n",
      "Mean of mu:  tensor(-0.4194, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 18 Average loss: 1793507.9246\n",
      "Mean of mu:  tensor(-0.4185, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 19 [0/4 (0%)]\tLoss: 1840255.445740\n",
      "Mean of mu:  tensor(-0.4095, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 19 Average loss: 1797791.3035\n",
      "Mean of mu:  tensor(-0.4092, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 20 [0/4 (0%)]\tLoss: 1775819.605615\n",
      "Mean of mu:  tensor(-0.4278, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 20 Average loss: 1788724.5486\n",
      "Mean of mu:  tensor(-0.4136, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 21 [0/4 (0%)]\tLoss: 1778744.425646\n",
      "Mean of mu:  tensor(-0.4303, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 21 Average loss: 1788840.5537\n",
      "Mean of mu:  tensor(-0.4153, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 22 [0/4 (0%)]\tLoss: 1781253.061862\n",
      "Mean of mu:  tensor(-0.4320, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 22 Average loss: 1785724.6419\n",
      "Mean of mu:  tensor(-0.4152, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 23 [0/4 (0%)]\tLoss: 1771064.147216\n",
      "Mean of mu:  tensor(-0.4354, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 23 Average loss: 1770576.0173\n",
      "Mean of mu:  tensor(-0.4165, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 24 [0/4 (0%)]\tLoss: 1743290.978776\n",
      "Mean of mu:  tensor(-0.4359, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 24 Average loss: 1746167.3149\n",
      "Mean of mu:  tensor(-0.4333, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 25 [0/4 (0%)]\tLoss: 1754206.929936\n",
      "Mean of mu:  tensor(-0.4173, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 25 Average loss: 1747096.3625\n",
      "Mean of mu:  tensor(-0.4297, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 26 [0/4 (0%)]\tLoss: 1781170.742030\n",
      "Mean of mu:  tensor(-0.4188, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 26 Average loss: 1738355.7140\n",
      "Mean of mu:  tensor(-0.4291, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 27 [0/4 (0%)]\tLoss: 1784848.045083\n",
      "Mean of mu:  tensor(-0.4187, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 27 Average loss: 1735566.0394\n",
      "Mean of mu:  tensor(-0.4323, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 28 [0/4 (0%)]\tLoss: 1700134.675976\n",
      "Mean of mu:  tensor(-0.4198, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 28 Average loss: 1690378.8321\n",
      "Mean of mu:  tensor(-0.4223, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 29 [0/4 (0%)]\tLoss: 1745581.217942\n",
      "Mean of mu:  tensor(-0.4395, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 29 Average loss: 1734719.3659\n",
      "Mean of mu:  tensor(-0.4379, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 30 [0/4 (0%)]\tLoss: 1753895.183859\n",
      "Mean of mu:  tensor(-0.4276, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 30 Average loss: 1700597.3829\n",
      "Mean of mu:  tensor(-0.4249, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 31 [0/4 (0%)]\tLoss: 1702815.797516\n",
      "Mean of mu:  tensor(-0.4453, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 31 Average loss: 1697571.7068\n",
      "Mean of mu:  tensor(-0.4291, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 32 [0/4 (0%)]\tLoss: 1646439.899734\n",
      "Mean of mu:  tensor(-0.4418, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 32 Average loss: 1664031.3097\n",
      "Mean of mu:  tensor(-0.4423, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 33 [0/4 (0%)]\tLoss: 1705150.053155\n",
      "Mean of mu:  tensor(-0.4263, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 33 Average loss: 1666828.7667\n",
      "Mean of mu:  tensor(-0.4421, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 34 [0/4 (0%)]\tLoss: 1717607.262263\n",
      "Mean of mu:  tensor(-0.4190, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 34 Average loss: 1827052.5839\n",
      "Mean of mu:  tensor(-0.4296, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 35 [0/4 (0%)]\tLoss: 1610058.023494\n",
      "Mean of mu:  tensor(-0.4497, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 35 Average loss: 1659520.8625\n",
      "Mean of mu:  tensor(-0.4572, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 36 [0/4 (0%)]\tLoss: 1648570.808943\n",
      "Mean of mu:  tensor(-0.4457, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 36 Average loss: 1662180.3792\n",
      "Mean of mu:  tensor(-0.4655, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 37 [0/4 (0%)]\tLoss: 1684242.251376\n",
      "Mean of mu:  tensor(-0.4616, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 37 Average loss: 1636600.8607\n",
      "Mean of mu:  tensor(-0.4864, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 38 [0/4 (0%)]\tLoss: 1593558.833297\n",
      "Mean of mu:  tensor(-0.4797, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 38 Average loss: 1610358.8044\n",
      "Mean of mu:  tensor(-0.5054, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 39 [0/4 (0%)]\tLoss: 1594079.764058\n",
      "Mean of mu:  tensor(-0.4896, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 39 Average loss: 1588174.9017\n",
      "Mean of mu:  tensor(-0.4962, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 40 [0/4 (0%)]\tLoss: 1562568.285053\n",
      "Mean of mu:  tensor(-0.5241, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 40 Average loss: 1588478.6542\n",
      "Mean of mu:  tensor(-0.5292, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 41 [0/4 (0%)]\tLoss: 1486658.452606\n",
      "Mean of mu:  tensor(-0.5060, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 41 Average loss: 2477906.9519\n",
      "Mean of mu:  tensor(-0.5981, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 42 [0/4 (0%)]\tLoss: 1478726.377302\n",
      "Mean of mu:  tensor(-0.6953, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 42 Average loss: 1598457.5030\n",
      "Mean of mu:  tensor(-0.7620, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 43 [0/4 (0%)]\tLoss: 1425488.635144\n",
      "Mean of mu:  tensor(-0.7993, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 43 Average loss: 1483304.8457\n",
      "Mean of mu:  tensor(-0.8555, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 44 [0/4 (0%)]\tLoss: 1505237.171036\n",
      "Mean of mu:  tensor(-0.9240, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 44 Average loss: 1524593.5785\n",
      "Mean of mu:  tensor(-0.9730, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 45 [0/4 (0%)]\tLoss: 1582342.370409\n",
      "Mean of mu:  tensor(-0.9935, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 45 Average loss: 1511342.5503\n",
      "Mean of mu:  tensor(-1.0329, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 46 [0/4 (0%)]\tLoss: 1566708.713806\n",
      "Mean of mu:  tensor(-1.0925, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 46 Average loss: 1563029.5720\n",
      "Mean of mu:  tensor(-1.1301, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 47 [0/4 (0%)]\tLoss: 1434768.508617\n",
      "Mean of mu:  tensor(-1.1354, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 47 Average loss: 1464852.6351\n",
      "Mean of mu:  tensor(-1.1876, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 48 [0/4 (0%)]\tLoss: 1804126.727388\n",
      "Mean of mu:  tensor(-1.1840, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 48 Average loss: 1633435.0159\n",
      "Mean of mu:  tensor(-1.2046, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 49 [0/4 (0%)]\tLoss: 1404171.918592\n",
      "Mean of mu:  tensor(-1.2467, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 49 Average loss: 1428984.3738\n",
      "Mean of mu:  tensor(-1.2630, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 50 [0/4 (0%)]\tLoss: 1351358.265154\n",
      "Mean of mu:  tensor(-1.2501, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 50 Average loss: 1370035.9021\n",
      "Mean of mu:  tensor(-1.2655, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 51 [0/4 (0%)]\tLoss: 1356766.905816\n",
      "Mean of mu:  tensor(-1.3007, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 51 Average loss: 1371394.4391\n",
      "Mean of mu:  tensor(-1.2862, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 52 [0/4 (0%)]\tLoss: 1828163.272853\n",
      "Mean of mu:  tensor(-1.3151, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 52 Average loss: 1767103.7050\n",
      "Mean of mu:  tensor(-1.2836, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 53 [0/4 (0%)]\tLoss: 1455950.506490\n",
      "Mean of mu:  tensor(-1.3064, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 53 Average loss: 1466648.2785\n",
      "Mean of mu:  tensor(-1.2764, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 54 [0/4 (0%)]\tLoss: 1433333.857681\n",
      "Mean of mu:  tensor(-1.3002, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 54 Average loss: 1496836.6984\n",
      "Mean of mu:  tensor(-1.2990, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 55 [0/4 (0%)]\tLoss: 1593872.527519\n",
      "Mean of mu:  tensor(-1.2759, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 55 Average loss: 1545714.1793\n",
      "Mean of mu:  tensor(-1.2782, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 56 [0/4 (0%)]\tLoss: 1278762.503915\n",
      "Mean of mu:  tensor(-1.3069, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 56 Average loss: 1391004.8384\n",
      "Mean of mu:  tensor(-1.3078, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 57 [0/4 (0%)]\tLoss: 1221738.694436\n",
      "Mean of mu:  tensor(-1.2909, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 57 Average loss: 1238173.1997\n",
      "Mean of mu:  tensor(-1.2907, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 58 [0/4 (0%)]\tLoss: 1392312.066401\n",
      "Mean of mu:  tensor(-1.3195, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 58 Average loss: 1569011.9497\n",
      "Mean of mu:  tensor(-1.3208, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 59 [0/4 (0%)]\tLoss: 1215593.716828\n",
      "Mean of mu:  tensor(-1.3006, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 59 Average loss: 1349406.6932\n",
      "Mean of mu:  tensor(-1.3041, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 60 [0/4 (0%)]\tLoss: 1391323.031078\n",
      "Mean of mu:  tensor(-1.3297, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 60 Average loss: 1454604.0874\n",
      "Mean of mu:  tensor(-1.3117, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 61 [0/4 (0%)]\tLoss: 1484239.517803\n",
      "Mean of mu:  tensor(-1.3430, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 61 Average loss: 1498864.8080\n",
      "Mean of mu:  tensor(-1.3472, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 62 [0/4 (0%)]\tLoss: 1444816.091406\n",
      "Mean of mu:  tensor(-1.3376, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 62 Average loss: 1478836.4589\n",
      "Mean of mu:  tensor(-1.3479, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 63 [0/4 (0%)]\tLoss: 1502219.762653\n",
      "Mean of mu:  tensor(-1.3768, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 63 Average loss: 1476953.1792\n",
      "Mean of mu:  tensor(-1.3707, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 64 [0/4 (0%)]\tLoss: 1308600.872585\n",
      "Mean of mu:  tensor(-1.4031, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 64 Average loss: 1393312.3704\n",
      "Mean of mu:  tensor(-1.4180, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 65 [0/4 (0%)]\tLoss: 1378791.422117\n",
      "Mean of mu:  tensor(-1.4171, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 65 Average loss: 1363299.7636\n",
      "Mean of mu:  tensor(-1.4589, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 66 [0/4 (0%)]\tLoss: 1272245.234380\n",
      "Mean of mu:  tensor(-1.4493, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 66 Average loss: 1175398.4048\n",
      "Mean of mu:  tensor(-1.4707, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 67 [0/4 (0%)]\tLoss: 1269914.256991\n",
      "Mean of mu:  tensor(-1.4946, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 67 Average loss: 1315866.0084\n",
      "Mean of mu:  tensor(-1.4680, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 68 [0/4 (0%)]\tLoss: 1209031.411756\n",
      "Mean of mu:  tensor(-1.4939, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 68 Average loss: 1275207.3708\n",
      "Mean of mu:  tensor(-1.4698, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 69 [0/4 (0%)]\tLoss: 1178016.091227\n",
      "Mean of mu:  tensor(-1.4904, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 69 Average loss: 1230503.8389\n",
      "Mean of mu:  tensor(-1.4998, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 70 [0/4 (0%)]\tLoss: 1293727.606902\n",
      "Mean of mu:  tensor(-1.4782, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 70 Average loss: 1253331.3094\n",
      "Mean of mu:  tensor(-1.5153, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 71 [0/4 (0%)]\tLoss: 1105929.842988\n",
      "Mean of mu:  tensor(-1.4972, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 71 Average loss: 1081580.8034\n",
      "Mean of mu:  tensor(-1.5342, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 72 [0/4 (0%)]\tLoss: 1347393.084321\n",
      "Mean of mu:  tensor(-1.5170, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 72 Average loss: 1271981.1214\n",
      "Mean of mu:  tensor(-1.5334, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 73 [0/4 (0%)]\tLoss: 1015337.208191\n",
      "Mean of mu:  tensor(-1.5631, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 73 Average loss: 1298793.9995\n",
      "Mean of mu:  tensor(-1.5698, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 74 [0/4 (0%)]\tLoss: 1053136.569969\n",
      "Mean of mu:  tensor(-1.5566, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 74 Average loss: 1078084.2486\n",
      "Mean of mu:  tensor(-1.5563, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 75 [0/4 (0%)]\tLoss: 1054950.591258\n",
      "Mean of mu:  tensor(-1.5895, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 75 Average loss: 1139183.1841\n",
      "Mean of mu:  tensor(-1.5713, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 76 [0/4 (0%)]\tLoss: 1121297.926057\n",
      "Mean of mu:  tensor(-1.6020, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 76 Average loss: 1510909.6374\n",
      "Mean of mu:  tensor(-1.5665, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 77 [0/4 (0%)]\tLoss: 1281124.373388\n",
      "Mean of mu:  tensor(-1.5733, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 77 Average loss: 1312319.5809\n",
      "Mean of mu:  tensor(-1.5430, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 78 [0/4 (0%)]\tLoss: 1280071.452303\n",
      "Mean of mu:  tensor(-1.5680, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 78 Average loss: 1318407.5245\n",
      "Mean of mu:  tensor(-1.5455, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 79 [0/4 (0%)]\tLoss: 1340031.623788\n",
      "Mean of mu:  tensor(-1.5658, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 79 Average loss: 1329132.6243\n",
      "Mean of mu:  tensor(-1.5490, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 80 [0/4 (0%)]\tLoss: 1322940.170313\n",
      "Mean of mu:  tensor(-1.5772, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 80 Average loss: 1313947.0392\n",
      "Mean of mu:  tensor(-1.5855, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 81 [0/4 (0%)]\tLoss: 1342515.972453\n",
      "Mean of mu:  tensor(-1.5642, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 81 Average loss: 1296532.7052\n",
      "Mean of mu:  tensor(-1.5771, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 82 [0/4 (0%)]\tLoss: 1274391.967936\n",
      "Mean of mu:  tensor(-1.6054, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 82 Average loss: 1273802.5181\n",
      "Mean of mu:  tensor(-1.5945, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 83 [0/4 (0%)]\tLoss: 1277840.088714\n",
      "Mean of mu:  tensor(-1.6298, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 83 Average loss: 1262349.4740\n",
      "Mean of mu:  tensor(-1.6425, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 84 [0/4 (0%)]\tLoss: 1234047.399947\n",
      "Mean of mu:  tensor(-1.6310, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 84 Average loss: 1218474.3717\n",
      "Mean of mu:  tensor(-1.6732, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 85 [0/4 (0%)]\tLoss: 1255565.473029\n",
      "Mean of mu:  tensor(-1.6578, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 85 Average loss: 1239133.6837\n",
      "Mean of mu:  tensor(-1.7018, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 86 [0/4 (0%)]\tLoss: 1244282.168922\n",
      "Mean of mu:  tensor(-1.6902, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 86 Average loss: 1195994.8312\n",
      "Mean of mu:  tensor(-1.7320, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 87 [0/4 (0%)]\tLoss: 1155814.572950\n",
      "Mean of mu:  tensor(-1.7217, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 87 Average loss: 1155127.3944\n",
      "Mean of mu:  tensor(-1.7651, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 88 [0/4 (0%)]\tLoss: 1001284.175637\n",
      "Mean of mu:  tensor(-1.7578, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 88 Average loss: 962318.1014\n",
      "Mean of mu:  tensor(-1.7764, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 89 [0/4 (0%)]\tLoss: 1085249.200625\n",
      "Mean of mu:  tensor(-1.8357, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 89 Average loss: 1025963.8164\n",
      "Mean of mu:  tensor(-1.8561, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 90 [0/4 (0%)]\tLoss: 877995.480354\n",
      "Mean of mu:  tensor(-1.8594, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 90 Average loss: 1205234.6462\n",
      "Mean of mu:  tensor(-1.8592, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 91 [0/4 (0%)]\tLoss: 1031451.744379\n",
      "Mean of mu:  tensor(-1.8225, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 91 Average loss: 951572.1767\n",
      "Mean of mu:  tensor(-1.8049, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 92 [0/4 (0%)]\tLoss: 979565.319498\n",
      "Mean of mu:  tensor(-1.8318, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 92 Average loss: 1030869.8773\n",
      "Mean of mu:  tensor(-1.8271, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 93 [0/4 (0%)]\tLoss: 1019036.503948\n",
      "Mean of mu:  tensor(-1.7883, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 93 Average loss: 1013140.8582\n",
      "Mean of mu:  tensor(-1.7884, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 94 [0/4 (0%)]\tLoss: 913644.369176\n",
      "Mean of mu:  tensor(-1.8292, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 94 Average loss: 983258.5292\n",
      "Mean of mu:  tensor(-1.8265, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 95 [0/4 (0%)]\tLoss: 871813.879378\n",
      "Mean of mu:  tensor(-1.8122, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 95 Average loss: 922088.9842\n",
      "Mean of mu:  tensor(-1.8179, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 96 [0/4 (0%)]\tLoss: 925252.366929\n",
      "Mean of mu:  tensor(-1.8446, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 96 Average loss: 849385.1663\n",
      "Mean of mu:  tensor(-1.8522, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 97 [0/4 (0%)]\tLoss: 1592420.315476\n",
      "Mean of mu:  tensor(-1.8240, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 97 Average loss: 1245662.4400\n",
      "Mean of mu:  tensor(-1.8241, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 98 [0/4 (0%)]\tLoss: 800903.801719\n",
      "Mean of mu:  tensor(-1.8447, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 98 Average loss: 781491.4762\n",
      "Mean of mu:  tensor(-1.8440, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 99 [0/4 (0%)]\tLoss: 917030.151597\n",
      "Mean of mu:  tensor(-1.8263, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 99 Average loss: 960582.5872\n",
      "Mean of mu:  tensor(-1.8306, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 100 [0/4 (0%)]\tLoss: 822695.790864\n",
      "Mean of mu:  tensor(-1.8684, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 100 Average loss: 801147.5184\n",
      "Mean of mu:  tensor(-1.8865, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 101 [0/4 (0%)]\tLoss: 818303.693115\n",
      "Mean of mu:  tensor(-1.8643, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 101 Average loss: 813329.7027\n",
      "Mean of mu:  tensor(-1.8810, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 102 [0/4 (0%)]\tLoss: 819362.226603\n",
      "Mean of mu:  tensor(-1.9167, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 102 Average loss: 875368.6065\n",
      "Mean of mu:  tensor(-1.8910, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 103 [0/4 (0%)]\tLoss: 719081.922861\n",
      "Mean of mu:  tensor(-1.9334, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 103 Average loss: 742022.1216\n",
      "Mean of mu:  tensor(-1.9092, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 104 [0/4 (0%)]\tLoss: 1943626.238512\n",
      "Mean of mu:  tensor(-1.8560, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 104 Average loss: 1351802.2202\n",
      "Mean of mu:  tensor(-1.7553, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 105 [0/4 (0%)]\tLoss: 712472.276132\n",
      "Mean of mu:  tensor(-1.7238, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 105 Average loss: 687422.5512\n",
      "Mean of mu:  tensor(-1.6372, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 106 [0/4 (0%)]\tLoss: 735528.528692\n",
      "Mean of mu:  tensor(-1.6095, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 106 Average loss: 737006.8218\n",
      "Mean of mu:  tensor(-1.5634, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 107 [0/4 (0%)]\tLoss: 853477.922688\n",
      "Mean of mu:  tensor(-1.4981, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 107 Average loss: 769773.9639\n",
      "Mean of mu:  tensor(-1.4878, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 108 [0/4 (0%)]\tLoss: 747017.101869\n",
      "Mean of mu:  tensor(-1.4342, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 108 Average loss: 746709.7602\n",
      "Mean of mu:  tensor(-1.4052, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 109 [0/4 (0%)]\tLoss: 700664.935299\n",
      "Mean of mu:  tensor(-1.4012, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 109 Average loss: 676651.1971\n",
      "Mean of mu:  tensor(-1.3789, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 110 [0/4 (0%)]\tLoss: 727888.619038\n",
      "Mean of mu:  tensor(-1.3362, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 110 Average loss: 727387.8947\n",
      "Mean of mu:  tensor(-1.3201, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 111 [0/4 (0%)]\tLoss: 658047.117064\n",
      "Mean of mu:  tensor(-1.3264, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 111 Average loss: 662007.6188\n",
      "Mean of mu:  tensor(-1.3121, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 112 [0/4 (0%)]\tLoss: 753481.759746\n",
      "Mean of mu:  tensor(-1.2839, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 112 Average loss: 750745.3234\n",
      "Mean of mu:  tensor(-1.3020, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 113 [0/4 (0%)]\tLoss: 684876.472575\n",
      "Mean of mu:  tensor(-1.2882, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 113 Average loss: 674406.5920\n",
      "Mean of mu:  tensor(-1.2870, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 114 [0/4 (0%)]\tLoss: 756011.593497\n",
      "Mean of mu:  tensor(-1.3018, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 114 Average loss: 677897.2782\n",
      "Mean of mu:  tensor(-1.3018, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 115 [0/4 (0%)]\tLoss: 755120.203823\n",
      "Mean of mu:  tensor(-1.2877, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 115 Average loss: 643018.7681\n",
      "Mean of mu:  tensor(-1.2863, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 116 [0/4 (0%)]\tLoss: 755623.966135\n",
      "Mean of mu:  tensor(-1.3100, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 116 Average loss: 724098.2860\n",
      "Mean of mu:  tensor(-1.3124, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 117 [0/4 (0%)]\tLoss: 671043.088034\n",
      "Mean of mu:  tensor(-1.2955, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 117 Average loss: 604884.1930\n",
      "Mean of mu:  tensor(-1.3019, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 118 [0/4 (0%)]\tLoss: 615004.064123\n",
      "Mean of mu:  tensor(-1.3222, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 118 Average loss: 786046.7197\n",
      "Mean of mu:  tensor(-1.3101, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 119 [0/4 (0%)]\tLoss: 595241.451834\n",
      "Mean of mu:  tensor(-1.3375, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 119 Average loss: 700657.5989\n",
      "Mean of mu:  tensor(-1.3182, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 120 [0/4 (0%)]\tLoss: 824183.170436\n",
      "Mean of mu:  tensor(-1.3435, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 120 Average loss: 994538.5191\n",
      "Mean of mu:  tensor(-1.3285, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 121 [0/4 (0%)]\tLoss: 645944.284725\n",
      "Mean of mu:  tensor(-1.3014, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 121 Average loss: 684998.0769\n",
      "Mean of mu:  tensor(-1.2930, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 122 [0/4 (0%)]\tLoss: 683060.352965\n",
      "Mean of mu:  tensor(-1.3017, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 122 Average loss: 723385.0753\n",
      "Mean of mu:  tensor(-1.2968, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 123 [0/4 (0%)]\tLoss: 684348.240750\n",
      "Mean of mu:  tensor(-1.2667, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 123 Average loss: 672231.7378\n",
      "Mean of mu:  tensor(-1.2772, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 124 [0/4 (0%)]\tLoss: 606991.871017\n",
      "Mean of mu:  tensor(-1.2545, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 124 Average loss: 648890.2568\n",
      "Mean of mu:  tensor(-1.2457, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 125 [0/4 (0%)]\tLoss: 540101.226808\n",
      "Mean of mu:  tensor(-1.2594, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 125 Average loss: 596749.5591\n",
      "Mean of mu:  tensor(-1.2520, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 126 [0/4 (0%)]\tLoss: 585782.914192\n",
      "Mean of mu:  tensor(-1.2258, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 126 Average loss: 583779.8115\n",
      "Mean of mu:  tensor(-1.2211, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 127 [0/4 (0%)]\tLoss: 652560.805269\n",
      "Mean of mu:  tensor(-1.2369, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 127 Average loss: 896720.1024\n",
      "Mean of mu:  tensor(-1.2342, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 128 [0/4 (0%)]\tLoss: 426991.706009\n",
      "Mean of mu:  tensor(-1.2613, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 128 Average loss: 556578.6765\n",
      "Mean of mu:  tensor(-1.2727, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 129 [0/4 (0%)]\tLoss: 578150.400908\n",
      "Mean of mu:  tensor(-1.2677, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 129 Average loss: 572575.8390\n",
      "Mean of mu:  tensor(-1.2768, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 130 [0/4 (0%)]\tLoss: 551904.514721\n",
      "Mean of mu:  tensor(-1.2990, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 130 Average loss: 612121.7569\n",
      "Mean of mu:  tensor(-1.2916, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 131 [0/4 (0%)]\tLoss: 625675.275433\n",
      "Mean of mu:  tensor(-1.3197, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 131 Average loss: 629630.0758\n",
      "Mean of mu:  tensor(-1.3288, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 132 [0/4 (0%)]\tLoss: 614220.283440\n",
      "Mean of mu:  tensor(-1.3178, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 132 Average loss: 613801.1517\n",
      "Mean of mu:  tensor(-1.3470, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 133 [0/4 (0%)]\tLoss: 507173.146314\n",
      "Mean of mu:  tensor(-1.3351, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 133 Average loss: 511869.5175\n",
      "Mean of mu:  tensor(-1.3635, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 134 [0/4 (0%)]\tLoss: 550012.390548\n",
      "Mean of mu:  tensor(-1.3504, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 134 Average loss: 514894.1776\n",
      "Mean of mu:  tensor(-1.3606, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 135 [0/4 (0%)]\tLoss: 535354.380561\n",
      "Mean of mu:  tensor(-1.3829, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 135 Average loss: 569981.0719\n",
      "Mean of mu:  tensor(-1.3908, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 136 [0/4 (0%)]\tLoss: 407926.365297\n",
      "Mean of mu:  tensor(-1.3828, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 136 Average loss: 446511.8817\n",
      "Mean of mu:  tensor(-1.4063, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 137 [0/4 (0%)]\tLoss: 472003.262018\n",
      "Mean of mu:  tensor(-1.3912, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 137 Average loss: 445103.2951\n",
      "Mean of mu:  tensor(-1.4165, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 138 [0/4 (0%)]\tLoss: 415450.295493\n",
      "Mean of mu:  tensor(-1.4012, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 138 Average loss: 435831.1591\n",
      "Mean of mu:  tensor(-1.4054, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 139 [0/4 (0%)]\tLoss: 452340.437228\n",
      "Mean of mu:  tensor(-1.4323, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 139 Average loss: 521991.4332\n",
      "Mean of mu:  tensor(-1.4132, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 140 [0/4 (0%)]\tLoss: 386407.001935\n",
      "Mean of mu:  tensor(-1.4293, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 140 Average loss: 452612.8502\n",
      "Mean of mu:  tensor(-1.4119, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 141 [0/4 (0%)]\tLoss: 438883.858106\n",
      "Mean of mu:  tensor(-1.4350, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 141 Average loss: 432776.7942\n",
      "Mean of mu:  tensor(-1.4359, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 142 [0/4 (0%)]\tLoss: 378650.736616\n",
      "Mean of mu:  tensor(-1.4239, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 142 Average loss: 369796.2875\n",
      "Mean of mu:  tensor(-1.4427, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 143 [0/4 (0%)]\tLoss: 359633.123164\n",
      "Mean of mu:  tensor(-1.4305, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 143 Average loss: 417632.1786\n",
      "Mean of mu:  tensor(-1.4312, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 144 [0/4 (0%)]\tLoss: 392458.834647\n",
      "Mean of mu:  tensor(-1.4552, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 144 Average loss: 384394.9281\n",
      "Mean of mu:  tensor(-1.4402, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 145 [0/4 (0%)]\tLoss: 424103.745430\n",
      "Mean of mu:  tensor(-1.4592, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 145 Average loss: 388360.1498\n",
      "Mean of mu:  tensor(-1.4445, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 146 [0/4 (0%)]\tLoss: 356934.271943\n",
      "Mean of mu:  tensor(-1.4686, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 146 Average loss: 373632.4866\n",
      "Mean of mu:  tensor(-1.4522, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 147 [0/4 (0%)]\tLoss: 396896.686585\n",
      "Mean of mu:  tensor(-1.4744, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 147 Average loss: 414822.7458\n",
      "Mean of mu:  tensor(-1.4810, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 148 [0/4 (0%)]\tLoss: 360478.516537\n",
      "Mean of mu:  tensor(-1.4660, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 148 Average loss: 372305.7072\n",
      "Mean of mu:  tensor(-1.4911, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 149 [0/4 (0%)]\tLoss: 330590.937430\n",
      "Mean of mu:  tensor(-1.4816, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 149 Average loss: 308140.1192\n",
      "Mean of mu:  tensor(-1.4875, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 150 [0/4 (0%)]\tLoss: 284590.741475\n",
      "Mean of mu:  tensor(-1.5092, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 150 Average loss: 297924.9705\n",
      "Mean of mu:  tensor(-1.5149, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 151 [0/4 (0%)]\tLoss: 400283.852962\n",
      "Mean of mu:  tensor(-1.5051, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 151 Average loss: 368451.2965\n",
      "Mean of mu:  tensor(-1.5298, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 152 [0/4 (0%)]\tLoss: 389415.159495\n",
      "Mean of mu:  tensor(-1.5147, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 152 Average loss: 340073.1857\n",
      "Mean of mu:  tensor(-1.5208, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 153 [0/4 (0%)]\tLoss: 276501.318266\n",
      "Mean of mu:  tensor(-1.5468, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 153 Average loss: 310775.6295\n",
      "Mean of mu:  tensor(-1.5313, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 154 [0/4 (0%)]\tLoss: 374307.499262\n",
      "Mean of mu:  tensor(-1.5577, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 154 Average loss: 331888.0943\n",
      "Mean of mu:  tensor(-1.5445, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 155 [0/4 (0%)]\tLoss: 354000.492299\n",
      "Mean of mu:  tensor(-1.5659, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 155 Average loss: 324898.5009\n",
      "Mean of mu:  tensor(-1.5526, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 156 [0/4 (0%)]\tLoss: 274146.099748\n",
      "Mean of mu:  tensor(-1.5792, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 156 Average loss: 405576.3604\n",
      "Mean of mu:  tensor(-1.5836, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 157 [0/4 (0%)]\tLoss: 288447.822699\n",
      "Mean of mu:  tensor(-1.5739, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 157 Average loss: 293628.2919\n",
      "Mean of mu:  tensor(-1.5990, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 158 [0/4 (0%)]\tLoss: 282042.915324\n",
      "Mean of mu:  tensor(-1.5834, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 158 Average loss: 280848.0137\n",
      "Mean of mu:  tensor(-1.5891, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 159 [0/4 (0%)]\tLoss: 307073.125695\n",
      "Mean of mu:  tensor(-1.6162, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 159 Average loss: 303927.9925\n",
      "Mean of mu:  tensor(-1.6189, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 160 [0/4 (0%)]\tLoss: 235080.879988\n",
      "Mean of mu:  tensor(-1.6077, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 160 Average loss: 253577.9792\n",
      "Mean of mu:  tensor(-1.6323, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 161 [0/4 (0%)]\tLoss: 271442.294962\n",
      "Mean of mu:  tensor(-1.6143, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 161 Average loss: 271476.2762\n",
      "Mean of mu:  tensor(-1.6188, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 162 [0/4 (0%)]\tLoss: 256904.574001\n",
      "Mean of mu:  tensor(-1.6464, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 162 Average loss: 293343.6131\n",
      "Mean of mu:  tensor(-1.6478, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 163 [0/4 (0%)]\tLoss: 263969.186632\n",
      "Mean of mu:  tensor(-1.6362, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 163 Average loss: 256176.9829\n",
      "Mean of mu:  tensor(-1.6607, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 164 [0/4 (0%)]\tLoss: 245676.455180\n",
      "Mean of mu:  tensor(-1.6418, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 164 Average loss: 253332.2825\n",
      "Mean of mu:  tensor(-1.6658, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 165 [0/4 (0%)]\tLoss: 219793.243295\n",
      "Mean of mu:  tensor(-1.6533, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 165 Average loss: 228698.7773\n",
      "Mean of mu:  tensor(-1.6549, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 166 [0/4 (0%)]\tLoss: 250675.848813\n",
      "Mean of mu:  tensor(-1.6813, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 166 Average loss: 247436.2283\n",
      "Mean of mu:  tensor(-1.6625, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 167 [0/4 (0%)]\tLoss: 217563.854675\n",
      "Mean of mu:  tensor(-1.6905, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 167 Average loss: 217384.7156\n",
      "Mean of mu:  tensor(-1.6938, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 168 [0/4 (0%)]\tLoss: 221411.038498\n",
      "Mean of mu:  tensor(-1.6755, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 168 Average loss: 236033.3895\n",
      "Mean of mu:  tensor(-1.6785, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 169 [0/4 (0%)]\tLoss: 226115.213303\n",
      "Mean of mu:  tensor(-1.7068, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 169 Average loss: 218039.4660\n",
      "Mean of mu:  tensor(-1.6903, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 170 [0/4 (0%)]\tLoss: 203735.980771\n",
      "Mean of mu:  tensor(-1.7115, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 170 Average loss: 205345.5423\n",
      "Mean of mu:  tensor(-1.6949, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 171 [0/4 (0%)]\tLoss: 213818.983719\n",
      "Mean of mu:  tensor(-1.7246, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 171 Average loss: 452796.2940\n",
      "Mean of mu:  tensor(-1.7454, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 172 [0/4 (0%)]\tLoss: 193403.598727\n",
      "Mean of mu:  tensor(-1.7389, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 172 Average loss: 191244.1601\n",
      "Mean of mu:  tensor(-1.7804, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 173 [0/4 (0%)]\tLoss: 233674.500012\n",
      "Mean of mu:  tensor(-1.7739, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 173 Average loss: 201735.8881\n",
      "Mean of mu:  tensor(-1.7889, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 174 [0/4 (0%)]\tLoss: 173980.454812\n",
      "Mean of mu:  tensor(-1.8256, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 174 Average loss: 172306.1381\n",
      "Mean of mu:  tensor(-1.8179, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 175 [0/4 (0%)]\tLoss: 173267.778403\n",
      "Mean of mu:  tensor(-1.8472, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 175 Average loss: 183416.2087\n",
      "Mean of mu:  tensor(-1.8582, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 176 [0/4 (0%)]\tLoss: 202995.994062\n",
      "Mean of mu:  tensor(-1.8506, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 176 Average loss: 176832.3910\n",
      "Mean of mu:  tensor(-1.8599, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 177 [0/4 (0%)]\tLoss: 154733.823189\n",
      "Mean of mu:  tensor(-1.8866, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 177 Average loss: 158167.8192\n",
      "Mean of mu:  tensor(-1.9001, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 178 [0/4 (0%)]\tLoss: 148185.902287\n",
      "Mean of mu:  tensor(-1.8799, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 178 Average loss: 154294.4227\n",
      "Mean of mu:  tensor(-1.8874, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 179 [0/4 (0%)]\tLoss: 138346.875476\n",
      "Mean of mu:  tensor(-1.9233, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 179 Average loss: 170700.1600\n",
      "Mean of mu:  tensor(-1.9304, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 180 [0/4 (0%)]\tLoss: 135932.608064\n",
      "Mean of mu:  tensor(-1.9080, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 180 Average loss: 142974.0576\n",
      "Mean of mu:  tensor(-1.9141, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 181 [0/4 (0%)]\tLoss: 138123.585316\n",
      "Mean of mu:  tensor(-1.9489, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 181 Average loss: 133834.1158\n",
      "Mean of mu:  tensor(-1.9250, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 182 [0/4 (0%)]\tLoss: 129376.027794\n",
      "Mean of mu:  tensor(-1.9595, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 182 Average loss: 126646.1585\n",
      "Mean of mu:  tensor(-1.9614, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 183 [0/4 (0%)]\tLoss: 123373.951970\n",
      "Mean of mu:  tensor(-1.9423, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 183 Average loss: 122306.7496\n",
      "Mean of mu:  tensor(-1.9438, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 184 [0/4 (0%)]\tLoss: 131193.159571\n",
      "Mean of mu:  tensor(-1.9777, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 184 Average loss: 132197.6507\n",
      "Mean of mu:  tensor(-1.9525, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 185 [0/4 (0%)]\tLoss: 126346.043865\n",
      "Mean of mu:  tensor(-1.9861, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 185 Average loss: 119181.2023\n",
      "Mean of mu:  tensor(-1.9606, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 186 [0/4 (0%)]\tLoss: 115039.880873\n",
      "Mean of mu:  tensor(-1.9939, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 186 Average loss: 125497.5470\n",
      "Mean of mu:  tensor(-1.9926, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 187 [0/4 (0%)]\tLoss: 119831.059382\n",
      "Mean of mu:  tensor(-1.9776, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 187 Average loss: 113743.0094\n",
      "Mean of mu:  tensor(-2.0058, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 188 [0/4 (0%)]\tLoss: 104603.659857\n",
      "Mean of mu:  tensor(-1.9798, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 188 Average loss: 97774.9917\n",
      "Mean of mu:  tensor(-1.9886, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 189 [0/4 (0%)]\tLoss: 104578.252577\n",
      "Mean of mu:  tensor(-2.0108, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 189 Average loss: 102014.8303\n",
      "Mean of mu:  tensor(-1.9922, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 190 [0/4 (0%)]\tLoss: 98173.101581\n",
      "Mean of mu:  tensor(-2.0190, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 190 Average loss: 92036.3863\n",
      "Mean of mu:  tensor(-1.9944, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 191 [0/4 (0%)]\tLoss: 96272.903157\n",
      "Mean of mu:  tensor(-2.0266, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 191 Average loss: 100539.4812\n",
      "Mean of mu:  tensor(-2.0234, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 192 [0/4 (0%)]\tLoss: 90650.810021\n",
      "Mean of mu:  tensor(-2.0061, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 192 Average loss: 88438.9621\n",
      "Mean of mu:  tensor(-2.0053, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 193 [0/4 (0%)]\tLoss: 80277.077888\n",
      "Mean of mu:  tensor(-2.0312, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 193 Average loss: 89072.2468\n",
      "Mean of mu:  tensor(-2.0359, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 194 [0/4 (0%)]\tLoss: 76461.530305\n",
      "Mean of mu:  tensor(-2.0073, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 194 Average loss: 75781.7117\n",
      "Mean of mu:  tensor(-2.0142, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 195 [0/4 (0%)]\tLoss: 82802.595208\n",
      "Mean of mu:  tensor(-2.0348, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 195 Average loss: 85030.0055\n",
      "Mean of mu:  tensor(-2.0115, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 196 [0/4 (0%)]\tLoss: 66219.248148\n",
      "Mean of mu:  tensor(-2.0431, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 196 Average loss: 69478.5575\n",
      "Mean of mu:  tensor(-2.0141, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 197 [0/4 (0%)]\tLoss: 70843.688947\n",
      "Mean of mu:  tensor(-2.0458, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 197 Average loss: 73158.7089\n",
      "Mean of mu:  tensor(-2.0167, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 198 [0/4 (0%)]\tLoss: 68257.755329\n",
      "Mean of mu:  tensor(-2.0483, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 198 Average loss: 69597.0653\n",
      "Mean of mu:  tensor(-2.0244, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 199 [0/4 (0%)]\tLoss: 62860.742691\n",
      "Mean of mu:  tensor(-2.0444, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 199 Average loss: 64219.0550\n",
      "Mean of mu:  tensor(-2.0232, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 200 [0/4 (0%)]\tLoss: 64872.622804\n",
      "Mean of mu:  tensor(-2.0480, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 200 Average loss: 64878.8877\n",
      "Mean of mu:  tensor(-2.0267, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 201 [0/4 (0%)]\tLoss: 66693.649470\n",
      "Mean of mu:  tensor(-2.0467, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 201 Average loss: 68782.7130\n",
      "Mean of mu:  tensor(-2.0257, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 202 [0/4 (0%)]\tLoss: 61368.576734\n",
      "Mean of mu:  tensor(-2.0510, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 202 Average loss: 59850.9857\n",
      "Mean of mu:  tensor(-2.0551, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 203 [0/4 (0%)]\tLoss: 66497.185428\n",
      "Mean of mu:  tensor(-2.0250, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 203 Average loss: 63094.2196\n",
      "Mean of mu:  tensor(-2.0292, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 204 [0/4 (0%)]\tLoss: 58648.378391\n",
      "Mean of mu:  tensor(-2.0540, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 204 Average loss: 56439.0869\n",
      "Mean of mu:  tensor(-2.0582, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 205 [0/4 (0%)]\tLoss: 54388.671333\n",
      "Mean of mu:  tensor(-2.0278, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 205 Average loss: 54266.7225\n",
      "Mean of mu:  tensor(-2.0537, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 206 [0/4 (0%)]\tLoss: 50427.218564\n",
      "Mean of mu:  tensor(-2.0351, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 206 Average loss: 48873.7075\n",
      "Mean of mu:  tensor(-2.0612, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 207 [0/4 (0%)]\tLoss: 54823.820305\n",
      "Mean of mu:  tensor(-2.0306, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 207 Average loss: 50902.7417\n",
      "Mean of mu:  tensor(-2.0627, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 208 [0/4 (0%)]\tLoss: 56379.394081\n",
      "Mean of mu:  tensor(-2.0320, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 208 Average loss: 53426.8030\n",
      "Mean of mu:  tensor(-2.0578, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 209 [0/4 (0%)]\tLoss: 45809.182349\n",
      "Mean of mu:  tensor(-2.0390, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 209 Average loss: 45535.5060\n",
      "Mean of mu:  tensor(-2.0610, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 210 [0/4 (0%)]\tLoss: 44538.246667\n",
      "Mean of mu:  tensor(-2.0373, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 210 Average loss: 43800.2347\n",
      "Mean of mu:  tensor(-2.0400, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 211 [0/4 (0%)]\tLoss: 46330.199202\n",
      "Mean of mu:  tensor(-2.0596, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 211 Average loss: 45379.3234\n",
      "Mean of mu:  tensor(-2.0624, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 212 [0/4 (0%)]\tLoss: 44713.663382\n",
      "Mean of mu:  tensor(-2.0390, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 212 Average loss: 43264.4917\n",
      "Mean of mu:  tensor(-2.0354, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 213 [0/4 (0%)]\tLoss: 38089.953096\n",
      "Mean of mu:  tensor(-2.0675, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 213 Average loss: 41365.4236\n",
      "Mean of mu:  tensor(-2.0614, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 214 [0/4 (0%)]\tLoss: 40107.201379\n",
      "Mean of mu:  tensor(-2.0425, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 214 Average loss: 40023.1165\n",
      "Mean of mu:  tensor(-2.0364, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 215 [0/4 (0%)]\tLoss: 37698.417990\n",
      "Mean of mu:  tensor(-2.0686, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 215 Average loss: 38949.4437\n",
      "Mean of mu:  tensor(-2.0651, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 216 [0/4 (0%)]\tLoss: 37543.004604\n",
      "Mean of mu:  tensor(-2.0417, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 216 Average loss: 36401.5625\n",
      "Mean of mu:  tensor(-2.0420, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 217 [0/4 (0%)]\tLoss: 34686.440269\n",
      "Mean of mu:  tensor(-2.0662, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 217 Average loss: 34856.4760\n",
      "Mean of mu:  tensor(-2.0450, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 218 [0/4 (0%)]\tLoss: 37530.412673\n",
      "Mean of mu:  tensor(-2.0642, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 218 Average loss: 35203.7222\n",
      "Mean of mu:  tensor(-2.0454, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 219 [0/4 (0%)]\tLoss: 36293.086805\n",
      "Mean of mu:  tensor(-2.0646, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 219 Average loss: 35296.0193\n",
      "Mean of mu:  tensor(-2.0458, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 220 [0/4 (0%)]\tLoss: 30653.830618\n",
      "Mean of mu:  tensor(-2.0649, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 220 Average loss: 31989.3482\n",
      "Mean of mu:  tensor(-2.0671, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 221 [0/4 (0%)]\tLoss: 35157.940219\n",
      "Mean of mu:  tensor(-2.0433, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 221 Average loss: 33376.9408\n",
      "Mean of mu:  tensor(-2.0456, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 222 [0/4 (0%)]\tLoss: 30589.244620\n",
      "Mean of mu:  tensor(-2.0645, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 222 Average loss: 30352.0438\n",
      "Mean of mu:  tensor(-2.0711, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 223 [0/4 (0%)]\tLoss: 29648.043633\n",
      "Mean of mu:  tensor(-2.0388, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 223 Average loss: 29688.8442\n",
      "Mean of mu:  tensor(-2.0388, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 224 [0/4 (0%)]\tLoss: 29340.483778\n",
      "Mean of mu:  tensor(-2.0708, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 224 Average loss: 28942.1478\n",
      "Mean of mu:  tensor(-2.0429, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 225 [0/4 (0%)]\tLoss: 26893.547430\n",
      "Mean of mu:  tensor(-2.0665, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 225 Average loss: 27893.3236\n",
      "Mean of mu:  tensor(-2.0428, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 226 [0/4 (0%)]\tLoss: 26079.253836\n",
      "Mean of mu:  tensor(-2.0664, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 226 Average loss: 27805.9784\n",
      "Mean of mu:  tensor(-2.0705, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 227 [0/4 (0%)]\tLoss: 27121.577521\n",
      "Mean of mu:  tensor(-2.0383, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 227 Average loss: 25983.1842\n",
      "Mean of mu:  tensor(-2.0637, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 228 [0/4 (0%)]\tLoss: 27367.933898\n",
      "Mean of mu:  tensor(-2.0446, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 228 Average loss: 26915.2653\n",
      "Mean of mu:  tensor(-2.0699, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 229 [0/4 (0%)]\tLoss: 26710.074532\n",
      "Mean of mu:  tensor(-2.0375, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 229 Average loss: 25008.4545\n",
      "Mean of mu:  tensor(-2.0438, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 230 [0/4 (0%)]\tLoss: 24495.120218\n",
      "Mean of mu:  tensor(-2.0625, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 230 Average loss: 24509.7830\n",
      "Mean of mu:  tensor(-2.0622, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 231 [0/4 (0%)]\tLoss: 22676.029178\n",
      "Mean of mu:  tensor(-2.0430, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 231 Average loss: 22886.7770\n",
      "Mean of mu:  tensor(-2.0618, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 232 [0/4 (0%)]\tLoss: 24205.626652\n",
      "Mean of mu:  tensor(-2.0427, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 232 Average loss: 23514.3086\n",
      "Mean of mu:  tensor(-2.0680, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 233 [0/4 (0%)]\tLoss: 22813.238212\n",
      "Mean of mu:  tensor(-2.0358, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 233 Average loss: 21390.6083\n",
      "Mean of mu:  tensor(-2.0399, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 234 [0/4 (0%)]\tLoss: 21931.211256\n",
      "Mean of mu:  tensor(-2.0633, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 234 Average loss: 21360.3405\n",
      "Mean of mu:  tensor(-2.0674, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 235 [0/4 (0%)]\tLoss: 22227.769441\n",
      "Mean of mu:  tensor(-2.0353, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 235 Average loss: 21212.3901\n",
      "Mean of mu:  tensor(-2.0353, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 236 [0/4 (0%)]\tLoss: 19944.223880\n",
      "Mean of mu:  tensor(-2.0675, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 236 Average loss: 20542.4830\n",
      "Mean of mu:  tensor(-2.0396, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 237 [0/4 (0%)]\tLoss: 19737.515038\n",
      "Mean of mu:  tensor(-2.0632, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 237 Average loss: 20050.9345\n",
      "Mean of mu:  tensor(-2.0608, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 238 [0/4 (0%)]\tLoss: 18317.640022\n",
      "Mean of mu:  tensor(-2.0419, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 238 Average loss: 18721.4393\n",
      "Mean of mu:  tensor(-2.0631, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 239 [0/4 (0%)]\tLoss: 17812.303649\n",
      "Mean of mu:  tensor(-2.0395, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 239 Average loss: 18424.2482\n",
      "Mean of mu:  tensor(-2.0607, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 240 [0/4 (0%)]\tLoss: 18006.825886\n",
      "Mean of mu:  tensor(-2.0417, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 240 Average loss: 17923.7497\n",
      "Mean of mu:  tensor(-2.0671, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 241 [0/4 (0%)]\tLoss: 18225.581705\n",
      "Mean of mu:  tensor(-2.0349, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 241 Average loss: 17282.0048\n",
      "Mean of mu:  tensor(-2.0349, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 242 [0/4 (0%)]\tLoss: 18138.747461\n",
      "Mean of mu:  tensor(-2.0669, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 242 Average loss: 17803.5528\n",
      "Mean of mu:  tensor(-2.0391, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 243 [0/4 (0%)]\tLoss: 15622.963736\n",
      "Mean of mu:  tensor(-2.0626, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 243 Average loss: 15668.0399\n",
      "Mean of mu:  tensor(-2.0413, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 244 [0/4 (0%)]\tLoss: 15572.437766\n",
      "Mean of mu:  tensor(-2.0601, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 244 Average loss: 16682.8944\n",
      "Mean of mu:  tensor(-2.0413, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 245 [0/4 (0%)]\tLoss: 15024.876242\n",
      "Mean of mu:  tensor(-2.0601, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 245 Average loss: 15141.1369\n",
      "Mean of mu:  tensor(-2.0667, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 246 [0/4 (0%)]\tLoss: 15803.493872\n",
      "Mean of mu:  tensor(-2.0346, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 246 Average loss: 14822.9721\n",
      "Mean of mu:  tensor(-2.0389, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 247 [0/4 (0%)]\tLoss: 15665.553017\n",
      "Mean of mu:  tensor(-2.0626, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 247 Average loss: 15407.6789\n",
      "Mean of mu:  tensor(-2.0670, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 248 [0/4 (0%)]\tLoss: 14909.685448\n",
      "Mean of mu:  tensor(-2.0350, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 248 Average loss: 14511.1929\n",
      "Mean of mu:  tensor(-2.0605, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 249 [0/4 (0%)]\tLoss: 15191.501542\n",
      "Mean of mu:  tensor(-2.0417, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 249 Average loss: 14442.5287\n",
      "Mean of mu:  tensor(-2.0418, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 250 [0/4 (0%)]\tLoss: 14606.213092\n",
      "Mean of mu:  tensor(-2.0608, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 250 Average loss: 13953.5222\n",
      "Mean of mu:  tensor(-2.0355, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 251 [0/4 (0%)]\tLoss: 12029.437178\n",
      "Mean of mu:  tensor(-2.0678, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 251 Average loss: 13132.8143\n",
      "Mean of mu:  tensor(-2.0679, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 252 [0/4 (0%)]\tLoss: 13822.741256\n",
      "Mean of mu:  tensor(-2.0359, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 252 Average loss: 13446.7458\n",
      "Mean of mu:  tensor(-2.0639, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 253 [0/4 (0%)]\tLoss: 12997.769836\n",
      "Mean of mu:  tensor(-2.0404, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 253 Average loss: 12665.9352\n",
      "Mean of mu:  tensor(-2.0404, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 254 [0/4 (0%)]\tLoss: 12179.374678\n",
      "Mean of mu:  tensor(-2.0640, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 254 Average loss: 12412.3727\n",
      "Mean of mu:  tensor(-2.0617, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 255 [0/4 (0%)]\tLoss: 12585.423380\n",
      "Mean of mu:  tensor(-2.0429, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 255 Average loss: 12641.6966\n",
      "Mean of mu:  tensor(-2.0429, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 256 [0/4 (0%)]\tLoss: 11789.647845\n",
      "Mean of mu:  tensor(-2.0616, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 256 Average loss: 12397.6235\n",
      "Mean of mu:  tensor(-2.0361, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 257 [0/4 (0%)]\tLoss: 11527.329680\n",
      "Mean of mu:  tensor(-2.0681, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 257 Average loss: 11408.2075\n",
      "Mean of mu:  tensor(-2.0425, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 258 [0/4 (0%)]\tLoss: 10851.639127\n",
      "Mean of mu:  tensor(-2.0613, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 258 Average loss: 11060.8350\n",
      "Mean of mu:  tensor(-2.0679, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 259 [0/4 (0%)]\tLoss: 12164.368474\n",
      "Mean of mu:  tensor(-2.0356, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 259 Average loss: 11382.8456\n",
      "Mean of mu:  tensor(-2.0610, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 260 [0/4 (0%)]\tLoss: 11467.625469\n",
      "Mean of mu:  tensor(-2.0420, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 260 Average loss: 10541.8172\n",
      "Mean of mu:  tensor(-2.0417, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 261 [0/4 (0%)]\tLoss: 11052.623529\n",
      "Mean of mu:  tensor(-2.0605, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 261 Average loss: 11368.9824\n",
      "Mean of mu:  tensor(-2.0628, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 262 [0/4 (0%)]\tLoss: 11470.183129\n",
      "Mean of mu:  tensor(-2.0393, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 262 Average loss: 10470.2023\n",
      "Mean of mu:  tensor(-2.0351, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 263 [0/4 (0%)]\tLoss: 9559.632476\n",
      "Mean of mu:  tensor(-2.0672, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 263 Average loss: 10115.2235\n",
      "Mean of mu:  tensor(-2.0672, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 264 [0/4 (0%)]\tLoss: 10172.379256\n",
      "Mean of mu:  tensor(-2.0351, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 264 Average loss: 9779.6164\n",
      "Mean of mu:  tensor(-2.0605, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 265 [0/4 (0%)]\tLoss: 9418.020444\n",
      "Mean of mu:  tensor(-2.0416, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 265 Average loss: 9327.6833\n",
      "Mean of mu:  tensor(-2.0350, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 266 [0/4 (0%)]\tLoss: 9343.161137\n",
      "Mean of mu:  tensor(-2.0670, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 266 Average loss: 9689.2827\n",
      "Mean of mu:  tensor(-2.0414, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 267 [0/4 (0%)]\tLoss: 10177.789957\n",
      "Mean of mu:  tensor(-2.0604, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 267 Average loss: 9944.7581\n",
      "Mean of mu:  tensor(-2.0417, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 268 [0/4 (0%)]\tLoss: 8752.508669\n",
      "Mean of mu:  tensor(-2.0607, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 268 Average loss: 8828.4970\n",
      "Mean of mu:  tensor(-2.0396, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 269 [0/4 (0%)]\tLoss: 8943.113821\n",
      "Mean of mu:  tensor(-2.0633, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 269 Average loss: 9870.4372\n",
      "Mean of mu:  tensor(-2.0354, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 270 [0/4 (0%)]\tLoss: 8126.802258\n",
      "Mean of mu:  tensor(-2.0674, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 270 Average loss: 8690.7932\n",
      "Mean of mu:  tensor(-2.0673, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 271 [0/4 (0%)]\tLoss: 8757.342131\n",
      "Mean of mu:  tensor(-2.0351, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 271 Average loss: 8407.5190\n",
      "Mean of mu:  tensor(-2.0670, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 272 [0/4 (0%)]\tLoss: 9244.219381\n",
      "Mean of mu:  tensor(-2.0347, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 272 Average loss: 8299.5208\n",
      "Mean of mu:  tensor(-2.0411, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 273 [0/4 (0%)]\tLoss: 7608.600538\n",
      "Mean of mu:  tensor(-2.0596, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 273 Average loss: 8091.2248\n",
      "Mean of mu:  tensor(-2.0594, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 274 [0/4 (0%)]\tLoss: 8982.386779\n",
      "Mean of mu:  tensor(-2.0404, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 274 Average loss: 8431.1511\n",
      "Mean of mu:  tensor(-2.0337, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 275 [0/4 (0%)]\tLoss: 6932.288150\n",
      "Mean of mu:  tensor(-2.0658, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 275 Average loss: 7980.3383\n",
      "Mean of mu:  tensor(-2.0380, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 276 [0/4 (0%)]\tLoss: 7387.976291\n",
      "Mean of mu:  tensor(-2.0615, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 276 Average loss: 7784.2850\n",
      "Mean of mu:  tensor(-2.0403, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 277 [0/4 (0%)]\tLoss: 6998.777345\n",
      "Mean of mu:  tensor(-2.0592, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 277 Average loss: 7837.6788\n",
      "Mean of mu:  tensor(-2.0659, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 278 [0/4 (0%)]\tLoss: 10783.725438\n",
      "Mean of mu:  tensor(-2.0339, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 278 Average loss: 8872.2263\n",
      "Mean of mu:  tensor(-2.0381, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 279 [0/4 (0%)]\tLoss: 6953.274671\n",
      "Mean of mu:  tensor(-2.0617, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 279 Average loss: 7318.1236\n",
      "Mean of mu:  tensor(-2.0406, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 280 [0/4 (0%)]\tLoss: 6887.005779\n",
      "Mean of mu:  tensor(-2.0596, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 280 Average loss: 6803.0141\n",
      "Mean of mu:  tensor(-2.0409, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 281 [0/4 (0%)]\tLoss: 7420.476956\n",
      "Mean of mu:  tensor(-2.0598, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 281 Average loss: 7175.6188\n",
      "Mean of mu:  tensor(-2.0666, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 282 [0/4 (0%)]\tLoss: 8018.407747\n",
      "Mean of mu:  tensor(-2.0345, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 282 Average loss: 7289.6058\n",
      "Mean of mu:  tensor(-2.0412, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 283 [0/4 (0%)]\tLoss: 9114.446587\n",
      "Mean of mu:  tensor(-2.0602, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 283 Average loss: 7865.3796\n",
      "Mean of mu:  tensor(-2.0602, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 284 [0/4 (0%)]\tLoss: 7058.876925\n",
      "Mean of mu:  tensor(-2.0415, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 284 Average loss: 6550.9068\n",
      "Mean of mu:  tensor(-2.0629, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 285 [0/4 (0%)]\tLoss: 7519.539655\n",
      "Mean of mu:  tensor(-2.0394, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 285 Average loss: 7048.0750\n",
      "Mean of mu:  tensor(-2.0354, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 286 [0/4 (0%)]\tLoss: 5710.154779\n",
      "Mean of mu:  tensor(-2.0677, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 286 Average loss: 6673.0325\n",
      "Mean of mu:  tensor(-2.0679, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 287 [0/4 (0%)]\tLoss: 7790.639938\n",
      "Mean of mu:  tensor(-2.0359, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 287 Average loss: 6638.8124\n",
      "Mean of mu:  tensor(-2.0639, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 288 [0/4 (0%)]\tLoss: 7008.456802\n",
      "Mean of mu:  tensor(-2.0405, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 288 Average loss: 6352.2457\n",
      "Mean of mu:  tensor(-2.0642, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 289 [0/4 (0%)]\tLoss: 6582.509786\n",
      "Mean of mu:  tensor(-2.0407, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 289 Average loss: 5969.8122\n",
      "Mean of mu:  tensor(-2.0408, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 290 [0/4 (0%)]\tLoss: 6228.788768\n",
      "Mean of mu:  tensor(-2.0644, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 290 Average loss: 5999.9474\n",
      "Mean of mu:  tensor(-2.0686, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 291 [0/4 (0%)]\tLoss: 6987.678501\n",
      "Mean of mu:  tensor(-2.0364, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 291 Average loss: 6329.6657\n",
      "Mean of mu:  tensor(-2.0407, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 292 [0/4 (0%)]\tLoss: 6025.753670\n",
      "Mean of mu:  tensor(-2.0642, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 292 Average loss: 5940.3290\n",
      "Mean of mu:  tensor(-2.0363, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 293 [0/4 (0%)]\tLoss: 4950.994040\n",
      "Mean of mu:  tensor(-2.0684, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 293 Average loss: 6213.3859\n",
      "Mean of mu:  tensor(-2.0427, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 294 [0/4 (0%)]\tLoss: 5290.251657\n",
      "Mean of mu:  tensor(-2.0614, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 294 Average loss: 5574.1939\n",
      "Mean of mu:  tensor(-2.0680, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 295 [0/4 (0%)]\tLoss: 6193.080510\n",
      "Mean of mu:  tensor(-2.0358, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 295 Average loss: 6094.2703\n",
      "Mean of mu:  tensor(-2.0680, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 296 [0/4 (0%)]\tLoss: 6375.861985\n",
      "Mean of mu:  tensor(-2.0358, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 296 Average loss: 5767.3831\n",
      "Mean of mu:  tensor(-2.0401, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 297 [0/4 (0%)]\tLoss: 6002.428497\n",
      "Mean of mu:  tensor(-2.0637, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 297 Average loss: 6230.1834\n",
      "Mean of mu:  tensor(-2.0637, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 298 [0/4 (0%)]\tLoss: 6608.144050\n",
      "Mean of mu:  tensor(-2.0401, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 298 Average loss: 5822.5821\n",
      "Mean of mu:  tensor(-2.0679, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 299 [0/4 (0%)]\tLoss: 5794.868810\n",
      "Mean of mu:  tensor(-2.0358, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 299 Average loss: 5696.9050\n",
      "Mean of mu:  tensor(-2.0402, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 300 [0/4 (0%)]\tLoss: 5965.120587\n",
      "Mean of mu:  tensor(-2.0638, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "====> Epoch: 300 Average loss: 5812.8352\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(epoch)\n",
    "#         test(epoch)\n",
    "#         with torch.no_grad():\n",
    "#             sample = torch.randn(64, 20).to(device)\n",
    "#             sample = model.decode(sample).cpu()\n",
    "#             save_image(sample.view(64, 1, 28, 28),\n",
    "#                        filename_img_reconst(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, data in enumerate(train_loader):\n",
    "    data = data.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    recon_batch, mu, logvar = model(data)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[20.5665, 20.8438, 19.1352,  ..., 17.6067, 19.3815, 19.0561],\n",
      "        [20.4854, 20.8134, 19.1559,  ..., 18.4930, 20.4946, 19.4191]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward>)\n",
      "tensor([[20.7715, 21.8704, 18.3239,  ..., 17.4519, 19.8153, 18.1344],\n",
      "        [20.9229, 21.6162, 19.2637,  ..., 17.2543, 20.0773, 18.5757]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[-2.2690e+01, -2.6498e+01, -2.8529e+01,  2.0720e+01,  3.4609e+01,\n",
      "         -2.2813e+01,  2.3199e+01, -1.3214e+01, -2.9610e+01,  2.5413e+01,\n",
      "         -2.8270e+01,  1.6048e+01,  2.6857e+01,  3.8466e+01,  6.2390e+00,\n",
      "         -2.9774e+01,  1.5713e+01, -2.8309e+01,  3.0123e+01, -2.7311e+01,\n",
      "          2.8372e+01, -3.4503e+01,  9.9211e+00,  2.9404e+01, -2.2419e+01,\n",
      "         -1.4783e+01,  2.7599e+01,  3.4626e+01, -3.6364e+01, -2.6622e+01,\n",
      "          2.6771e+01,  2.6931e+01, -2.8425e+01,  3.8790e+00,  2.2791e+01,\n",
      "          3.5134e+01, -2.9265e+01,  5.6496e-01,  1.3300e+01, -3.1215e+01,\n",
      "          1.4574e+01, -2.3281e+01, -2.7162e+01, -2.8232e+01, -1.0265e+01,\n",
      "         -4.2446e+00, -3.9507e+01,  6.0393e-02,  1.5397e+01, -1.5757e+01],\n",
      "        [-2.3079e+01, -2.6742e+01, -2.8859e+01,  2.0865e+01,  3.5060e+01,\n",
      "         -2.2882e+01,  2.3569e+01, -1.3327e+01, -2.9934e+01,  2.5825e+01,\n",
      "         -2.8558e+01,  1.6206e+01,  2.7133e+01,  3.8806e+01,  6.4001e+00,\n",
      "         -3.0201e+01,  1.5795e+01, -2.8631e+01,  3.0495e+01, -2.7589e+01,\n",
      "          2.8574e+01, -3.4690e+01,  9.9340e+00,  2.9820e+01, -2.2476e+01,\n",
      "         -1.4996e+01,  2.8020e+01,  3.4957e+01, -3.6802e+01, -2.6790e+01,\n",
      "          2.6757e+01,  2.7170e+01, -2.8763e+01,  3.6396e+00,  2.3072e+01,\n",
      "          3.5391e+01, -2.9602e+01,  6.7536e-01,  1.3458e+01, -3.1740e+01,\n",
      "          1.4664e+01, -2.3678e+01, -2.7476e+01, -2.8501e+01, -1.0430e+01,\n",
      "         -4.4448e+00, -3.9974e+01,  3.0774e-03,  1.5542e+01, -1.6139e+01]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward>) tensor([[  0.3737,  -6.1756,  -9.3302,   3.3967, -10.5191, -34.0843,   0.3787,\n",
      "          -4.8276,  -9.1270,  -5.9895,  -1.7070,   3.4176,  -2.9121,  -3.2730,\n",
      "         -10.9755,  -4.4646,  -1.9421,   1.5979,  -5.4200,  -2.6245,   2.0253,\n",
      "          -4.2073,  -7.9187, -11.5012,   2.9178,   3.1058,  -5.9970,  -6.6173,\n",
      "           1.9459,   3.1008,  -8.0152, -10.0365,   0.9726,   0.4382,  -0.9070,\n",
      "           2.4205,  -9.0466,  -2.7697,  -2.9193,   0.8726, -32.6439, -18.0101,\n",
      "          -1.6414,  -4.7347,   0.6431,  -9.0485,  -2.5237,  -5.0406,  -9.4654,\n",
      "           2.2650],\n",
      "        [  0.5481,  -6.2451,  -9.3556,   3.5556, -10.7000, -34.6106,   0.2634,\n",
      "          -4.8013,  -9.2847,  -5.9452,  -1.6606,   3.6268,  -2.8715,  -3.2982,\n",
      "         -11.1679,  -4.4231,  -1.9258,   1.5954,  -5.4583,  -2.7749,   2.0166,\n",
      "          -4.1254,  -8.1271, -11.8088,   2.8603,   3.0982,  -5.7696,  -6.8375,\n",
      "           1.9850,   3.2766,  -8.0940, -10.0901,   1.0128,   0.3498,  -1.0150,\n",
      "           2.4659,  -9.1062,  -2.7713,  -2.9083,   1.0052, -32.9603, -18.2712,\n",
      "          -1.7322,  -4.7483,   0.8290,  -9.0910,  -2.5229,  -5.0348,  -9.6792,\n",
      "           2.1215]], dtype=torch.float64, grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(recon_batch)\n",
    "print(data)\n",
    "print(mu, logvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vaep",
   "language": "python",
   "name": "vaep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
