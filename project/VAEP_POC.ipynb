{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Proof of Concept - VAEP \n",
    "Variational Autoencoder of the Proteome (VAEP), reconstructiong samples on the peptide level using `log`-transformed on peptide intensities. This is the proof of concept (POC) for later use. \n",
    "\n",
    "- Fit VAE to Hela-Sample data (3 samples) and overfit. (Functional test of code)\n",
    "- Fit \n",
    "\n",
    "### Handling Missing values\n",
    "In this semi-supervised setting, where the samples are both input and target, missing values have to be imputed in the sample for the input space, but these values should not be considered for the loss function as their truth is unkown. \n",
    "\n",
    "### Alternatives\n",
    "\n",
    "- [`sklearn.imputer.IterativeImputer`](https://scikit-learn.org/stable/modules/impute.html#iterative-imputer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Setup\n",
    "> You won't have to re-run this (saves times for big in-memory datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import vaep\n",
    "from vaep.transform import log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data\\\\sample_peptides_n4.pkl'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import src.file_utils as io_\n",
    "FOLDER_DATA = 'data'\n",
    "files = io_.search_files(path=FOLDER_DATA, query='.pkl')\n",
    "file = io_.check_for_key(files, 'peptides_n4') # ToDo: check for more than one key behaviour\n",
    "file # sample_peptides.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "peptides_all = pd.read_pickle(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Experiment\n",
    "> Re-run everything below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "REMOVE_MISSING = False\n",
    "IMPUTE = False\n",
    "FILL_NA = False\n",
    "print_hyperparameter = lambda name, x: print(\"{} = {}\".format(name,x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Run all cells below from here for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "N_FEAT = 10\n",
    "peptides = peptides_all.sample(n=N_FEAT, axis=1)\n",
    "peptides = peptides.apply(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REMOVE_MISSING = False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Sequence</th>\n",
       "      <th>TYLVSGQPLEEIITYYPAMK</th>\n",
       "      <th>IMLPWDPTGK</th>\n",
       "      <th>ELLTLDEK</th>\n",
       "      <th>AGDAFGDTSFLNSK</th>\n",
       "      <th>RTAATLATHELR</th>\n",
       "      <th>DAAFEALGTALK</th>\n",
       "      <th>ILPEIIPILEEGLRSQK</th>\n",
       "      <th>EAMVMANNVYK</th>\n",
       "      <th>RHPYFYAPELLFFAK</th>\n",
       "      <th>FIPFIGVVK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MQ1.6.0.1_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_Hela_01_200327</th>\n",
       "      <td>NaN</td>\n",
       "      <td>20.580653</td>\n",
       "      <td>19.682602</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.476428</td>\n",
       "      <td>17.378798</td>\n",
       "      <td>17.395746</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.115976</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_Hela_01_200330</th>\n",
       "      <td>NaN</td>\n",
       "      <td>20.580653</td>\n",
       "      <td>19.682602</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.476428</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.395746</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.115976</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_Hela_01_20190104110509_200331</th>\n",
       "      <td>18.718189</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.73782</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.980376</td>\n",
       "      <td>18.190676</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_Hela_02_200331</th>\n",
       "      <td>NaN</td>\n",
       "      <td>20.693466</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.582508</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.331613</td>\n",
       "      <td>16.782329</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.760634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Sequence                                            TYLVSGQPLEEIITYYPAMK  \\\n",
       "MQ1.6.0.1_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_He...                   NaN   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...                   NaN   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...             18.718189   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...                   NaN   \n",
       "\n",
       "Sequence                                            IMLPWDPTGK   ELLTLDEK  \\\n",
       "MQ1.6.0.1_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_He...   20.580653  19.682602   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...   20.580653  19.682602   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...         NaN        NaN   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...   20.693466        NaN   \n",
       "\n",
       "Sequence                                            AGDAFGDTSFLNSK  \\\n",
       "MQ1.6.0.1_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_He...             NaN   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...             NaN   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...        17.73782   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...             NaN   \n",
       "\n",
       "Sequence                                            RTAATLATHELR  \\\n",
       "MQ1.6.0.1_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_He...     17.476428   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...     17.476428   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...           NaN   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...     17.582508   \n",
       "\n",
       "Sequence                                            DAAFEALGTALK  \\\n",
       "MQ1.6.0.1_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_He...     17.378798   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...           NaN   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...     17.980376   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...           NaN   \n",
       "\n",
       "Sequence                                            ILPEIIPILEEGLRSQK  \\\n",
       "MQ1.6.0.1_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_He...          17.395746   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...          17.395746   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...          18.190676   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...          17.331613   \n",
       "\n",
       "Sequence                                            EAMVMANNVYK  \\\n",
       "MQ1.6.0.1_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_He...          NaN   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...          NaN   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...          NaN   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...    16.782329   \n",
       "\n",
       "Sequence                                            RHPYFYAPELLFFAK  FIPFIGVVK  \n",
       "MQ1.6.0.1_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_He...        19.115976        NaN  \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...        19.115976        NaN  \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...              NaN        NaN  \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...              NaN  16.760634  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_hyperparameter(\"REMOVE_MISSING\", REMOVE_MISSING)\n",
    "if REMOVE_MISSING:\n",
    "    mask = peptides.isna().sum() == 0\n",
    "    peptides = peptides.loc[:,mask]\n",
    "peptides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPUTE = False\n"
     ]
    }
   ],
   "source": [
    "print_hyperparameter(\"IMPUTE\", IMPUTE)\n",
    "if IMPUTE:\n",
    "    from vaep.imputation import imputation_normal_distribution\n",
    "    imputed = peptides.iloc.apply(imputation_normal_distribution)\n",
    "    imputed    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "n_samples, n_features = peptides.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Impute missing values as 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detection_limit = float(int(peptides.min().min()))\n",
    "detection_limit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILL_NA = False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Sequence</th>\n",
       "      <th>TYLVSGQPLEEIITYYPAMK</th>\n",
       "      <th>IMLPWDPTGK</th>\n",
       "      <th>ELLTLDEK</th>\n",
       "      <th>AGDAFGDTSFLNSK</th>\n",
       "      <th>RTAATLATHELR</th>\n",
       "      <th>DAAFEALGTALK</th>\n",
       "      <th>ILPEIIPILEEGLRSQK</th>\n",
       "      <th>EAMVMANNVYK</th>\n",
       "      <th>RHPYFYAPELLFFAK</th>\n",
       "      <th>FIPFIGVVK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MQ1.6.0.1_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_Hela_01_200327</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_Hela_01_200330</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_Hela_01_20190104110509_200331</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_Hela_02_200331</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Sequence                                            TYLVSGQPLEEIITYYPAMK  \\\n",
       "MQ1.6.0.1_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_He...                 False   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...                 False   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...                  True   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...                 False   \n",
       "\n",
       "Sequence                                            IMLPWDPTGK  ELLTLDEK  \\\n",
       "MQ1.6.0.1_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_He...        True      True   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...        True      True   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...       False     False   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...        True     False   \n",
       "\n",
       "Sequence                                            AGDAFGDTSFLNSK  \\\n",
       "MQ1.6.0.1_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_He...           False   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...           False   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...            True   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...           False   \n",
       "\n",
       "Sequence                                            RTAATLATHELR  \\\n",
       "MQ1.6.0.1_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_He...          True   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...          True   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...         False   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...          True   \n",
       "\n",
       "Sequence                                            DAAFEALGTALK  \\\n",
       "MQ1.6.0.1_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_He...          True   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...         False   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...          True   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...         False   \n",
       "\n",
       "Sequence                                            ILPEIIPILEEGLRSQK  \\\n",
       "MQ1.6.0.1_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_He...               True   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...               True   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...               True   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...               True   \n",
       "\n",
       "Sequence                                            EAMVMANNVYK  \\\n",
       "MQ1.6.0.1_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_He...        False   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...        False   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...        False   \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...         True   \n",
       "\n",
       "Sequence                                            RHPYFYAPELLFFAK  FIPFIGVVK  \n",
       "MQ1.6.0.1_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_He...             True      False  \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...             True      False  \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...            False      False  \n",
       "MQ1.6.1.12_20190103_QE8_nLC0_LiNi_QC_MNT_15cm_H...            False       True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_hyperparameter('FILL_NA', FILL_NA)\n",
    "if FILL_NA:\n",
    "    peptides.fillna(detection_limit, inplace=True)\n",
    "else:\n",
    "    mask_observed = peptides.notna()\n",
    "    display(mask_observed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Data Loading\n",
    "Custom Dataset based on [PyTorch Data loading tutorial](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html). See also [`torch.utils.data`](https://pytorch.org/docs/master/data.html#module-torch.utils.data) documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class PeptideDatasetInMemory(Dataset):\n",
    "    \"\"\"Peptide Dataset fully in memory.\"\"\"\n",
    "    def __init__(self, data: pd.DataFrame, fill_na=0):\n",
    "        self.mask_obs = torch.from_numpy(data.notna().values)\n",
    "        data = data.fillna(fill_na)\n",
    "        self.peptides = torch.from_numpy(data.values)\n",
    "        self.length_ = len(data)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length_\n",
    "    \n",
    "    def __getitem__(self, idx):       \n",
    "        return self.peptides[idx], self.mask_obs[idx]\n",
    "\n",
    "dataset_in_memory = PeptideDatasetInMemory(peptides, detection_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_in_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peptide Intensities: \n",
      " tensor([[16.0000, 20.5807, 19.6826, 16.0000, 17.4764, 16.0000, 17.3957, 16.0000,\n",
      "         19.1160, 16.0000],\n",
      "        [18.7182, 16.0000, 16.0000, 17.7378, 16.0000, 17.9804, 18.1907, 16.0000,\n",
      "         16.0000, 16.0000]], dtype=torch.float64) \n",
      "######\n",
      "Masking non-observed: \n",
      " tensor([[ 0.0000, 20.5807, 19.6826,  0.0000, 17.4764,  0.0000, 17.3957,  0.0000,\n",
      "         19.1160,  0.0000],\n",
      "        [18.7182,  0.0000,  0.0000, 17.7378,  0.0000, 17.9804, 18.1907,  0.0000,\n",
      "          0.0000,  0.0000]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "peptide_intensities, masked_obs = dataset_in_memory[1:3]\n",
    "print(\"Peptide Intensities: \\n\",peptide_intensities,\"\\n######\")\n",
    "print(\"Masking non-observed: \\n\", peptide_intensities * masked_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "A Dataset needs a the methods `__len__` and `__getitem__, so it can be feed to a `DataLoader`, this mean the following has to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[16.0000, 20.5807, 19.6826, 16.0000, 17.4764, 17.3788, 17.3957, 16.0000,\n",
       "          19.1160, 16.0000],\n",
       "         [16.0000, 20.5807, 19.6826, 16.0000, 17.4764, 16.0000, 17.3957, 16.0000,\n",
       "          19.1160, 16.0000],\n",
       "         [18.7182, 16.0000, 16.0000, 17.7378, 16.0000, 17.9804, 18.1907, 16.0000,\n",
       "          16.0000, 16.0000],\n",
       "         [16.0000, 20.6935, 16.0000, 16.0000, 17.5825, 16.0000, 17.3316, 16.7823,\n",
       "          16.0000, 16.7606]], dtype=torch.float64),\n",
       " tensor([[False,  True,  True, False,  True,  True,  True, False,  True, False],\n",
       "         [False,  True,  True, False,  True, False,  True, False,  True, False],\n",
       "         [ True, False, False,  True, False,  True,  True, False, False, False],\n",
       "         [False,  True, False, False,  True, False,  True,  True, False,  True]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_in_memory[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## PyTorch Implementation of VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Default Command Line Arguments\n",
    "- later parameters will be passed a final program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(batch_size=2, cuda=False, epochs=600, log_interval=2, no_cuda=True, seed=43)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vaep.cmd import parse_args\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 600\n",
    "args = parse_args(['--batch-size', str(BATCH_SIZE), '--no-cuda', '--seed', '43', '--epochs', str(EPOCHS), '--log-interval', str(BATCH_SIZE)])\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "args.inital_lr = 1e-05\n",
    "args.layers    = 1e-05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Create a DataLoader instance\n",
    "Passing the DataSet instance in memory to the DataLoader creates a generator for training which shuffles the data on training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if args.cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msampler\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mbatch_sampler\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mnum_workers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcollate_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mpin_memory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdrop_last\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mworker_init_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmultiprocessing_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Data loader. Combines a dataset and a sampler, and provides an iterable over\n",
       "the given dataset.\n",
       "\n",
       "The :class:`~torch.utils.data.DataLoader` supports both map-style and\n",
       "iterable-style datasets with single- or multi-process loading, customizing\n",
       "loading order and optional automatic batching (collation) and memory pinning.\n",
       "\n",
       "See :py:mod:`torch.utils.data` documentation page for more details.\n",
       "\n",
       "Arguments:\n",
       "    dataset (Dataset): dataset from which to load the data.\n",
       "    batch_size (int, optional): how many samples per batch to load\n",
       "        (default: ``1``).\n",
       "    shuffle (bool, optional): set to ``True`` to have the data reshuffled\n",
       "        at every epoch (default: ``False``).\n",
       "    sampler (Sampler, optional): defines the strategy to draw samples from\n",
       "        the dataset. If specified, :attr:`shuffle` must be ``False``.\n",
       "    batch_sampler (Sampler, optional): like :attr:`sampler`, but returns a batch of\n",
       "        indices at a time. Mutually exclusive with :attr:`batch_size`,\n",
       "        :attr:`shuffle`, :attr:`sampler`, and :attr:`drop_last`.\n",
       "    num_workers (int, optional): how many subprocesses to use for data\n",
       "        loading. ``0`` means that the data will be loaded in the main process.\n",
       "        (default: ``0``)\n",
       "    collate_fn (callable, optional): merges a list of samples to form a\n",
       "        mini-batch of Tensor(s).  Used when using batched loading from a\n",
       "        map-style dataset.\n",
       "    pin_memory (bool, optional): If ``True``, the data loader will copy Tensors\n",
       "        into CUDA pinned memory before returning them.  If your data elements\n",
       "        are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type,\n",
       "        see the example below.\n",
       "    drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,\n",
       "        if the dataset size is not divisible by the batch size. If ``False`` and\n",
       "        the size of dataset is not divisible by the batch size, then the last batch\n",
       "        will be smaller. (default: ``False``)\n",
       "    timeout (numeric, optional): if positive, the timeout value for collecting a batch\n",
       "        from workers. Should always be non-negative. (default: ``0``)\n",
       "    worker_init_fn (callable, optional): If not ``None``, this will be called on each\n",
       "        worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as\n",
       "        input, after seeding and before data loading. (default: ``None``)\n",
       "\n",
       "\n",
       ".. warning:: If the ``spawn`` start method is used, :attr:`worker_init_fn`\n",
       "             cannot be an unpicklable object, e.g., a lambda function. See\n",
       "             :ref:`multiprocessing-best-practices` on more details related\n",
       "             to multiprocessing in PyTorch.\n",
       "\n",
       ".. note:: ``len(dataloader)`` heuristic is based on the length of the sampler used.\n",
       "          When :attr:`dataset` is an :class:`~torch.utils.data.IterableDataset`,\n",
       "          ``len(dataset)`` (if implemented) is returned instead, regardless\n",
       "          of multi-process loading configurations, because PyTorch trust\n",
       "          user :attr:`dataset` code in correctly handling multi-process\n",
       "          loading to avoid duplicate data. See `Dataset Types`_ for more\n",
       "          details on these two types of datasets and how\n",
       "          :class:`~torch.utils.data.IterableDataset` interacts with `Multi-process data loading`_.\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\kzl465\\anaconda3\\envs\\vaep\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\n",
       "\u001b[1;31mType:\u001b[0m           type\n",
       "\u001b[1;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.utils.data.DataLoader?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=dataset_in_memory,\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Iterate over the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nummber of samples in mini-batch: 2 \tObject-Type: <class 'list'>\n",
      "Nummber of samples in mini-batch: 2 \tObject-Type: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "for data in train_loader:\n",
    "    print(\"Nummber of samples in mini-batch: {}\".format(len(data)),\n",
    "          \"\\tObject-Type: {}\".format(type(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nummber of samples in mini-batch: 2 \tObject-Type: <class 'torch.Tensor'>\n",
      "tensor([[16.0000, 20.6935, 16.0000, 16.0000, 17.5825, 16.0000, 17.3316, 16.7823,\n",
      "         16.0000, 16.7606],\n",
      "        [18.7182, 16.0000, 16.0000, 17.7378, 16.0000, 17.9804, 18.1907, 16.0000,\n",
      "         16.0000, 16.0000]], dtype=torch.float64)\n",
      "tensor([[False,  True, False, False,  True, False,  True,  True, False,  True],\n",
      "        [ True, False, False,  True, False,  True,  True, False, False, False]])\n",
      "Nummber of samples in mini-batch: 2 \tObject-Type: <class 'torch.Tensor'>\n",
      "tensor([[16.0000, 20.5807, 19.6826, 16.0000, 17.4764, 17.3788, 17.3957, 16.0000,\n",
      "         19.1160, 16.0000],\n",
      "        [16.0000, 20.5807, 19.6826, 16.0000, 17.4764, 16.0000, 17.3957, 16.0000,\n",
      "         19.1160, 16.0000]], dtype=torch.float64)\n",
      "tensor([[False,  True,  True, False,  True,  True,  True, False,  True, False],\n",
      "        [False,  True,  True, False,  True, False,  True, False,  True, False]])\n"
     ]
    }
   ],
   "source": [
    "for i, (data, mask) in enumerate(train_loader):\n",
    "    print(\"Nummber of samples in mini-batch: {}\".format(len(data)),\n",
    "          \"\\tObject-Type: {}\".format(type(mask)))\n",
    "    print(data)\n",
    "    print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### VAE Model\n",
    "\n",
    "- adapted from basic [PyTorch VAE tutorial](https://github.com/pytorch/examples/tree/master/vae)\n",
    "- single hidden encoding and decoding layer, one middle hidden layer being the latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# from IPython.core.debugger import set_trace # invoke debugging \n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        n_neurons = 1000\n",
    "\n",
    "        self.fc1 = nn.Linear(n_features, n_neurons)\n",
    "        self.fc21 = nn.Linear(n_neurons, 50)\n",
    "        self.fc22 = nn.Linear(n_neurons, 50)\n",
    "        self.fc3 = nn.Linear(50, n_neurons)\n",
    "        self.fc4 = nn.Linear(n_neurons, n_features)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return self.fc4(h3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, n_features))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "\n",
    "model = VAE().to(device)\n",
    "model.double()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "# Reconstruction + KL divergence losses summed over all *non-masked* elements and batch\n",
    "def loss_function(recon_x, x, mask, mu, logvar, t=0.9):\n",
    "    \"\"\"Loss function only considering the observed values in the \n",
    "    reconstruction loss.\"\"\"\n",
    "    \n",
    "    # Default MSE loss would have a too big nominator (Would this matter?)\n",
    "    # MSE = F.mse_loss(input=recon_x, target=x, reduction='mean')\n",
    "    \n",
    "    # MSE of observed values\n",
    "    MSE = F.mse_loss(input=recon_x*mask,\n",
    "                     target=x*mask,\n",
    "                     reduction='sum')\n",
    "    MSE /= mask.sum() # only consider observed number of values\n",
    "    \n",
    "    # KL-divergence\n",
    "    ## see Appendix B from VAE paper:\n",
    "    ## Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    ## https://arxiv.org/abs/1312.6114\n",
    "    ## 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return t*MSE + (1-t)*KLD\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, mask) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mask, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "#         if batch_idx % args.log_interval == 0:\n",
    "#             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "#                 epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "#                 100. * batch_idx / len(train_loader),\n",
    "#                 loss.item() / len(data)))\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))\n",
    "\n",
    "# # No test set here\n",
    "# def test(epoch):\n",
    "#     model.eval()\n",
    "#     test_loss = 0\n",
    "#     with torch.no_grad():\n",
    "#         for i, (data, mask) in enumerate(test_loader):\n",
    "#             data = data.to(device)\n",
    "#             recon_batch, mu, logvar = model(data)\n",
    "#             test_loss += loss_function(recon_batch, data, mask, mu, logvar).item()\n",
    "# \n",
    "#     test_loss /= len(test_loader.dataset)\n",
    "#     print('====> Test set loss: {:.4f}'.format(test_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "Collapsed": "false",
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1 Average loss: 1433.9426\n",
      "====> Epoch: 2 Average loss: 1302.4768\n",
      "====> Epoch: 3 Average loss: 1209.5769\n",
      "====> Epoch: 4 Average loss: 1096.0888\n",
      "====> Epoch: 5 Average loss: 992.1412\n",
      "====> Epoch: 6 Average loss: 935.7090\n",
      "====> Epoch: 7 Average loss: 862.8980\n",
      "====> Epoch: 8 Average loss: 823.6378\n",
      "====> Epoch: 9 Average loss: 763.3002\n",
      "====> Epoch: 10 Average loss: 732.8845\n",
      "====> Epoch: 11 Average loss: 678.0465\n",
      "====> Epoch: 12 Average loss: 623.8913\n",
      "====> Epoch: 13 Average loss: 609.9456\n",
      "====> Epoch: 14 Average loss: 567.1102\n",
      "====> Epoch: 15 Average loss: 538.2294\n",
      "====> Epoch: 16 Average loss: 512.8249\n",
      "====> Epoch: 17 Average loss: 497.7225\n",
      "====> Epoch: 18 Average loss: 469.0381\n",
      "====> Epoch: 19 Average loss: 462.8169\n",
      "====> Epoch: 20 Average loss: 432.9566\n",
      "====> Epoch: 21 Average loss: 418.3955\n",
      "====> Epoch: 22 Average loss: 418.2901\n",
      "====> Epoch: 23 Average loss: 403.8908\n",
      "====> Epoch: 24 Average loss: 376.8999\n",
      "====> Epoch: 25 Average loss: 375.1311\n",
      "====> Epoch: 26 Average loss: 355.5880\n",
      "====> Epoch: 27 Average loss: 357.2372\n",
      "====> Epoch: 28 Average loss: 348.2304\n",
      "====> Epoch: 29 Average loss: 340.2125\n",
      "====> Epoch: 30 Average loss: 329.9350\n",
      "====> Epoch: 31 Average loss: 313.9468\n",
      "====> Epoch: 32 Average loss: 304.0739\n",
      "====> Epoch: 33 Average loss: 304.4652\n",
      "====> Epoch: 34 Average loss: 299.0699\n",
      "====> Epoch: 35 Average loss: 294.1603\n",
      "====> Epoch: 36 Average loss: 285.5629\n",
      "====> Epoch: 37 Average loss: 278.0739\n",
      "====> Epoch: 38 Average loss: 282.6430\n",
      "====> Epoch: 39 Average loss: 274.0480\n",
      "====> Epoch: 40 Average loss: 270.1890\n",
      "====> Epoch: 41 Average loss: 266.9091\n",
      "====> Epoch: 42 Average loss: 266.9530\n",
      "====> Epoch: 43 Average loss: 263.4289\n",
      "====> Epoch: 44 Average loss: 259.6644\n",
      "====> Epoch: 45 Average loss: 245.7237\n",
      "====> Epoch: 46 Average loss: 258.6910\n",
      "====> Epoch: 47 Average loss: 254.4950\n",
      "====> Epoch: 48 Average loss: 244.5372\n",
      "====> Epoch: 49 Average loss: 242.8070\n",
      "====> Epoch: 50 Average loss: 227.2896\n",
      "====> Epoch: 51 Average loss: 241.0543\n",
      "====> Epoch: 52 Average loss: 237.4746\n",
      "====> Epoch: 53 Average loss: 237.7252\n",
      "====> Epoch: 54 Average loss: 225.6280\n",
      "====> Epoch: 55 Average loss: 224.9598\n",
      "====> Epoch: 56 Average loss: 220.5464\n",
      "====> Epoch: 57 Average loss: 223.8943\n",
      "====> Epoch: 58 Average loss: 217.1046\n",
      "====> Epoch: 59 Average loss: 222.9630\n",
      "====> Epoch: 60 Average loss: 218.5928\n",
      "====> Epoch: 61 Average loss: 221.0653\n",
      "====> Epoch: 62 Average loss: 218.4106\n",
      "====> Epoch: 63 Average loss: 205.2719\n",
      "====> Epoch: 64 Average loss: 211.6689\n",
      "====> Epoch: 65 Average loss: 213.0704\n",
      "====> Epoch: 66 Average loss: 199.2954\n",
      "====> Epoch: 67 Average loss: 202.1842\n",
      "====> Epoch: 68 Average loss: 210.0178\n",
      "====> Epoch: 69 Average loss: 202.3296\n",
      "====> Epoch: 70 Average loss: 181.8422\n",
      "====> Epoch: 71 Average loss: 198.9097\n",
      "====> Epoch: 72 Average loss: 198.6090\n",
      "====> Epoch: 73 Average loss: 205.2715\n",
      "====> Epoch: 74 Average loss: 191.6391\n",
      "====> Epoch: 75 Average loss: 202.7276\n",
      "====> Epoch: 76 Average loss: 187.9993\n",
      "====> Epoch: 77 Average loss: 192.5073\n",
      "====> Epoch: 78 Average loss: 190.4116\n",
      "====> Epoch: 79 Average loss: 190.4147\n",
      "====> Epoch: 80 Average loss: 190.0440\n",
      "====> Epoch: 81 Average loss: 196.4614\n",
      "====> Epoch: 82 Average loss: 176.8985\n",
      "====> Epoch: 83 Average loss: 184.4163\n",
      "====> Epoch: 84 Average loss: 174.9263\n",
      "====> Epoch: 85 Average loss: 184.6804\n",
      "====> Epoch: 86 Average loss: 183.6441\n",
      "====> Epoch: 87 Average loss: 180.6938\n",
      "====> Epoch: 88 Average loss: 180.1366\n",
      "====> Epoch: 89 Average loss: 187.4847\n",
      "====> Epoch: 90 Average loss: 183.7450\n",
      "====> Epoch: 91 Average loss: 183.0497\n",
      "====> Epoch: 92 Average loss: 177.0282\n",
      "====> Epoch: 93 Average loss: 175.3239\n",
      "====> Epoch: 94 Average loss: 180.7387\n",
      "====> Epoch: 95 Average loss: 175.8071\n",
      "====> Epoch: 96 Average loss: 173.7732\n",
      "====> Epoch: 97 Average loss: 167.8551\n",
      "====> Epoch: 98 Average loss: 170.6929\n",
      "====> Epoch: 99 Average loss: 174.2904\n",
      "====> Epoch: 100 Average loss: 174.6184\n",
      "====> Epoch: 101 Average loss: 172.1579\n",
      "====> Epoch: 102 Average loss: 173.4930\n",
      "====> Epoch: 103 Average loss: 169.7388\n",
      "====> Epoch: 104 Average loss: 169.2239\n",
      "====> Epoch: 105 Average loss: 170.8525\n",
      "====> Epoch: 106 Average loss: 165.4441\n",
      "====> Epoch: 107 Average loss: 163.7194\n",
      "====> Epoch: 108 Average loss: 165.1587\n",
      "====> Epoch: 109 Average loss: 158.4671\n",
      "====> Epoch: 110 Average loss: 166.3015\n",
      "====> Epoch: 111 Average loss: 161.8963\n",
      "====> Epoch: 112 Average loss: 164.3585\n",
      "====> Epoch: 113 Average loss: 164.6579\n",
      "====> Epoch: 114 Average loss: 162.0345\n",
      "====> Epoch: 115 Average loss: 159.8519\n",
      "====> Epoch: 116 Average loss: 159.1868\n",
      "====> Epoch: 117 Average loss: 150.5681\n",
      "====> Epoch: 118 Average loss: 155.6425\n",
      "====> Epoch: 119 Average loss: 163.2857\n",
      "====> Epoch: 120 Average loss: 154.4515\n",
      "====> Epoch: 121 Average loss: 156.4140\n",
      "====> Epoch: 122 Average loss: 163.7511\n",
      "====> Epoch: 123 Average loss: 160.1270\n",
      "====> Epoch: 124 Average loss: 158.9329\n",
      "====> Epoch: 125 Average loss: 161.6912\n",
      "====> Epoch: 126 Average loss: 158.4197\n",
      "====> Epoch: 127 Average loss: 154.9582\n",
      "====> Epoch: 128 Average loss: 150.2585\n",
      "====> Epoch: 129 Average loss: 158.0896\n",
      "====> Epoch: 130 Average loss: 151.9744\n",
      "====> Epoch: 131 Average loss: 153.9486\n",
      "====> Epoch: 132 Average loss: 152.6262\n",
      "====> Epoch: 133 Average loss: 160.5138\n",
      "====> Epoch: 134 Average loss: 151.1706\n",
      "====> Epoch: 135 Average loss: 152.5093\n",
      "====> Epoch: 136 Average loss: 150.6426\n",
      "====> Epoch: 137 Average loss: 152.9676\n",
      "====> Epoch: 138 Average loss: 149.0250\n",
      "====> Epoch: 139 Average loss: 150.9885\n",
      "====> Epoch: 140 Average loss: 154.5235\n",
      "====> Epoch: 141 Average loss: 155.9278\n",
      "====> Epoch: 142 Average loss: 154.0293\n",
      "====> Epoch: 143 Average loss: 147.1995\n",
      "====> Epoch: 144 Average loss: 153.5837\n",
      "====> Epoch: 145 Average loss: 149.2659\n",
      "====> Epoch: 146 Average loss: 152.4925\n",
      "====> Epoch: 147 Average loss: 148.8647\n",
      "====> Epoch: 148 Average loss: 144.1315\n",
      "====> Epoch: 149 Average loss: 146.8605\n",
      "====> Epoch: 150 Average loss: 140.8550\n",
      "====> Epoch: 151 Average loss: 151.8068\n",
      "====> Epoch: 152 Average loss: 145.6526\n",
      "====> Epoch: 153 Average loss: 144.9204\n",
      "====> Epoch: 154 Average loss: 143.4461\n",
      "====> Epoch: 155 Average loss: 141.0008\n",
      "====> Epoch: 156 Average loss: 146.4971\n",
      "====> Epoch: 157 Average loss: 143.1099\n",
      "====> Epoch: 158 Average loss: 139.5487\n",
      "====> Epoch: 159 Average loss: 139.3175\n",
      "====> Epoch: 160 Average loss: 142.3574\n",
      "====> Epoch: 161 Average loss: 142.3446\n",
      "====> Epoch: 162 Average loss: 145.5713\n",
      "====> Epoch: 163 Average loss: 143.1940\n",
      "====> Epoch: 164 Average loss: 137.5883\n",
      "====> Epoch: 165 Average loss: 138.4110\n",
      "====> Epoch: 166 Average loss: 140.0781\n",
      "====> Epoch: 167 Average loss: 139.6528\n",
      "====> Epoch: 168 Average loss: 133.5836\n",
      "====> Epoch: 169 Average loss: 138.6415\n",
      "====> Epoch: 170 Average loss: 138.4514\n",
      "====> Epoch: 171 Average loss: 135.8811\n",
      "====> Epoch: 172 Average loss: 136.5376\n",
      "====> Epoch: 173 Average loss: 136.6928\n",
      "====> Epoch: 174 Average loss: 132.9674\n",
      "====> Epoch: 175 Average loss: 136.6034\n",
      "====> Epoch: 176 Average loss: 143.7271\n",
      "====> Epoch: 177 Average loss: 140.0489\n",
      "====> Epoch: 178 Average loss: 137.6359\n",
      "====> Epoch: 179 Average loss: 136.2388\n",
      "====> Epoch: 180 Average loss: 134.6166\n",
      "====> Epoch: 181 Average loss: 139.8487\n",
      "====> Epoch: 182 Average loss: 137.2015\n",
      "====> Epoch: 183 Average loss: 130.4865\n",
      "====> Epoch: 184 Average loss: 137.5928\n",
      "====> Epoch: 185 Average loss: 138.2958\n",
      "====> Epoch: 186 Average loss: 128.0696\n",
      "====> Epoch: 187 Average loss: 132.1197\n",
      "====> Epoch: 188 Average loss: 135.7060\n",
      "====> Epoch: 189 Average loss: 132.0935\n",
      "====> Epoch: 190 Average loss: 131.5090\n",
      "====> Epoch: 191 Average loss: 128.7026\n",
      "====> Epoch: 192 Average loss: 132.0691\n",
      "====> Epoch: 193 Average loss: 133.8605\n",
      "====> Epoch: 194 Average loss: 130.7716\n",
      "====> Epoch: 195 Average loss: 132.2383\n",
      "====> Epoch: 196 Average loss: 132.0548\n",
      "====> Epoch: 197 Average loss: 128.1103\n",
      "====> Epoch: 198 Average loss: 133.9781\n",
      "====> Epoch: 199 Average loss: 129.5793\n",
      "====> Epoch: 200 Average loss: 130.5152\n",
      "====> Epoch: 201 Average loss: 129.1774\n",
      "====> Epoch: 202 Average loss: 126.5802\n",
      "====> Epoch: 203 Average loss: 130.1206\n",
      "====> Epoch: 204 Average loss: 130.0317\n",
      "====> Epoch: 205 Average loss: 131.1922\n",
      "====> Epoch: 206 Average loss: 124.8885\n",
      "====> Epoch: 207 Average loss: 122.6685\n",
      "====> Epoch: 208 Average loss: 128.8452\n",
      "====> Epoch: 209 Average loss: 122.7033\n",
      "====> Epoch: 210 Average loss: 128.8748\n",
      "====> Epoch: 211 Average loss: 128.0303\n",
      "====> Epoch: 212 Average loss: 125.4472\n",
      "====> Epoch: 213 Average loss: 121.9347\n",
      "====> Epoch: 214 Average loss: 124.2669\n",
      "====> Epoch: 215 Average loss: 124.5357\n",
      "====> Epoch: 216 Average loss: 121.9668\n",
      "====> Epoch: 217 Average loss: 122.5898\n",
      "====> Epoch: 218 Average loss: 126.2780\n",
      "====> Epoch: 219 Average loss: 121.1635\n",
      "====> Epoch: 220 Average loss: 116.1110\n",
      "====> Epoch: 221 Average loss: 117.0300\n",
      "====> Epoch: 222 Average loss: 123.5697\n",
      "====> Epoch: 223 Average loss: 119.9046\n",
      "====> Epoch: 224 Average loss: 120.3025\n",
      "====> Epoch: 225 Average loss: 123.7132\n",
      "====> Epoch: 226 Average loss: 124.0041\n",
      "====> Epoch: 227 Average loss: 120.9631\n",
      "====> Epoch: 228 Average loss: 115.3810\n",
      "====> Epoch: 229 Average loss: 117.3265\n",
      "====> Epoch: 230 Average loss: 117.7995\n",
      "====> Epoch: 231 Average loss: 118.2001\n",
      "====> Epoch: 232 Average loss: 116.1296\n",
      "====> Epoch: 233 Average loss: 117.2161\n",
      "====> Epoch: 234 Average loss: 121.9360\n",
      "====> Epoch: 235 Average loss: 117.3947\n",
      "====> Epoch: 236 Average loss: 121.0462\n",
      "====> Epoch: 237 Average loss: 111.1811\n",
      "====> Epoch: 238 Average loss: 115.6321\n",
      "====> Epoch: 239 Average loss: 115.1903\n",
      "====> Epoch: 240 Average loss: 112.5222\n",
      "====> Epoch: 241 Average loss: 113.7434\n",
      "====> Epoch: 242 Average loss: 115.3146\n",
      "====> Epoch: 243 Average loss: 112.5620\n",
      "====> Epoch: 244 Average loss: 113.8462\n",
      "====> Epoch: 245 Average loss: 119.3141\n",
      "====> Epoch: 246 Average loss: 118.5147\n",
      "====> Epoch: 247 Average loss: 109.9957\n",
      "====> Epoch: 248 Average loss: 112.3471\n",
      "====> Epoch: 249 Average loss: 114.2251\n",
      "====> Epoch: 250 Average loss: 108.7324\n",
      "====> Epoch: 251 Average loss: 110.6983\n",
      "====> Epoch: 252 Average loss: 106.0395\n",
      "====> Epoch: 253 Average loss: 114.2731\n",
      "====> Epoch: 254 Average loss: 112.1784\n",
      "====> Epoch: 255 Average loss: 113.6862\n",
      "====> Epoch: 256 Average loss: 108.0080\n",
      "====> Epoch: 257 Average loss: 109.6342\n",
      "====> Epoch: 258 Average loss: 109.6098\n",
      "====> Epoch: 259 Average loss: 103.4900\n",
      "====> Epoch: 260 Average loss: 106.4912\n",
      "====> Epoch: 261 Average loss: 110.6511\n",
      "====> Epoch: 262 Average loss: 106.4972\n",
      "====> Epoch: 263 Average loss: 113.0349\n",
      "====> Epoch: 264 Average loss: 110.3592\n",
      "====> Epoch: 265 Average loss: 113.1024\n",
      "====> Epoch: 266 Average loss: 105.7121\n",
      "====> Epoch: 267 Average loss: 106.1757\n",
      "====> Epoch: 268 Average loss: 97.9429\n",
      "====> Epoch: 269 Average loss: 104.9055\n",
      "====> Epoch: 270 Average loss: 111.2932\n",
      "====> Epoch: 271 Average loss: 107.0214\n",
      "====> Epoch: 272 Average loss: 105.3669\n",
      "====> Epoch: 273 Average loss: 105.1249\n",
      "====> Epoch: 274 Average loss: 100.7988\n",
      "====> Epoch: 275 Average loss: 106.9125\n",
      "====> Epoch: 276 Average loss: 103.6049\n",
      "====> Epoch: 277 Average loss: 103.6326\n",
      "====> Epoch: 278 Average loss: 104.1888\n",
      "====> Epoch: 279 Average loss: 103.4467\n",
      "====> Epoch: 280 Average loss: 102.1040\n",
      "====> Epoch: 281 Average loss: 100.9833\n",
      "====> Epoch: 282 Average loss: 100.3769\n",
      "====> Epoch: 283 Average loss: 101.9507\n",
      "====> Epoch: 284 Average loss: 102.1522\n",
      "====> Epoch: 285 Average loss: 100.2702\n",
      "====> Epoch: 286 Average loss: 106.1137\n",
      "====> Epoch: 287 Average loss: 104.0684\n",
      "====> Epoch: 288 Average loss: 98.0976\n",
      "====> Epoch: 289 Average loss: 102.0218\n",
      "====> Epoch: 290 Average loss: 100.8178\n",
      "====> Epoch: 291 Average loss: 103.6874\n",
      "====> Epoch: 292 Average loss: 102.7851\n",
      "====> Epoch: 293 Average loss: 99.6986\n",
      "====> Epoch: 294 Average loss: 98.1411\n",
      "====> Epoch: 295 Average loss: 97.8980\n",
      "====> Epoch: 296 Average loss: 96.4769\n",
      "====> Epoch: 297 Average loss: 98.0321\n",
      "====> Epoch: 298 Average loss: 97.9207\n",
      "====> Epoch: 299 Average loss: 100.2432\n",
      "====> Epoch: 300 Average loss: 96.1285\n",
      "====> Epoch: 301 Average loss: 97.1772\n",
      "====> Epoch: 302 Average loss: 93.5061\n",
      "====> Epoch: 303 Average loss: 96.2084\n",
      "====> Epoch: 304 Average loss: 95.8954\n",
      "====> Epoch: 305 Average loss: 96.2080\n",
      "====> Epoch: 306 Average loss: 94.0839\n",
      "====> Epoch: 307 Average loss: 91.2185\n",
      "====> Epoch: 308 Average loss: 99.8745\n",
      "====> Epoch: 309 Average loss: 95.4736\n",
      "====> Epoch: 310 Average loss: 92.0301\n",
      "====> Epoch: 311 Average loss: 93.8259\n",
      "====> Epoch: 312 Average loss: 90.8107\n",
      "====> Epoch: 313 Average loss: 95.7343\n",
      "====> Epoch: 314 Average loss: 91.8616\n",
      "====> Epoch: 315 Average loss: 94.7628\n",
      "====> Epoch: 316 Average loss: 91.5672\n",
      "====> Epoch: 317 Average loss: 93.3050\n",
      "====> Epoch: 318 Average loss: 92.0133\n",
      "====> Epoch: 319 Average loss: 91.9670\n",
      "====> Epoch: 320 Average loss: 91.8811\n",
      "====> Epoch: 321 Average loss: 90.4550\n",
      "====> Epoch: 322 Average loss: 89.3130\n",
      "====> Epoch: 323 Average loss: 91.6051\n",
      "====> Epoch: 324 Average loss: 97.7021\n",
      "====> Epoch: 325 Average loss: 90.0281\n",
      "====> Epoch: 326 Average loss: 87.6879\n",
      "====> Epoch: 327 Average loss: 89.9690\n",
      "====> Epoch: 328 Average loss: 88.9134\n",
      "====> Epoch: 329 Average loss: 89.5758\n",
      "====> Epoch: 330 Average loss: 86.6033\n",
      "====> Epoch: 331 Average loss: 90.0803\n",
      "====> Epoch: 332 Average loss: 90.7942\n",
      "====> Epoch: 333 Average loss: 86.6597\n",
      "====> Epoch: 334 Average loss: 86.0610\n",
      "====> Epoch: 335 Average loss: 85.7975\n",
      "====> Epoch: 336 Average loss: 85.8004\n",
      "====> Epoch: 337 Average loss: 87.3629\n",
      "====> Epoch: 338 Average loss: 85.1878\n",
      "====> Epoch: 339 Average loss: 84.0484\n",
      "====> Epoch: 340 Average loss: 84.2165\n",
      "====> Epoch: 341 Average loss: 85.7689\n",
      "====> Epoch: 342 Average loss: 80.8059\n",
      "====> Epoch: 343 Average loss: 84.5297\n",
      "====> Epoch: 344 Average loss: 88.3807\n",
      "====> Epoch: 345 Average loss: 85.7615\n",
      "====> Epoch: 346 Average loss: 82.2535\n",
      "====> Epoch: 347 Average loss: 84.4406\n",
      "====> Epoch: 348 Average loss: 81.2640\n",
      "====> Epoch: 349 Average loss: 82.6101\n",
      "====> Epoch: 350 Average loss: 84.2910\n",
      "====> Epoch: 351 Average loss: 86.6917\n",
      "====> Epoch: 352 Average loss: 83.5718\n",
      "====> Epoch: 353 Average loss: 81.8610\n",
      "====> Epoch: 354 Average loss: 85.2744\n",
      "====> Epoch: 355 Average loss: 81.4900\n",
      "====> Epoch: 356 Average loss: 81.8672\n",
      "====> Epoch: 357 Average loss: 84.5372\n",
      "====> Epoch: 358 Average loss: 82.1843\n",
      "====> Epoch: 359 Average loss: 82.8723\n",
      "====> Epoch: 360 Average loss: 82.4172\n",
      "====> Epoch: 361 Average loss: 76.8892\n",
      "====> Epoch: 362 Average loss: 79.4149\n",
      "====> Epoch: 363 Average loss: 79.1833\n",
      "====> Epoch: 364 Average loss: 80.3930\n",
      "====> Epoch: 365 Average loss: 80.3438\n",
      "====> Epoch: 366 Average loss: 77.6945\n",
      "====> Epoch: 367 Average loss: 81.9625\n",
      "====> Epoch: 368 Average loss: 77.8368\n",
      "====> Epoch: 369 Average loss: 78.3751\n",
      "====> Epoch: 370 Average loss: 78.4418\n",
      "====> Epoch: 371 Average loss: 78.1041\n",
      "====> Epoch: 372 Average loss: 80.6246\n",
      "====> Epoch: 373 Average loss: 82.0747\n",
      "====> Epoch: 374 Average loss: 78.0849\n",
      "====> Epoch: 375 Average loss: 77.8784\n",
      "====> Epoch: 376 Average loss: 76.8340\n",
      "====> Epoch: 377 Average loss: 72.9695\n",
      "====> Epoch: 378 Average loss: 77.1622\n",
      "====> Epoch: 379 Average loss: 75.8882\n",
      "====> Epoch: 380 Average loss: 76.6180\n",
      "====> Epoch: 381 Average loss: 77.3060\n",
      "====> Epoch: 382 Average loss: 74.5202\n",
      "====> Epoch: 383 Average loss: 78.5680\n",
      "====> Epoch: 384 Average loss: 78.5585\n",
      "====> Epoch: 385 Average loss: 75.6731\n",
      "====> Epoch: 386 Average loss: 72.0199\n",
      "====> Epoch: 387 Average loss: 72.7255\n",
      "====> Epoch: 388 Average loss: 74.3941\n",
      "====> Epoch: 389 Average loss: 74.0258\n",
      "====> Epoch: 390 Average loss: 74.6819\n",
      "====> Epoch: 391 Average loss: 78.2467\n",
      "====> Epoch: 392 Average loss: 72.1967\n",
      "====> Epoch: 393 Average loss: 71.4158\n",
      "====> Epoch: 394 Average loss: 72.1473\n",
      "====> Epoch: 395 Average loss: 71.8232\n",
      "====> Epoch: 396 Average loss: 72.7904\n",
      "====> Epoch: 397 Average loss: 74.3056\n",
      "====> Epoch: 398 Average loss: 72.4721\n",
      "====> Epoch: 399 Average loss: 69.4225\n",
      "====> Epoch: 400 Average loss: 69.8240\n",
      "====> Epoch: 401 Average loss: 74.6592\n",
      "====> Epoch: 402 Average loss: 71.7575\n",
      "====> Epoch: 403 Average loss: 72.5650\n",
      "====> Epoch: 404 Average loss: 71.6684\n",
      "====> Epoch: 405 Average loss: 68.4086\n",
      "====> Epoch: 406 Average loss: 70.4048\n",
      "====> Epoch: 407 Average loss: 71.1594\n",
      "====> Epoch: 408 Average loss: 72.8134\n",
      "====> Epoch: 409 Average loss: 69.3355\n",
      "====> Epoch: 410 Average loss: 70.3814\n",
      "====> Epoch: 411 Average loss: 72.0938\n",
      "====> Epoch: 412 Average loss: 70.1198\n",
      "====> Epoch: 413 Average loss: 68.5030\n",
      "====> Epoch: 414 Average loss: 67.9970\n",
      "====> Epoch: 415 Average loss: 70.6857\n",
      "====> Epoch: 416 Average loss: 71.1342\n",
      "====> Epoch: 417 Average loss: 68.3584\n",
      "====> Epoch: 418 Average loss: 66.4229\n",
      "====> Epoch: 419 Average loss: 69.6000\n",
      "====> Epoch: 420 Average loss: 72.2878\n",
      "====> Epoch: 421 Average loss: 68.0667\n",
      "====> Epoch: 422 Average loss: 65.9437\n",
      "====> Epoch: 423 Average loss: 68.5861\n",
      "====> Epoch: 424 Average loss: 66.0950\n",
      "====> Epoch: 425 Average loss: 67.6941\n",
      "====> Epoch: 426 Average loss: 65.6209\n",
      "====> Epoch: 427 Average loss: 67.8643\n",
      "====> Epoch: 428 Average loss: 66.9143\n",
      "====> Epoch: 429 Average loss: 64.5881\n",
      "====> Epoch: 430 Average loss: 67.0841\n",
      "====> Epoch: 431 Average loss: 69.5846\n",
      "====> Epoch: 432 Average loss: 65.3080\n",
      "====> Epoch: 433 Average loss: 64.4299\n",
      "====> Epoch: 434 Average loss: 66.2246\n",
      "====> Epoch: 435 Average loss: 65.2441\n",
      "====> Epoch: 436 Average loss: 66.2476\n",
      "====> Epoch: 437 Average loss: 67.5560\n",
      "====> Epoch: 438 Average loss: 63.6077\n",
      "====> Epoch: 439 Average loss: 64.5237\n",
      "====> Epoch: 440 Average loss: 64.5413\n",
      "====> Epoch: 441 Average loss: 63.2509\n",
      "====> Epoch: 442 Average loss: 64.8038\n",
      "====> Epoch: 443 Average loss: 65.9196\n",
      "====> Epoch: 444 Average loss: 61.8427\n",
      "====> Epoch: 445 Average loss: 63.5720\n",
      "====> Epoch: 446 Average loss: 64.9562\n",
      "====> Epoch: 447 Average loss: 63.2376\n",
      "====> Epoch: 448 Average loss: 62.3002\n",
      "====> Epoch: 449 Average loss: 61.2994\n",
      "====> Epoch: 450 Average loss: 62.4511\n",
      "====> Epoch: 451 Average loss: 64.9848\n",
      "====> Epoch: 452 Average loss: 65.8674\n",
      "====> Epoch: 453 Average loss: 63.8209\n",
      "====> Epoch: 454 Average loss: 61.4177\n",
      "====> Epoch: 455 Average loss: 67.2113\n",
      "====> Epoch: 456 Average loss: 61.8489\n",
      "====> Epoch: 457 Average loss: 60.9532\n",
      "====> Epoch: 458 Average loss: 62.0793\n",
      "====> Epoch: 459 Average loss: 65.5563\n",
      "====> Epoch: 460 Average loss: 61.7089\n",
      "====> Epoch: 461 Average loss: 57.7629\n",
      "====> Epoch: 462 Average loss: 60.5902\n",
      "====> Epoch: 463 Average loss: 60.7657\n",
      "====> Epoch: 464 Average loss: 59.3470\n",
      "====> Epoch: 465 Average loss: 60.2787\n",
      "====> Epoch: 466 Average loss: 58.4522\n",
      "====> Epoch: 467 Average loss: 59.1642\n",
      "====> Epoch: 468 Average loss: 60.2672\n",
      "====> Epoch: 469 Average loss: 62.6802\n",
      "====> Epoch: 470 Average loss: 57.8101\n",
      "====> Epoch: 471 Average loss: 61.7279\n",
      "====> Epoch: 472 Average loss: 60.5827\n",
      "====> Epoch: 473 Average loss: 58.5061\n",
      "====> Epoch: 474 Average loss: 58.9532\n",
      "====> Epoch: 475 Average loss: 62.2396\n",
      "====> Epoch: 476 Average loss: 56.5305\n",
      "====> Epoch: 477 Average loss: 55.1989\n",
      "====> Epoch: 478 Average loss: 56.2567\n",
      "====> Epoch: 479 Average loss: 57.4332\n",
      "====> Epoch: 480 Average loss: 58.7362\n",
      "====> Epoch: 481 Average loss: 58.4215\n",
      "====> Epoch: 482 Average loss: 58.7476\n",
      "====> Epoch: 483 Average loss: 55.3917\n",
      "====> Epoch: 484 Average loss: 58.7388\n",
      "====> Epoch: 485 Average loss: 60.8296\n",
      "====> Epoch: 486 Average loss: 57.4099\n",
      "====> Epoch: 487 Average loss: 55.8760\n",
      "====> Epoch: 488 Average loss: 60.0722\n",
      "====> Epoch: 489 Average loss: 57.1177\n",
      "====> Epoch: 490 Average loss: 59.2332\n",
      "====> Epoch: 491 Average loss: 57.2132\n",
      "====> Epoch: 492 Average loss: 56.0671\n",
      "====> Epoch: 493 Average loss: 56.3103\n",
      "====> Epoch: 494 Average loss: 56.5347\n",
      "====> Epoch: 495 Average loss: 53.0809\n",
      "====> Epoch: 496 Average loss: 54.8472\n",
      "====> Epoch: 497 Average loss: 55.4856\n",
      "====> Epoch: 498 Average loss: 56.6073\n",
      "====> Epoch: 499 Average loss: 53.1146\n",
      "====> Epoch: 500 Average loss: 56.3220\n",
      "====> Epoch: 501 Average loss: 56.5192\n",
      "====> Epoch: 502 Average loss: 57.1194\n",
      "====> Epoch: 503 Average loss: 54.5330\n",
      "====> Epoch: 504 Average loss: 56.7932\n",
      "====> Epoch: 505 Average loss: 56.0711\n",
      "====> Epoch: 506 Average loss: 52.9181\n",
      "====> Epoch: 507 Average loss: 52.7294\n",
      "====> Epoch: 508 Average loss: 51.9035\n",
      "====> Epoch: 509 Average loss: 52.9440\n",
      "====> Epoch: 510 Average loss: 53.4638\n",
      "====> Epoch: 511 Average loss: 54.4068\n",
      "====> Epoch: 512 Average loss: 52.6778\n",
      "====> Epoch: 513 Average loss: 51.9846\n",
      "====> Epoch: 514 Average loss: 52.0407\n",
      "====> Epoch: 515 Average loss: 52.4472\n",
      "====> Epoch: 516 Average loss: 50.3665\n",
      "====> Epoch: 517 Average loss: 55.0935\n",
      "====> Epoch: 518 Average loss: 53.0930\n",
      "====> Epoch: 519 Average loss: 53.2064\n",
      "====> Epoch: 520 Average loss: 52.0439\n",
      "====> Epoch: 521 Average loss: 54.7608\n",
      "====> Epoch: 522 Average loss: 55.2256\n",
      "====> Epoch: 523 Average loss: 51.8022\n",
      "====> Epoch: 524 Average loss: 53.5615\n",
      "====> Epoch: 525 Average loss: 52.5179\n",
      "====> Epoch: 526 Average loss: 51.4060\n",
      "====> Epoch: 527 Average loss: 51.6506\n",
      "====> Epoch: 528 Average loss: 50.5720\n",
      "====> Epoch: 529 Average loss: 51.8287\n",
      "====> Epoch: 530 Average loss: 52.2188\n",
      "====> Epoch: 531 Average loss: 52.0590\n",
      "====> Epoch: 532 Average loss: 50.7007\n",
      "====> Epoch: 533 Average loss: 50.9225\n",
      "====> Epoch: 534 Average loss: 52.8655\n",
      "====> Epoch: 535 Average loss: 48.9947\n",
      "====> Epoch: 536 Average loss: 50.0803\n",
      "====> Epoch: 537 Average loss: 50.7523\n",
      "====> Epoch: 538 Average loss: 50.5138\n",
      "====> Epoch: 539 Average loss: 50.0166\n",
      "====> Epoch: 540 Average loss: 50.1847\n",
      "====> Epoch: 541 Average loss: 50.9513\n",
      "====> Epoch: 542 Average loss: 47.5962\n",
      "====> Epoch: 543 Average loss: 49.3016\n",
      "====> Epoch: 544 Average loss: 50.2357\n",
      "====> Epoch: 545 Average loss: 49.2592\n",
      "====> Epoch: 546 Average loss: 50.7756\n",
      "====> Epoch: 547 Average loss: 48.0898\n",
      "====> Epoch: 548 Average loss: 47.7731\n",
      "====> Epoch: 549 Average loss: 49.7220\n",
      "====> Epoch: 550 Average loss: 49.8740\n",
      "====> Epoch: 551 Average loss: 47.4588\n",
      "====> Epoch: 552 Average loss: 47.1520\n",
      "====> Epoch: 553 Average loss: 47.2123\n",
      "====> Epoch: 554 Average loss: 46.6456\n",
      "====> Epoch: 555 Average loss: 47.3803\n",
      "====> Epoch: 556 Average loss: 46.6188\n",
      "====> Epoch: 557 Average loss: 49.9579\n",
      "====> Epoch: 558 Average loss: 46.5137\n",
      "====> Epoch: 559 Average loss: 48.6127\n",
      "====> Epoch: 560 Average loss: 46.4835\n",
      "====> Epoch: 561 Average loss: 47.2074\n",
      "====> Epoch: 562 Average loss: 47.4725\n",
      "====> Epoch: 563 Average loss: 47.3538\n",
      "====> Epoch: 564 Average loss: 46.4436\n",
      "====> Epoch: 565 Average loss: 45.2828\n",
      "====> Epoch: 566 Average loss: 45.1474\n",
      "====> Epoch: 567 Average loss: 45.4216\n",
      "====> Epoch: 568 Average loss: 49.2288\n",
      "====> Epoch: 569 Average loss: 46.5769\n",
      "====> Epoch: 570 Average loss: 47.0631\n",
      "====> Epoch: 571 Average loss: 44.7741\n",
      "====> Epoch: 572 Average loss: 45.3909\n",
      "====> Epoch: 573 Average loss: 44.7475\n",
      "====> Epoch: 574 Average loss: 45.6033\n",
      "====> Epoch: 575 Average loss: 46.0075\n",
      "====> Epoch: 576 Average loss: 46.1840\n",
      "====> Epoch: 577 Average loss: 46.5735\n",
      "====> Epoch: 578 Average loss: 46.1170\n",
      "====> Epoch: 579 Average loss: 45.5709\n",
      "====> Epoch: 580 Average loss: 45.5181\n",
      "====> Epoch: 581 Average loss: 45.4302\n",
      "====> Epoch: 582 Average loss: 44.5182\n",
      "====> Epoch: 583 Average loss: 46.4096\n",
      "====> Epoch: 584 Average loss: 45.2386\n",
      "====> Epoch: 585 Average loss: 46.7452\n",
      "====> Epoch: 586 Average loss: 45.5119\n",
      "====> Epoch: 587 Average loss: 44.0686\n",
      "====> Epoch: 588 Average loss: 44.4643\n",
      "====> Epoch: 589 Average loss: 44.1570\n",
      "====> Epoch: 590 Average loss: 44.5923\n",
      "====> Epoch: 591 Average loss: 43.0375\n",
      "====> Epoch: 592 Average loss: 43.5964\n",
      "====> Epoch: 593 Average loss: 44.0564\n",
      "====> Epoch: 594 Average loss: 43.7008\n",
      "====> Epoch: 595 Average loss: 44.5191\n",
      "====> Epoch: 596 Average loss: 45.6343\n",
      "====> Epoch: 597 Average loss: 43.3436\n",
      "====> Epoch: 598 Average loss: 41.9378\n",
      "====> Epoch: 599 Average loss: 45.6696\n",
      "====> Epoch: 600 Average loss: 42.8445\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "for batch_idx, (data, mask) in enumerate(train_loader):\n",
    "    data = data.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    recon_batch, mu, logvar = model(data)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tensor([13.8214, 17.6445, 14.2478, 12.8008, 15.9272, 13.3975, 16.4665, 14.4127,\n",
      "        13.5715, 11.1374], dtype=torch.float64, grad_fn=<SelectBackward>),\n",
      "tensor([16.0000, 20.5807, 19.6826, 16.0000, 17.4764, 16.0000, 17.3957, 16.0000,\n",
      "        19.1160, 16.0000], dtype=torch.float64)\n",
      "\n",
      "tensor([12.6226, 17.4818, 12.9331, 11.9694, 15.1505, 13.5199, 15.8620, 14.1113,\n",
      "        12.7104, 10.6033], dtype=torch.float64, grad_fn=<SelectBackward>),\n",
      "tensor([16.0000, 20.5807, 19.6826, 16.0000, 17.4764, 17.3788, 17.3957, 16.0000,\n",
      "        19.1160, 16.0000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for x_recon, x in zip(recon_batch, data):\n",
    "    print(\"\\n{},\\n{}\".format(x_recon,x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Latent space for two samples (mean and logvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tensor([-2.3338,  4.4171,  2.4118,  2.3225,  0.8095,  1.9072, -2.7328,  3.0975,\n",
      "         2.5912,  3.0253,  4.9794,  7.5737, -0.7739,  1.9621, -3.9962,  6.3436,\n",
      "        -4.5875,  3.4362,  2.5427,  1.6525,  3.4686,  2.0423, -2.7293, -2.7275,\n",
      "         4.4718, -0.7327, -3.6151, -0.3192,  8.5042,  1.7399, -1.6256,  3.0905,\n",
      "         1.8523, -0.9228,  3.5593, -4.9261, -0.9060,  1.3435, -1.6048,  4.0263,\n",
      "         2.0511, -3.1987, -3.2239,  4.1090,  5.5218,  4.2370,  3.8534,  3.8592,\n",
      "         2.0801,  1.8580], dtype=torch.float64, grad_fn=<SelectBackward>),\n",
      "tensor([ 0.7091,  1.0253,  0.5991,  0.6709,  0.6955,  1.0126,  0.7310,  1.0793,\n",
      "         0.5643,  0.5852,  1.2417,  0.4722,  1.2415,  0.7318,  1.0387,  0.4421,\n",
      "         0.9005,  1.1317,  1.0081,  3.9041,  0.7244,  0.7216,  0.7797,  1.0136,\n",
      "         0.8133,  0.9258,  1.2682,  0.8514,  1.1720,  0.5595,  1.5556,  1.0062,\n",
      "         1.1772,  0.6372,  1.5532,  0.9005,  1.1960, 45.9151,  1.2396,  2.5072,\n",
      "         8.6153,  0.6591,  0.7486,  0.8445,  1.0943,  3.0504,  1.6373,  0.9797,\n",
      "         0.7935,  0.6015], dtype=torch.float64, grad_fn=<ExpBackward>)\n",
      "\n",
      "tensor([-2.3623,  4.4373,  2.4170,  2.1189,  0.6898,  1.8268, -2.6940,  2.9956,\n",
      "         2.6388,  2.9664,  5.0613,  7.5509, -0.6537,  1.9016, -4.0506,  6.4926,\n",
      "        -4.4585,  3.4701,  2.6499,  1.7964,  3.6511,  2.0525, -2.6687, -2.6799,\n",
      "         4.7108, -0.5743, -3.5913, -0.5557,  8.4487,  1.8038, -1.5693,  3.0736,\n",
      "         1.7822, -0.9367,  3.6636, -5.0219, -0.8843,  1.3306, -1.8620,  4.0724,\n",
      "         2.2188, -3.2980, -3.3275,  4.0329,  5.5105,  4.1575,  3.8295,  3.7420,\n",
      "         1.9899,  1.9525], dtype=torch.float64, grad_fn=<SelectBackward>),\n",
      "tensor([ 0.7139,  1.0685,  0.5611,  0.6370,  0.7001,  0.8706,  0.6758,  1.1168,\n",
      "         0.5463,  0.5744,  1.2024,  0.5231,  1.2258,  0.8679,  1.0286,  0.5022,\n",
      "         0.8840,  1.2053,  0.9907,  4.0169,  0.7700,  0.7358,  0.7659,  0.9477,\n",
      "         0.8711,  0.8958,  1.1974,  0.9155,  1.5043,  0.5762,  1.4054,  0.9160,\n",
      "         1.2755,  0.6913,  1.5191,  0.8772,  1.2074, 41.1945,  1.1538,  2.3900,\n",
      "         8.8507,  0.7689,  0.6599,  0.9072,  1.1077,  3.1936,  1.5585,  0.9769,\n",
      "         0.8867,  0.6254], dtype=torch.float64, grad_fn=<ExpBackward>)\n"
     ]
    }
   ],
   "source": [
    "for mu_, logvar_ in zip(mu, logvar):\n",
    "    print(\"\\n{},\\n{}\".format(mu_, torch.exp(logvar_)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vaep",
   "language": "python",
   "name": "vaep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
