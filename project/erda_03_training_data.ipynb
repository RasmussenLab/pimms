{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a set of training data\n",
    "\n",
    "Use a set of (most) common peptides to create inital data sets\n",
    "\n",
    "- based on `Counter` over all outputs from search (here: MaxQuant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import json\n",
    "import random  # shuffle, seed\n",
    "import functools\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import multiprocessing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "import vaep\n",
    "from vaep.io import data_objects\n",
    "\n",
    "import config as config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "def select_files_by_parent_folder(fpaths:List, years:List):\n",
    "    selected = []\n",
    "    for year_folder in years:\n",
    "        # several passes, but not a bottle neck\n",
    "        selected += [dump for dump in fpaths if year_folder in dump.parent.stem]\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED: int = 42  # Random seed for reproducibility\n",
    "FEAT_COMPLETNESS_CUTOFF = 0.25 # Minimal proportion of samples which have to share a feature\n",
    "YEARS = ['2017','2018', '2019', '2020']\n",
    "SAMPLE_COL = 'Sample ID'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a specific config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# options = ['peptides', 'evidence', 'proteinGroups']\n",
    "from config.training_data import peptides as cfg\n",
    "# from config.training_data import evidence as cfg\n",
    "# from config.training_data import proteinGroups as cfg\n",
    "\n",
    "{k: getattr(cfg, k) for k in dir(cfg) if not k.startswith('_')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_folder = 'data/selected/'\n",
    "out_folder = Path(out_folder) / cfg.NAME\n",
    "out_folder.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set defaults from file (allows to potentially overwrite parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal structure of config.py files\n",
    "NAME = cfg.NAME\n",
    "BASE_NAME = cfg.BASE_NAME\n",
    "\n",
    "TYPES_DUMP = cfg.TYPES_DUMP\n",
    "TYPES_COUNT = cfg.TYPES_COUNT\n",
    "\n",
    "IDX_COLS_LONG = cfg.IDX_COLS_LONG\n",
    "\n",
    "LOAD_DUMP = cfg.LOAD_DUMP\n",
    "\n",
    "CounterClass = cfg.CounterClass\n",
    "FNAME_COUNTER = cfg.FNAME_COUNTER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selected IDs\n",
    "\n",
    "- currently only `Sample ID` is used\n",
    "- path are to `.raw` raw files, not the output folder (could be changed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_id_old_new: str = 'data/rename/selected_old_new_id_mapping.csv' # selected samples with pride and original id\n",
    "df_ids = pd.read_csv(fn_id_old_new)\n",
    "df_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = CounterClass(FNAME_COUNTER)\n",
    "counts = counter.get_df_counts()\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TYPES_COUNT:\n",
    "    counts = counts.convert_dtypes().astype({'Charge': int}) #\n",
    "mask = counts['proportion'] >= FEAT_COMPLETNESS_CUTOFF\n",
    "counts.loc[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on selected samples, retain features that potentially could be in the subset\n",
    "\n",
    "- if 1000 samples are selected, and given at treshold of 25%, one would need at least 250 observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treshold_counts = int(len(df_ids) * FEAT_COMPLETNESS_CUTOFF)\n",
    "mask = counts['counts'] >= treshold_counts\n",
    "counts.loc[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX_selected = counts.loc[mask].set_index('Sequence').index\n",
    "IDX_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_dumps = df_ids[\"Sample ID\"]\n",
    "selected_dumps = {k: counter.dumps[k] for k in selected_dumps}\n",
    "selected_dumps = list(selected_dumps.items())\n",
    "selected_dumps[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_WORKERS = 8\n",
    "IDX = IDX_selected\n",
    "\n",
    "def load_fct(path):\n",
    "    s = (\n",
    "    pd.read_csv(path, index_col=\"Sequence\", usecols=[\"Sequence\", \"Intensity\"])\n",
    "    .notna()\n",
    "    .squeeze()\n",
    "    .astype(pd.Int8Dtype())\n",
    "    )\n",
    "    return s\n",
    "\n",
    "\n",
    "def collect(folders, index=IDX, load_fct=load_fct):\n",
    "    current = multiprocessing.current_process()\n",
    "    i = current._identity[0] % N_WORKERS + 1\n",
    "    print(\" \", end=\"\", flush=True)\n",
    "\n",
    "    failed = []\n",
    "    all = pd.DataFrame(index=index)\n",
    "\n",
    "    with tqdm_notebook(total=len(folders), position=i) as pbar:\n",
    "        for id, path in folders:\n",
    "            try:\n",
    "                s = load_fct(path)\n",
    "                s.name = id\n",
    "                all = all.join(s, how='left')\n",
    "            except FileNotFoundError:\n",
    "                logging.warning(f\"File not found: {path}\")\n",
    "                failed.append((id, path))\n",
    "            except pd.errors.EmptyDataError:\n",
    "                logging.warning(f\"Empty file: {path}\")\n",
    "                failed.append((id, path))\n",
    "            pbar.update(1)\n",
    "            \n",
    "    return all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with multiprocessing.Pool(N_WORKERS) as p:\n",
    "    all = list(\n",
    "        tqdm_notebook(\n",
    "            p.imap(collect, np.array_split(selected_dumps, N_WORKERS)),\n",
    "            total=N_WORKERS,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "all = pd.concat(all, axis=1)\n",
    "all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_samples = all.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = out_folder / 'count_samples.json'\n",
    "count_samples.to_json(fname)\n",
    "\n",
    "vaep.plotting.make_large_descriptors(size='medium')\n",
    "\n",
    "ax = count_samples.sort_values().plot(rot=90, ylabel='observations')\n",
    "vaep.savefig(ax.get_figure(), fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "all = all.T\n",
    "all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = out_folder / config.insert_shape(all,  'absent_present_pattern_selected{}.pkl')\n",
    "all.to_pickle(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_features = all.sum()\n",
    "fname = out_folder / 'count_feat.json'\n",
    "count_features.to_json(fname)\n",
    "\n",
    "ax = count_features.sort_values().plot(rot=90, ylabel='observations') \n",
    "vaep.savefig(ax.get_figure(), fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "all.to_csv(fname.with_suffix('.csv'), chunksize=1_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selected Features\n",
    "\n",
    "- index names should also match!\n",
    "- if not-> rather use a list?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fct(path):\n",
    "    s = (\n",
    "    pd.read_csv(path, index_col=\"Sequence\", usecols=[\"Sequence\", \"Intensity\"])\n",
    "    .squeeze()\n",
    "    .astype(pd.Int64Dtype())\n",
    "    )\n",
    "    return s\n",
    "\n",
    "all = None\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "collect_intensities = partial(collect, index=IDX, load_fct=load_fct)\n",
    "\n",
    "with multiprocessing.Pool(N_WORKERS) as p:\n",
    "    all = list(\n",
    "        tqdm_notebook(\n",
    "            p.imap(collect_intensities, np.array_split(selected_dumps, N_WORKERS)),\n",
    "            total=N_WORKERS,\n",
    "        )\n",
    "    )  \n",
    "    \n",
    "all = pd.concat(all, axis=1)\n",
    "all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all.memory_usage(deep=True).sum() / (2**20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = out_folder / config.insert_shape(all,  'intensities_wide_selected{}.pkl') \n",
    "all.to_pickle(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# all = all.T\n",
    "# all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all.to_pickle(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = counts.loc[mask].set_index(counter.idx_names).sort_index().index\n",
    "# selected_features.name = 'Gene names' # needs to be fixed\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Dumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_dumps = select_files_by_parent_folder(list(counter.dumps.values()), years=YEARS)\n",
    "print(\"Total number of files:\", len(selected_dumps))\n",
    "selected_dumps[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load one dump\n",
    "\n",
    "- check that this looks like you expect it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LOAD_DUMP(selected_dumps[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process folders\n",
    "\n",
    "- potentially in parallel, aggregating results\n",
    "- if needed: debug using two samples\n",
    "\n",
    "Design decisions\n",
    "- long format of data with categorical features (to save memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Callable\n",
    "from pandas.errors import EmptyDataError\n",
    "\n",
    "def process_folders(fpaths: List[Path],\n",
    "                    selected_features: pd.Index,\n",
    "                    load_folder: Callable,\n",
    "                    id_col='Sample ID',\n",
    "                    dtypes: dict = {\n",
    "                        'Sample ID': 'category',\n",
    "                        'Sequence': 'category'}) -> tuple:\n",
    "    print(f\"started new process with {len(fpaths)} files.\")\n",
    "    data_intensity = []\n",
    "    for i, fpath in enumerate(fpaths):\n",
    "        if not i % 10: print(f\"File ({i}): {fpath}\")\n",
    "        sample_name = fpath.stem\n",
    "        try:\n",
    "            # does some filtering\n",
    "            dump = load_folder(fpath)\n",
    "        except EmptyDataError:\n",
    "            logging.warning(f'Empty dump: {fpath}')\n",
    "            continue\n",
    "        except FileNotFoundError:\n",
    "            logging.warning(f'Missing dump: {fpath}')\n",
    "            continue\n",
    "        \n",
    "        # long data format\n",
    "        sequences_available = dump.index.intersection(selected_features)\n",
    "        dump = dump.loc[sequences_available, 'Intensity'].reset_index()\n",
    "        dump[id_col] = sample_name\n",
    "        dump = dump.astype(dtypes)\n",
    "        data_intensity.append(dump)\n",
    "    \n",
    "    data_intensity = pd.concat(data_intensity, copy=False, ignore_index=True)\n",
    "    data_intensity = data_intensity.astype(dtypes)\n",
    "    return data_intensity\n",
    "\n",
    "# # experiment\n",
    "# process_folders(selected_dumps[:2],\n",
    "#                 selected_features=selected_features,\n",
    "#                 load_folder=LOAD_DUMP,\n",
    "#                 dtypes=TYPES_DUMP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "process_folders_peptides = functools.partial(process_folders,\n",
    "                                             selected_features=selected_features,\n",
    "                                             load_folder=LOAD_DUMP,\n",
    "                                             dtypes=TYPES_DUMP)\n",
    "collected_dfs = data_objects.collect_in_chuncks(paths=selected_dumps,\n",
    "                                                process_chunk_fct=process_folders_peptides,\n",
    "                                                chunks=200,\n",
    "                                                n_workers=1 # to debug, don't multiprocess\n",
    "                                               )\n",
    "\n",
    "# one would need to aggregate categories first to keep them during aggregation?\n",
    "collected_dfs = pd.concat(collected_dfs, copy=False, ignore_index=True)\n",
    "collected_dfs = collected_dfs.astype(TYPES_DUMP)\n",
    "df_intensities = collected_dfs\n",
    "df_intensities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Except Intensities everything should be of data type category in order to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intensities.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intensities.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Check how many samples could be loaded, set total number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = df_intensities[SAMPLE_COL].nunique()\n",
    "M = len(selected_features)\n",
    "N,M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set index columns provided and squeeze to series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intensities = df_intensities.set_index(IDX_COLS_LONG).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_name = f'{BASE_NAME}_' + '_'.join(YEARS)\n",
    "fname = config.FOLDER_DATA / config.insert_shape(df_intensities,  base_name + '{}.pkl', shape=(N,M))\n",
    "print(f\"{fname = }\")\n",
    "df_intensities.to_pickle(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Only binary pickle format works for now\n",
    "- csv and reshaping the data needs to much memory for a single erda instance with many samples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
