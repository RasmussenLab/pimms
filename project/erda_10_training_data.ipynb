{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a set of training data\n",
    "\n",
    "Use a set of (most) common peptides to create inital data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import json\n",
    "import random  # shuffle, seed\n",
    "import functools\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import src.config as config\n",
    "from src.config import FOLDER_PROCESSED\n",
    "import vaep.io\n",
    "from vaep.io import data_objects\n",
    "\n",
    "from src.config import FNAME_C_PEPTIDES, FNAME_C_EVIDENCE, FNAME_C_PG, FNAME_C_GENES\n",
    "\n",
    "FNAME_C_PEPTIDES, FNAME_C_EVIDENCE, FNAME_C_PG, FNAME_C_GENES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N: int = 10_000  # Number of max samples\n",
    "RANDOM_SEED: int = 42  # Random seed for reproducibility\n",
    "\n",
    "FEAT_COMPLETNESS_CUTOFF = 0.25 # Minimal proportion of samples which have to share a feature\n",
    "\n",
    "YEARS = ['2018', '2019']\n",
    "\n",
    "\n",
    "TYPES_EVIDENCE = {'Sample ID': 'category',\n",
    "                  'Sequence': 'category',\n",
    "                  'Charge': 'category',}\n",
    "\n",
    "IDX_COLS_LONG = ['Sample ID', 'Sequence', 'Charge'] # in order "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Charged Peptides\n",
    "\n",
    "counter = data_objects.EvidenceCounter(FNAME_C_EVIDENCE)\n",
    "counts = counter.get_df_counts()\n",
    "counts = counts.convert_dtypes().astype({'Charge': int})\n",
    "mask = counts['proportion'] >= FEAT_COMPLETNESS_CUTOFF\n",
    "counts.loc[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_evidence = counts.loc[mask].set_index(counter.idx_names).sort_index().index\n",
    "selected_evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumps = list(counter.dumps.values())\n",
    "\n",
    "\n",
    "def load_evidence_dump(fpath, index_col=counter.idx_names):\n",
    "    df = pd.read_csv(fpath, index_col=index_col)\n",
    "    return df\n",
    "\n",
    "load_evidence_dump(dumps[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "def select_files_by_parent_folder(fpaths:List, years:List):\n",
    "    selected = []\n",
    "    for year_folder in years:\n",
    "        # several passes, but not a bottle neck\n",
    "        selected += [dump for dump in fpaths if year_folder in dump.parent.stem]\n",
    "    return selected\n",
    "\n",
    "selected_dumps = select_files_by_parent_folder(dumps, years=YEARS)\n",
    "print(\"Total number of files:\", len(selected_dumps))\n",
    "selected_dumps[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Callable\n",
    "\n",
    "def process_folders(fpaths: List[Path],\n",
    "                    selected_features: pd.Index,\n",
    "                    load_folder: Callable,\n",
    "                    id_col='Sample ID',\n",
    "                    dtypes: dict = {\n",
    "                        'Sample ID': 'category',\n",
    "                        'Sequence': 'category'}) -> tuple:\n",
    "    print(f\"started new process with {len(fpaths)} files.\")\n",
    "    data_intensity = []\n",
    "    for i, fpath in enumerate(fpaths):\n",
    "        if not i % 10: print(f\"File ({i}): {fpath}\")\n",
    "        sample_name = fpath.stem\n",
    "        dump = load_folder(fpath)\n",
    "        sequences_available = dump.index.intersection(selected_features)\n",
    "        dump = dump.loc[sequences_available, 'Intensity'].reset_index()\n",
    "        dump[id_col] = sample_name\n",
    "        dump = dump.astype(dtypes)\n",
    "        data_intensity.append(dump)\n",
    "    \n",
    "    data_intensity = pd.concat(data_intensity, copy=False, ignore_index=True)\n",
    "    data_intensity = data_intensity.astype(dtypes)\n",
    "    return data_intensity\n",
    "\n",
    "# experiment:\n",
    "# process_folders(dumps[:2],\n",
    "#                 selected_features=selected_evidence,\n",
    "#                 load_folder=load_evidence_dump,\n",
    "#                 dtypes=TYPES_EVIDENCE).dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# N = 100\n",
    "N = min(len(selected_dumps), N)\n",
    "\n",
    "LOAD_DUMP = load_evidence_dump\n",
    "SELECTED_FEATURES = selected_evidence\n",
    "\n",
    "\n",
    "process_folders_peptides = functools.partial(process_folders,\n",
    "                                             selected_features=SELECTED_FEATURES,\n",
    "                                             load_folder=LOAD_DUMP)\n",
    "collected_dfs = data_objects.collect_in_chuncks(paths=selected_dumps,\n",
    "                                                process_chunk_fct=process_folders_peptides,\n",
    "                                                chunks=200)\n",
    "\n",
    "# one would need to aggregate categories first to keep them during aggregation?\n",
    "collected_dfs = pd.concat(collected_dfs, copy=False, ignore_index=True)\n",
    "collected_dfs = collected_dfs.astype(TYPES_EVIDENCE)\n",
    "df_intensities = collected_dfs\n",
    "df_intensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intensities.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intensities = df_intensities.set_index(IDX_COLS_LONG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = len(selected_evidence)\n",
    "N = len(selected_dumps)\n",
    "N,M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intensities.sort_index(inplace=True)\n",
    "base_name = \"df_intensities_evidence_long\" + '_'.join(YEARS)\n",
    "fname = config.FOLDER_DATA / config.insert_shape(df_intensities, base_name + '{}.csv', shape=(N,M))\n",
    "print(f\"{fname = }\")\n",
    "df_intensities.to_csv(fname)\n",
    "df_intensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = config.FOLDER_DATA / config.insert_shape(df_intensities, base_name + '{}.pkl', shape=(N,M))\n",
    "print(f\"{fname = }\")\n",
    "df_intensities.to_pickle(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregated Peptides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peptide_counter = data_objects.PeptideCounter(FNAME_C_PEPTIDES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peptide_counts = peptide_counter.get_df_counts()\n",
    "mask = peptide_counts['proportion'] >= FEAT_COMPLETNESS_CUTOFF\n",
    "peptide_counts.loc[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_peptides = peptide_counts.loc[mask].set_index('Sequence').index\n",
    "selected_peptides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumps = list(peptide_counter.dumps.values())\n",
    "\n",
    "peptides = data_objects.load_agg_peptide_dump(dumps[0])\n",
    "peptides.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "N = min(len(dumps), N)\n",
    "\n",
    "data_intensity = {}\n",
    "support = {}\n",
    "# again with multiprocessing? await functions?\n",
    "for fp_training_sample in tqdm(dumps[:N]):\n",
    "    sample_name = fp_training_sample.stem\n",
    "    peptides = data_objects.load_agg_peptide_dump(fp_training_sample)\n",
    "    sequences_available = peptides.index.intersection(selected_peptides)\n",
    "    support[sample_name] = len(sequences_available)\n",
    "    data_intensity[sample_name] = peptides.loc[sequences_available,\n",
    "                                               'Intensity'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intensities = pd.DataFrame.from_dict(data_intensity).T\n",
    "df_intensities.index.name = 'Sample ID'\n",
    "df_intensities.sort_index(inplace=True)\n",
    "df_intensities.to_csv(config.FOLDER_DATA /\n",
    "                      config.insert_shape(df_intensities, 'df_intensities{}.csv'))\n",
    "df_intensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config.FOLDER_DATA /\n",
    "          config.insert_shape(df_intensities, 'support_agg_peptides{}.json'), 'w') as f:\n",
    "    json.dump(support, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
