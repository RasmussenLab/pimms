{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RawFiles Database\n",
    "\n",
    "- overview of raw files.\n",
    "\n",
    "Created data and figures\n",
    "\n",
    "```bash\n",
    "'data/all_raw_files_dump_duplicated.txt'\n",
    "'data/all_raw_files_dump_unique.csv' # csv file\n",
    "'Figures/raw_file_overview.pdf'\n",
    "```\n",
    "\n",
    "and uses \n",
    "\n",
    "```bash\n",
    "'data/all_raw_files_dump.txt'\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path, PurePosixPath\n",
    "from collections import namedtuple\n",
    "from functools import partial\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import logging\n",
    "from src.logging import setup_logger\n",
    "from src import config\n",
    "from vaep import utils\n",
    "\n",
    "cfg = config.Config()\n",
    "\n",
    "logger = logging.getLogger('vaep')\n",
    "logger = setup_logger(logger, fname_base='04_all_raw_files.ipynb')\n",
    "\n",
    "RawFile = namedtuple('RawFile', 'name path bytes')\n",
    "cfg.FN_ALL_RAW_FILES = config.FOLDER_DATA / config.FN_ALL_RAW_FILES\n",
    "data = []\n",
    "with open(cfg.FN_ALL_RAW_FILES) as f:\n",
    "    for line in f:\n",
    "        line = line.split()\n",
    "        path = Path(line[-1])\n",
    "        data.append(RawFile(path.stem, path, int(line[4])))\n",
    "\n",
    "data = pd.DataFrame.from_records(\n",
    "    data, columns=RawFile._fields, index=RawFile._fields[0])\n",
    "data.sort_values(by='path', inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['size_gb'] = data['bytes'] / 1024 ** 3\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding duplicates\n",
    "\n",
    "- add a numeric index column to identify samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['num_index'] = pd.RangeIndex(stop=len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data.index.is_unique:\n",
    "    print('Only unique files in index.')\n",
    "else:\n",
    "    non_unique = data.index.value_counts()\n",
    "    non_unique = non_unique[non_unique > 1]\n",
    "    # should this be browseable?\n",
    "    display('Non-unique files', non_unique)\n",
    "    print(f'Number of files with more than 2 duplicates: {(non_unique > 2).sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For same sized groups, remove first the onces in the `MNT` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_remove = None\n",
    "non_unique_remaining = None\n",
    "if not data.index.is_unique:\n",
    "    _data_to_remove = data.loc[non_unique.index]\n",
    "    data_to_remove = pd.DataFrame()\n",
    "    non_unique_remaining = pd.DataFrame()\n",
    "    for idx, g in _data_to_remove.groupby(level=0):\n",
    "        mask = ['\\\\MNT' in str(x) for x in g.path]\n",
    "        data_to_remove = data_to_remove.append(g[mask])\n",
    "        non_unique_remaining = non_unique_remaining.append(g[[x!=True for x in mask]])\n",
    "        \n",
    "assert len(data.loc[non_unique.index]) == len(non_unique_remaining) + len(data_to_remove)\n",
    "data_to_remove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main junk of duplicated files in in `MNT` subfolders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_unique_remaining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files with the same name and the same size are considered the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_to_remove = non_unique_remaining['bytes'].duplicated(keep='last')\n",
    "data_to_remove = data_to_remove.append(non_unique_remaining[mask_to_remove])\n",
    "assert len(data_to_remove) == 1037 , 'File appended twice?'\n",
    "data_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Save {data_to_remove['size_gb'].sum():1.0f} GB disk space by deleting {len(data_to_remove)} files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_unique = data.reset_index().set_index('num_index').drop(data_to_remove.set_index('num_index').index).set_index('name')\n",
    "data_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show files which are duplicated, but have different sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two files have the same name, but different sizes\n",
    "data_unique.loc[data_unique.index.duplicated(False)] if not data_unique.index.is_unique else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save unique files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.FN_ALL_RAW_FILES_UNIQUE = utils.append_to_filepath(cfg.FN_ALL_RAW_FILES, config.build_df_fname(data_unique, 'unique'), new_suffix='csv')\n",
    "data_unique.to_csv(cfg.FN_ALL_RAW_FILES_UNIQUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export file paths to file to remove them, e.g using `rm $(<filenames.txt))` following [this description](https://stackoverflow.com/a/18618543/9684872).\n",
    "\n",
    "```bash\n",
    "# remove empty lines\n",
    "cat all_raw_files_dump_duplicated.txt | grep .raw > all_raw_files_dump_duplicated_cleaned.txt\n",
    "ls `cat all_raw_files_dump_duplicated_cleaned`\n",
    "rm -i `cat all_raw_files_dump_duplicated_cleaned`\n",
    "rm -i $(<all_raw_files_dump_duplicated_cleaned.txt)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.FN_ALL_RAW_FILES_DUPLICATED = utils.append_to_filepath(cfg.FN_ALL_RAW_FILES, 'duplicated')\n",
    "\n",
    "with open(cfg.FN_ALL_RAW_FILES_DUPLICATED, 'w') as f:\n",
    "    for _path in data_to_remove['path']:\n",
    "        _path = PurePosixPath(_path)\n",
    "        f.write(f'{_path}\\r\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(ncols=2, gridspec_kw={\"width_ratios\": [\n",
    "                         5, 1], \"wspace\": 0.3}, figsize=(16, 8))\n",
    "data_unique['size_gb'].plot.hist(bins=30, ax=axes[0])\n",
    "data_unique['size_gb'].plot(kind='box', ax=axes[1])\n",
    "\n",
    "\n",
    "cfg.raw_file_overview = config.FIGUREFOLDER / 'raw_file_overview.pdf'\n",
    "\n",
    "fig.savefig(cfg.raw_file_overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find fractionated samples for raw files\n",
    "\n",
    "- franctionated samples need to be processed together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "queries = set()\n",
    "\n",
    "def find_indices_containing_query(query, X):\n",
    "    mask = X.index.str.contains(query)\n",
    "    X_query = X.loc[mask].sort_index()\n",
    "    queries.add(query)\n",
    "    return X_query\n",
    "\n",
    "def get_unique_stem(query, index:pd.Index):\n",
    "    \"\"\"Gets stem filename, by splitting filename left of query and remove last underscore _.\n",
    "    \n",
    "    Fractionated samples seem to be named by fraction type. Last field indicates fraction.\n",
    "    \"\"\"\n",
    "    ret = index.str.split(query).str[0].str.rsplit('_', n=1).str[0]\n",
    "#     ret = index.str.rsplit('_', n=1).str[0]\n",
    "    return sorted(list(set(ret)))\n",
    "\n",
    "def show_fractions(stub:str, df):\n",
    "    subset = df[df.index.str.contains(stub)]\n",
    "    print(repr(stub))\n",
    "    display(subset)\n",
    "    display(f'N: {len(subset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = data_unique.index\n",
    "\n",
    "find_indices_containing_query = partial(find_indices_containing_query, X=data_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = '[Ff]rac'\n",
    "df_selected = find_indices_containing_query(q)\n",
    "df_selected.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_unique = get_unique_stem(q, df_selected.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples where current approach of spliting based on frac does not work.\n",
    "# frac denotes here the total number of fractions (3, 6, 8, 12, 24, 46)\n",
    "\n",
    "frac_special_cases = [\n",
    "    # continue with samples below 2019 (select in DropDown below)\n",
    "    '20180508_QE3_nLC5_DBJ_DIAprot_HELA_500ng_GPF',\n",
    "    '20180528_QE5_Evo2_DBJ_DIAprot_HeLa_500ng',\n",
    "    '20190108_QE7_Evo1_DBJ_SA_LFQpho_HELA_PACs_200ug', # s mssing in LFQphos\n",
    "    '20190108_QE7_Evo1_DBJ_SA_LFQphos_HELA_PAC_200ug',\n",
    "    '20190108_QE7_Evo1_DBJ_SA_LFQphos_HELA_PAC_300ug',\n",
    "    '20190108_QE7_Evo1_DBJ_SA_LFQphos_HELA_PAC_400ug',\n",
    "    '20190212_QE5_Evo1_DBJ_LFQprot',\n",
    "    '20190314_QE3_DBJ_Evo2_LFQphos_Hela_200ug_StageTip',\n",
    "    '20190314_QE3_DBJ_Evo2_LFQphos_Hela_380ug_StageTip', # first t missing in StagetTip\n",
    "    '20190314_QE3_DBJ_Evo2_LFQphos_Hela_380ug_StagetTip',\n",
    "    '20190402_QE3_Evo1_DBJ_DIAprot_HELA',\n",
    "    '20190402_QE3_Evo1_DBJ_LFQprot_HELA',\n",
    "    '20190430_QE3_Evo2_DBJ_HELA_14cmCol_60degrees_5min',\n",
    "    '20190430_QE3_Evo2_DBJ_LFQprot_HELA-14cmCol_44min',\n",
    "    '20190507_QE5_Evo1_DBJ_LFQprot_Subcell_HeLa_Ctrl',\n",
    "    '20190507_QE5_Evo1_DBJ_LFQprot_Subcell_library_HeLa_Ctrl_Ani_Mix',\n",
    "    '20190622_EXP1_Evo1_AMV_SubCell-library-HeLa_21min-30000',\n",
    "    '20190628_EXP1_Evo1_AMV_SubCell-library-HeLa_21min-30000',   \n",
    "]\n",
    "\n",
    "# exclude keys and handle separately. Remaining keys can be used directly to create list of inputs.\n",
    "frac_unique = sorted(list(set(frac_unique) - set(frac_special_cases)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_data = widgets.Dropdown(options=frac_unique, index=0)\n",
    "show_fractions = partial(show_fractions, df=df_selected)\n",
    "out_sel = widgets.interactive_output(show_fractions, {'stub': w_data})\n",
    "widgets.VBox([w_data, out_sel])\n",
    "#stub, export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `frac12` indicates 12 splits. If there are more, some of them were re-measured, e.g. `0190920_QE3_nLC3_MJ_pSILAC_HeLa_48h_Frac01_Rep3_20190924081042`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For quantified samples\n",
    "- show scatter plot between sample size and number of quantified peptides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta data for all samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.analyzers; import importlib; importlib.reload(src.analyzers)\n",
    "from src.analyzers import AnalyzePeptides\n",
    "analysis = AnalyzePeptides(cfg.FN_ALL_RAW_FILES_UNIQUE) # ToDo: Add numbers to file names\n",
    "analysis.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.add_metadata(add_prop_not_na=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metadata has to casses less due to duplicates with differnt file sizes ( see above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.df.loc[analysis.df.index.duplicated(False)] # keep the larger one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profiling report\n",
    "using pandas-profiling library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_profiling import ProfileReport\n",
    "profile = ProfileReport(analysis.df_meta, title=\"Pandas Profiling Report\")\n",
    "profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(cfg) # return a dict which is rendered differently in ipython"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vaep",
   "language": "python",
   "name": "vaep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
