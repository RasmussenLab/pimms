{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RawFiles Database\n",
    "\n",
    "- overview of raw files.\n",
    "\n",
    "Created data and figures\n",
    "\n",
    "```bash\n",
    "'data/all_raw_files_dump_duplicated.txt'\n",
    "'data/all_raw_files_dump_unique.csv' # csv file\n",
    "'Figures/raw_file_overview.pdf'\n",
    "```\n",
    "\n",
    "and uses \n",
    "\n",
    "```bash\n",
    "'data/all_raw_files_dump.txt'\n",
    "```\n",
    "\n",
    "The ladder can be created using `find` on a server:\n",
    "\n",
    "```bash\n",
    "find . -name '*.raw' -exec ls -l {} \\; > all_raw_files_dump_2021_10_27.txt\n",
    "# alternative (changes the format)\n",
    "find . -name '*.raw' -ls > all_raw_files_dump_2021_10_27.txt\n",
    "```\n",
    "\n",
    "which was executed in the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path, PurePosixPath\n",
    "from collections import namedtuple\n",
    "from functools import partial\n",
    "import yaml\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "import logging\n",
    "from src.logging import setup_logger\n",
    "from src.analyzers import AnalyzePeptides\n",
    "from src.data_objects import MqAllSummaries\n",
    "from src.rawfiles import RawFileViewer, get_unique_stem, find_indices_containing_query, show_fractions\n",
    "from src import config\n",
    "from vaep import utils\n",
    "\n",
    "cfg = config.Config()\n",
    "\n",
    "logger = logging.getLogger('vaep')\n",
    "logger = setup_logger(logger, fname_base='04_all_raw_files_ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# FN_ALL_RAW_FILES = config.FOLDER_DATA / config.FN_ALL_RAW_FILES\n",
    "FN_ALL_RAW_FILES : Path = config.FOLDER_DATA / 'all_raw_files_dump_2021_10_29.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.FN_ALL_RAW_FILES = FN_ALL_RAW_FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RawFile = namedtuple('RawFile', 'name path bytes')\n",
    "\n",
    "data = []\n",
    "with open(cfg.FN_ALL_RAW_FILES) as f:\n",
    "    for line in f:\n",
    "        line = line.split(maxsplit=8) # ignore white spaces in file names, example:\n",
    "        #'-rw-r--r--. 1 501 501 282917566 Dec  3  2022 ./share_hela_raw/MNT_202220220921_EXLP1_Evo1_LiNi_ - Copy1.raw'\n",
    "        path = Path(line[-1])\n",
    "        data.append(RawFile(path.stem, path, int(line[4])))\n",
    "\n",
    "data = pd.DataFrame.from_records(\n",
    "    data, columns=RawFile._fields, index=RawFile._fields[0])\n",
    "\n",
    "data.sort_values(by='path', inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['size_gb'] = data['bytes'] / 1024 ** 3\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding duplicates\n",
    "\n",
    "- add a numeric index column to identify samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['num_index'] = pd.RangeIndex(stop=len(data))\n",
    "mask_non_unique = data.reset_index().duplicated(subset=['name', 'bytes'])\n",
    "mask_non_unique.index = data.index\n",
    "idx_non_unique = data.loc[mask_non_unique].index.unique()\n",
    "idx_non_unique # min number of files to remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_duplicates(df):\n",
    "    if df.index.is_unique:\n",
    "        print('Only unique files in index.')\n",
    "        return None\n",
    "    else:\n",
    "        non_unique = df.index.value_counts()\n",
    "        non_unique = non_unique[non_unique > 1]\n",
    "        # should this be browseable?\n",
    "        print(f'Number of files with more than 2 duplicates: {(non_unique > 2).sum()}')\n",
    "        return non_unique\n",
    "\n",
    "non_unique = check_for_duplicates(df=data)\n",
    "non_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there cases where only two files share the same name and have different file sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[\n",
    "    non_unique.index.difference(idx_non_unique) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For same sized groups, remove first the onces in the `MNT` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in_MNT_to_remove = None\n",
    "non_unique_remaining = None\n",
    "if not data.index.is_unique:\n",
    "    _data_to_remove = data.loc[idx_non_unique]\n",
    "    data_in_MNT_to_remove = pd.DataFrame()\n",
    "    non_unique_remaining = pd.DataFrame()\n",
    "    for idx, g in _data_to_remove.groupby(level=0):\n",
    "        mask = ['\\\\MNT' in str(x) for x in g.path]\n",
    "        assert len(mask) != sum(mask) , f'All files in MNT subfolders: {idx}'\n",
    "        data_in_MNT_to_remove = data_in_MNT_to_remove.append(g[mask])\n",
    "        non_unique_remaining = non_unique_remaining.append(g[[x!=True for x in mask]])\n",
    "\n",
    "    del _data_to_remove, mask, idx, g\n",
    "\n",
    "assert len(data.loc[idx_non_unique]) == len(non_unique_remaining) + len(data_in_MNT_to_remove)\n",
    "assert len(non_unique_remaining.loc[['\\\\MNT' in str(x) for x in non_unique_remaining.path]]) == 0, \"There are files in MNT folder left\"\n",
    "data_in_MNT_to_remove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main junk of duplicated files in in `MNT` subfolders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_unique_remaining_counts = check_for_duplicates(non_unique_remaining)\n",
    "non_unique_remaining.loc[non_unique_remaining_counts.index.unique()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files with the same name and the same size are considered the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_non_unique_remaining = non_unique_remaining.reset_index().duplicated(subset=['name', 'bytes'])\n",
    "mask_non_unique_remaining.index = non_unique_remaining.index\n",
    "data_to_remove = data_in_MNT_to_remove.append(\n",
    "                    non_unique_remaining.loc[mask_non_unique_remaining]\n",
    ")\n",
    "data_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Save {data_to_remove['size_gb'].sum():1.0f} GB disk space by deleting {len(data_to_remove)} files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_unique = data.reset_index().set_index('num_index').drop(data_to_remove.set_index('num_index').index).set_index('name')\n",
    "data_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that every index to remove is still present in `data_unique` which is data to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_unique.loc[data_to_remove.index.unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(data_unique) + len(data_to_remove)  == len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show files which are duplicated, but have different sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two files have the same name, but different sizes\n",
    "data_unique.loc[data_unique.index.duplicated(False)] if not data_unique.index.is_unique else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save unique files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.FN_ALL_RAW_FILES_UNIQUE = utils.append_to_filepath(cfg.FN_ALL_RAW_FILES, config.build_df_fname(data_unique, 'unique'), new_suffix='csv')\n",
    "data_unique.to_csv(cfg.FN_ALL_RAW_FILES_UNIQUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export file paths to file to remove them, e.g using `rm $(<filenames.txt))` following [this description](https://stackoverflow.com/a/18618543/9684872).\n",
    "\n",
    "```bash\n",
    "# remove empty lines\n",
    "cat all_raw_files_dump_duplicated.txt | grep .raw > all_raw_files_dump_duplicated_cleaned.txt\n",
    "ls `cat all_raw_files_dump_duplicated_cleaned`\n",
    "rm -i `cat all_raw_files_dump_duplicated_cleaned`\n",
    "rm -i $(<all_raw_files_dump_duplicated_cleaned.txt)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.FN_ALL_RAW_FILES_DUPLICATED = utils.append_to_filepath(cfg.FN_ALL_RAW_FILES, 'duplicated')\n",
    "\n",
    "with open(cfg.FN_ALL_RAW_FILES_DUPLICATED, 'w') as f:\n",
    "    for _path in data_to_remove['path']:\n",
    "        _path = PurePosixPath(_path)\n",
    "        f.write(f'{_path}\\r\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, gridspec_kw={\"width_ratios\": [\n",
    "                         5, 1], \"wspace\": 0.3}, figsize=(16, 8))\n",
    "data_unique['size_gb'].plot.hist(bins=30, ax=axes[0])\n",
    "data_unique['size_gb'].plot(kind='box', ax=axes[1])\n",
    "\n",
    "\n",
    "cfg.raw_file_overview = config.FIGUREFOLDER / 'raw_file_overview.pdf'\n",
    "\n",
    "fig.savefig(cfg.raw_file_overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_unique.describe(np.linspace(0.1, 0.9, 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find fractionated samples for raw files\n",
    "\n",
    "- franctionated samples need to be processed together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer = RawFileViewer(data_unique, outputfolder=config.FOLDER_DATA)\n",
    "_ = viewer.view()\n",
    "display(_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query: fractionated samples\n",
    "\n",
    "hard coded query to output fractionated samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = data_unique.index\n",
    "\n",
    "find_indices_containing_query = partial(find_indices_containing_query, X=data_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = '[Ff]rac'  # query field\n",
    "df_selected = find_indices_containing_query(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_unique = get_unique_stem(q, df_selected.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples where current approach of spliting based on frac does not work.\n",
    "# frac denotes here the total number of fractions (3, 6, 8, 12, 24, 46)\n",
    "\n",
    "frac_special_cases = [\n",
    "    # continue with samples below 2019 (select in DropDown below)\n",
    "    '20180508_QE3_nLC5_DBJ_DIAprot_HELA_500ng_GPF',\n",
    "    '20180528_QE5_Evo2_DBJ_DIAprot_HeLa_500ng',\n",
    "    '20190108_QE7_Evo1_DBJ_SA_LFQpho_HELA_PACs_200ug', # s mssing in LFQphos\n",
    "    '20190108_QE7_Evo1_DBJ_SA_LFQphos_HELA_PAC_200ug',\n",
    "    '20190108_QE7_Evo1_DBJ_SA_LFQphos_HELA_PAC_300ug',\n",
    "    '20190108_QE7_Evo1_DBJ_SA_LFQphos_HELA_PAC_400ug',\n",
    "    '20190212_QE5_Evo1_DBJ_LFQprot',\n",
    "    '20190314_QE3_DBJ_Evo2_LFQphos_Hela_200ug_StageTip',\n",
    "    '20190314_QE3_DBJ_Evo2_LFQphos_Hela_380ug_StageTip', # first t missing in StagetTip\n",
    "    '20190314_QE3_DBJ_Evo2_LFQphos_Hela_380ug_StagetTip',\n",
    "    '20190402_QE3_Evo1_DBJ_DIAprot_HELA',\n",
    "    '20190402_QE3_Evo1_DBJ_LFQprot_HELA',\n",
    "    '20190430_QE3_Evo2_DBJ_HELA_14cmCol_60degrees_5min',\n",
    "    '20190430_QE3_Evo2_DBJ_LFQprot_HELA-14cmCol_44min',\n",
    "    '20190507_QE5_Evo1_DBJ_LFQprot_Subcell_HeLa_Ctrl',\n",
    "    '20190507_QE5_Evo1_DBJ_LFQprot_Subcell_library_HeLa_Ctrl_Ani_Mix',\n",
    "    '20190622_EXP1_Evo1_AMV_SubCell-library-HeLa_21min-30000',\n",
    "    '20190628_EXP1_Evo1_AMV_SubCell-library-HeLa_21min-30000',   \n",
    "]\n",
    "\n",
    "# exclude keys and handle separately. Remaining keys can be used directly to create list of inputs.\n",
    "frac_unique = sorted(list(set(frac_unique) - set(frac_special_cases)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_data = widgets.Dropdown(options=frac_unique, index=0)\n",
    "show_fractions_frac = partial(show_fractions, df=df_selected)\n",
    "out_sel = widgets.interactive_output(show_fractions_frac, {'stub': w_data})\n",
    "widgets.VBox([w_data, out_sel]) # repr of class\n",
    "#stub, export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `frac12` indicates 12 splits. If there are more, some of them were re-measured, e.g. `0190920_QE3_nLC3_MJ_pSILAC_HeLa_48h_Frac01_Rep3_20190924081042`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For quantified samples\n",
    "- show scatter plot between sample size and number of quantified peptides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.fname_1000_most_common_peptides = FN_PEPTIDE_INTENSITIES = config.FOLDER_DATA / 'df_intensities_N07285_M01000' \n",
    "common_peptides = AnalyzePeptides.from_file(cfg.fname_1000_most_common_peptides)\n",
    "common_peptides = common_peptides.df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.FN_ALL_SUMMARIES = config.FN_ALL_SUMMARIES\n",
    "mq_summaries = MqAllSummaries(cfg.FN_ALL_SUMMARIES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only keep one copy of files with the same name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_unique.loc[data_unique.index.duplicated(False)] if not data_unique.index.is_unique else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_unique_index = data_unique.index.duplicated()\n",
    "data_unique_index = data_unique.loc[~data_unique_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_idx_missing = mq_summaries.df.index.difference(data_unique_index.index)\n",
    "assert not len(_idx_missing), f\"There are missing files processed in the list of raw files: {_idx_missing}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> They can be duplicated files with the same file size. Not the case for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_shared = mq_summaries.df.index.intersection(data_unique.index)\n",
    "\n",
    "_file_sizes = data_unique.loc[idx_shared, 'size_gb']\n",
    "_file_sizes.loc[_file_sizes.index.duplicated(False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_file_sizes = _file_sizes.loc[~_file_sizes.index.duplicated(keep='last')]\n",
    "mq_summaries.df.loc[idx_shared, 'file size in GB'] = _file_sizes\n",
    "cols = ['Peptide Sequences Identified', 'file size in GB']\n",
    "mq_summaries.df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mq_summaries.df[cols].describe(np.linspace(0.05, 0.95, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=3, gridspec_kw={\"width_ratios\": [\n",
    "                         5, 1, 1], \"wspace\": 0.3}, figsize=(20, 8))\n",
    "\n",
    "ax = axes[0]\n",
    "ax = mq_summaries.df.plot.scatter(x=cols[0], y=cols[1], ax=ax)\n",
    "ax.axvline(x=15000)\n",
    "\n",
    "ax = axes[1]\n",
    "ax = mq_summaries.df[cols[0]].plot(kind='box', ax=ax)\n",
    "\n",
    "\n",
    "ax = axes[2]\n",
    "ax = mq_summaries.df[cols[1]].plot(kind='box', ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some files with a large number of identified peptides, the file size information seems to be missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.figure_1 = config.FIGUREFOLDER / 'figure_1.pdf'\n",
    "\n",
    "fig.savefig(cfg.figure_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 15_000\n",
    "mask = mq_summaries.df[cols[0]] > threshold\n",
    "print(\n",
    "    f\"for threshold of {threshold:,d} quantified peptides:\\n\"\n",
    "    f\"Total number of files is {mask.sum()}\\n\"\n",
    "    \"Minimum file-size is {:.3f} GB.\\n\".format(\n",
    "        mq_summaries.df.loc[mask, cols[1]].min())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta data for all samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From raw file reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_parse = data_unique.loc[idx_shared, 'path'].apply(lambda path: str(PurePosixPath(path)).strip())\n",
    "files_to_parse = files_to_parse\n",
    "files_to_parse = dict(files=files_to_parse.to_list())\n",
    "cfg.remote_files = config.FOLDER_DATA / 'remote_files.yaml'\n",
    "with open(cfg.remote_files, 'w') as f:\n",
    "    yaml.dump(files_to_parse, f)\n",
    "print(f\"Saved list of files to: {cfg.remote_files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = AnalyzePeptides.from_file(cfg.FN_ALL_RAW_FILES_UNIQUE,index_col='name') # ToDo: Add numbers to file names\n",
    "analysis.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "analysis.add_metadata(add_prop_not_na=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Metadata has fewer cases due to duplicates with differnt file sizes ( see above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.df.loc[analysis.df.index.duplicated(False)] # keep the larger one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profiling report\n",
    "using pandas-profiling library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "profile = ProfileReport(analysis.df_meta, title=\"Pandas Profiling Report\")\n",
    "profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(cfg) # return a dict which is rendered differently in ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
