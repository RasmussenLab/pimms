{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "149ddbfc-c339-4843-8801-9eca57b0ff5e",
   "metadata": {},
   "source": [
    "# Analyis of grid hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00a7b57-371d-4463-8da4-201e793f4401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import vaep.nb\n",
    "matplotlib.rcParams['figure.figsize'] = [12.0, 6.0]\n",
    "\n",
    "import vaep.io\n",
    "import vaep.pandas\n",
    "import vaep.utils\n",
    "from vaep.io import datasplits\n",
    "from vaep import sampling\n",
    "from vaep.analyzers import compare_predictions\n",
    "import vaep.plotting.plotly as px_vaep\n",
    "\n",
    "pd.options.display.max_columns = 45\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.multi_sparse = False\n",
    "\n",
    "logger = vaep.logging.setup_nb_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5b7560-f825-4461-95ad-bf90accd2e87",
   "metadata": {},
   "source": [
    "## Papermill parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfd7732-e5c1-45fb-aff3-4a5dc2446078",
   "metadata": {},
   "source": [
    "papermill parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6189a40-cfe2-4a53-9ce9-d7f86080bf79",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "metrics_json:str = \"path/to/all_metrics.json\" # file path to metrics json\n",
    "configs_json:str = \"path/to/all_configs.json\" # file path to configs json (\"meta data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333b0bba-e6ed-44bf-a7cc-f8f378c42d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    assert pathlib.Path(metrics_json).exists()\n",
    "    assert pathlib.Path(configs_json).exists()\n",
    "except AssertionError:\n",
    "    metrics_json = snakemake.input.metrics\n",
    "    configs_json = snakemake.input.config\n",
    "    print(f\"{metrics_json = }\", f\"{configs_json = }\", sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29aad0d1-c175-4b19-aa87-51140de3cf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_out = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4643bbea-09af-479a-b3e5-52fbd47fa6af",
   "metadata": {},
   "source": [
    "## Metrics of each run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc31625-6f9d-4f96-b531-93561e7bafb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_metrics_json = pathlib.Path(metrics_json)\n",
    "path_configs_json = pathlib.Path(configs_json)\n",
    "FOLDER = path_metrics_json.parent\n",
    "\n",
    "metrics_dict = vaep.io.load_json(path_metrics_json)\n",
    "configs_dict = vaep.io.load_json(path_configs_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c27e42-63a3-4ce8-b5e1-a022918c08b3",
   "metadata": {},
   "source": [
    "Random sample metric schema (all should be the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05645db5-fed7-435a-a105-63c5166be601",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_sampled = vaep.utils.sample_iterable(metrics_dict, 1)[0]\n",
    "key_map = vaep.pandas.key_map(metrics_dict[key_sampled])\n",
    "key_map  # keys of dictionary for a single run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d92fb12-c384-4bb2-acce-4ec1d0b6597c",
   "metadata": {},
   "source": [
    "Metrics a `pandas.DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f324d3-48bf-43ef-aa2a-c0bd1ab9c188",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics_dict_multikey = {}\n",
    "for k, run_metrics in metrics_dict.items():\n",
    "    metrics_dict_multikey[k] = {eval(k): v for k, v in run_metrics.items()}\n",
    "\n",
    "metrics = pd.DataFrame.from_dict(metrics_dict_multikey, orient='index')\n",
    "metrics.columns.names = ['subset', 'data_split', 'model', 'metric_name']\n",
    "metrics.index.name = 'id'\n",
    "metrics = metrics.dropna(axis=1, how='all')\n",
    "metrics = metrics.stack('model')\n",
    "metrics = metrics.drop_duplicates()\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c1bfc3-f189-4a73-b13a-5e24ab113a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.sort_values(by=('NA interpolated', 'valid_fake_na', 'MAE'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5f9b28-195e-4c74-a8de-c776fd0f4849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort_by = 'MAE'\n",
    "metric_columns = ['MSE', 'MAE']\n",
    "model_keys = ['collab', 'dae', 'vae']\n",
    "subset = metrics.columns.levels[0][0]\n",
    "print(f\"{subset = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7278b3-ef8a-43f8-ade0-d8777b9b41b4",
   "metadata": {},
   "source": [
    "## Configuration of each run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24e754e-9c0e-46dd-962e-084da27c9229",
   "metadata": {},
   "source": [
    "Experiment metadata from configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815da3a8-11e7-4ed4-bdaf-dc331eddc175",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_json(path_configs_json).T\n",
    "meta['hidden_layers'] = meta.loc[meta['hidden_layers'].notna(\n",
    "), 'hidden_layers'].apply(tuple)  # make list a tuple\n",
    "meta['n_hidden_layers'] = meta.hidden_layers.loc[meta['hidden_layers'].notna()\n",
    "                                                 ].apply(len).fillna(0)\n",
    "\n",
    "mask_collab = meta.index.str.contains('collab')\n",
    "meta.loc[mask_collab, 'batch_size'] = meta.loc[mask_collab, 'batch_size_collab']\n",
    "meta.loc[mask_collab, 'hidden_layers'] = None\n",
    "\n",
    "meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dd468f-8995-403d-a389-6c4e4e912cd5",
   "metadata": {},
   "source": [
    "Batch size for collab models depends on a factor (as the data in long format has roughly  N samples * M features entries)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c44238-09d1-4c79-95f3-597cce466cb2",
   "metadata": {},
   "source": [
    "## Colorcoded metrics\n",
    "\n",
    "- can be one of the [matplotlib color maps](https://matplotlib.org/stable/tutorials/colors/colormaps.html), which also have reversed version indicated by `*_r`\n",
    "\n",
    "``` python\n",
    "['Accent', 'Accent_r', 'Blues', 'Blues_r', 'BrBG', 'BrBG_r', 'BuGn', 'BuGn_r', 'BuPu', 'BuPu_r', 'CMRmap', 'CMRmap_r', 'Dark2', 'Dark2_r', 'GnBu', 'GnBu_r', 'Greens', 'Greens_r', 'Greys', 'Greys_r', 'OrRd', 'OrRd_r', 'Oranges', 'Oranges_r', 'PRGn', 'PRGn_r', 'Paired', 'Paired_r', 'Pastel1', 'Pastel1_r', 'Pastel2', 'Pastel2_r', 'PiYG', 'PiYG_r', 'PuBu', 'PuBuGn', 'PuBuGn_r', 'PuBu_r', 'PuOr', 'PuOr_r', 'PuRd', 'PuRd_r', 'Purples', 'Purples_r', 'RdBu', 'RdBu_r', 'RdGy', 'RdGy_r', 'RdPu', 'RdPu_r', 'RdYlBu', 'RdYlBu_r', 'RdYlGn', 'RdYlGn_r', 'Reds', 'Reds_r', 'Set1', 'Set1_r', 'Set2', 'Set2_r', 'Set3', 'Set3_r', 'Spectral', 'Spectral_r', 'Wistia', 'Wistia_r', 'YlGn', 'YlGnBu', 'YlGnBu_r', 'YlGn_r', 'YlOrBr', 'YlOrBr_r', 'YlOrRd', 'YlOrRd_r', 'afmhot', 'afmhot_r', 'autumn', 'autumn_r', 'binary', 'binary_r', 'bone', 'bone_r', 'brg', 'brg_r', 'bwr', 'bwr_r', 'cividis', 'cividis_r', 'cool', 'cool_r', 'coolwarm', 'coolwarm_r', 'copper', 'copper_r', 'cubehelix', 'cubehelix_r', 'flag', 'flag_r', 'gist_earth', 'gist_earth_r', 'gist_gray', 'gist_gray_r', 'gist_heat', 'gist_heat_r', 'gist_ncar', 'gist_ncar_r', 'gist_rainbow', 'gist_rainbow_r', 'gist_stern', 'gist_stern_r', 'gist_yarg', 'gist_yarg_r', 'gnuplot', 'gnuplot2', 'gnuplot2_r', 'gnuplot_r', 'gray', 'gray_r', 'hot', 'hot_r', 'hsv', 'hsv_r', 'inferno', 'inferno_r', 'jet', 'jet_r', 'magma', 'magma_r', 'nipy_spectral', 'nipy_spectral_r', 'ocean', 'ocean_r', 'pink', 'pink_r', 'plasma', 'plasma_r', 'prism', 'prism_r', 'rainbow', 'rainbow_r', 'seismic', 'seismic_r', 'spring', 'spring_r', 'summer', 'summer_r', 'tab10', 'tab10_r', 'tab20', 'tab20_r', 'tab20b', 'tab20b_r', 'tab20c', 'tab20c_r', 'terrain', 'terrain_r', 'turbo', 'turbo_r', 'twilight', 'twilight_r', 'twilight_shifted', 'twilight_shifted_r', 'viridis', 'viridis_r', 'winter', 'winter_r']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a9c512-3d62-47bf-a2e6-cae812139181",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = 'cividis_r'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc40f89-177b-4750-8141-430ac49e5b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_styled = metrics.unstack('model')\n",
    "\n",
    "metrics_styled = (\n",
    "    metrics_styled.set_index(\n",
    "        pd.MultiIndex.from_frame(\n",
    "            meta.loc[metrics_styled.index, [\n",
    "                'latent_dim', 'hidden_layers', 'batch_size']]\n",
    "        ))\n",
    "    .sort_index()\n",
    "    .stack('model')\n",
    "    .style.background_gradient(cmap)\n",
    ")\n",
    "metrics = metrics_styled.data\n",
    "metrics_styled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98751f1e-e3e3-42ba-87e2-0d1b33d71927",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = FOLDER / 'metrics_styled.xlsx'\n",
    "files_out['metrics_styled.xlsx'] = fname\n",
    "metrics_styled.to_excel(fname)\n",
    "logger.info(f\"Saved styled metrics: {fname}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57933a40-4833-43f4-aa1c-79e20c64ed34",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Plot Top 10 for simulated Na validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee241c3-ec2b-4a3c-bd7f-464dee83c750",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = metrics[subset][\"valid_fake_na\"].sort_values(\n",
    "    'MSE').iloc[:10, :-1].plot(rot=45,\n",
    "                               x_compat=False,\n",
    "                               xticks=list(range(10)),\n",
    "                               marker='o',\n",
    "                               linestyle='',\n",
    "                               )\n",
    "_ = ax.set_xticklabels(ax.get_xticklabels(), rotation=45,\n",
    "                       horizontalalignment='right')\n",
    "fig = ax.get_figure()\n",
    "fig.tight_layout()\n",
    "vaep.savefig(fig, name='top_10_models_validation_fake_na', folder=FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b21959-b63d-45ba-9c02-486c49f9e1f6",
   "metadata": {},
   "source": [
    "## Create metrics in long format\n",
    "\n",
    "To use colors meaningfully, the long format of the data is needed.\n",
    "\n",
    "Rebuild metrics from dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2279f91a-37fd-4fbf-875d-ed34a4c3d77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_long = pd.DataFrame.from_dict(metrics_dict_multikey, orient='index')\n",
    "columns_names = ['subset', 'data_split', 'model', 'metric_name']\n",
    "metrics_long.columns.names = columns_names\n",
    "metrics_long.index.name = 'id'\n",
    "metrics_long.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6862bcfa-c01b-43ed-8a51-ddb994533d5e",
   "metadata": {},
   "source": [
    "Combine with total number of simulated NAs the metric is based on (`N`) into single column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5077fed-91dd-4459-9e64-257114ef3efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_N = metrics_long.loc[:, pd.IndexSlice[:, :, :, 'N']]\n",
    "metrics_N = metrics_N.stack(\n",
    "    ['subset', 'data_split', 'model', 'metric_name']).unstack('metric_name').astype(int)\n",
    "metrics_N  # .unstack(['subset', 'data_split', 'model',])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b761ec0-6f98-4cf7-a119-a9e3cfdca275",
   "metadata": {},
   "source": [
    "join total number of simulated NAs (`N`) used to compute metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8816c310-86ed-498b-9b3f-31ff2761317a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_long = metrics_long.loc[:, pd.IndexSlice[:, :, :, metric_columns]]\n",
    "metrics_long = metrics_long.stack(metrics_long.columns.names).to_frame(\n",
    "    'metric_value').reset_index('metric_name').join(metrics_N)\n",
    "metrics_long"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ff17f3-2723-440c-9a26-1ca40ed5008b",
   "metadata": {},
   "source": [
    "join metadata for each metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ccda5f-354e-40ae-8b5e-c37a34219a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_long = metrics_long.reset_index(\n",
    "    ['subset', 'data_split', 'model']).join(meta)\n",
    "metrics_long.index.name = 'id'\n",
    "metrics_long.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3a8b50-fa2e-4b37-bda5-464e337f6bbb",
   "metadata": {},
   "source": [
    "Combine number of parameters into one columns (they are mutually exclusive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b3467a-4b2b-4271-8550-827166398c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_long['n_params'] = metrics_long['n_params_collab']\n",
    "for key in ['DAE', 'VAE']:\n",
    "    mask = metrics_long.model == key\n",
    "    metrics_long.loc[mask, 'n_params'] = metrics_long.loc[mask,\n",
    "                                                          f'n_params_{key.lower()}']\n",
    "mask = metrics_long.model == 'interpolated'\n",
    "# at least overall (and 1 for the number of replicates?)\n",
    "metrics_long.loc[mask, 'n_params'] = 1\n",
    "mask = metrics_long.model == 'median'\n",
    "# number of features to calculate median of\n",
    "metrics_long.loc[mask, 'n_params'] = metrics_long.loc[mask, 'M']\n",
    "\n",
    "metrics_long[[*columns_names, 'n_params',\n",
    "              'n_params_vae', 'n_params_dae', 'n_params_collab']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800e4833-67eb-4656-89fd-04fc602a4ad6",
   "metadata": {},
   "source": [
    "A a descriptive column describing the `subset` and the total number of simulated NAs in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe64834f-b73f-47c2-b669-0c50b8aa6a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_long['subset_w_N'] = metrics_long['subset'].str[0:] + \\\n",
    "    ' - N: ' + metrics_long['N'].apply(lambda x: f\"{x:,d}\")\n",
    "metrics_long[['subset_w_N', 'subset']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c09cd35-5dad-44e0-b0b1-f0e5f5f0c693",
   "metadata": {},
   "source": [
    "Save for later inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0513100-b917-4f3d-b85b-0e054c99f293",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = FOLDER / 'metrics_long_df.csv'\n",
    "files_out['metrics_long_df.csv'] = fname\n",
    "metrics_long.to_csv(fname)  # Should all the plots be done without the metrics?\n",
    "logger.info(f\"Saved metrics in long format: {fname}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4607c64-2e90-4ed6-b337-8e210d7c37de",
   "metadata": {},
   "source": [
    "# Collection of Performance plots \n",
    "\n",
    "- specify `labels_dict` for plotly plotting\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcf7405-e205-4558-9335-083b398b97c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict = {\"NA not interpolated valid_collab collab MSE\": 'MSE',\n",
    "               'batch_size_collab': 'bs',\n",
    "               'n_hidden_layers': \"No. of hidden layers\",\n",
    "               'latent_dim': 'hidden layer dimension',\n",
    "               'subset_w_N': 'subset',\n",
    "               'n_params': 'no. of parameter',\n",
    "               \"metric_value\": 'value',\n",
    "               'metric_name': 'metric',\n",
    "               'freq': 'freq/feature prevalence (across samples)'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291f2894-2d9c-4fb5-ad7b-9b693d66d464",
   "metadata": {},
   "source": [
    "## Plot result of hyperparameter search for collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b60021c-fbef-45e0-a7df-01267e1691d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not robust\n",
    "category_orders = {'model': ['median', 'interpolated', 'collab', 'DAE', 'VAE'],\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa12123-a809-407c-81a2-62bf1b54b61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = \"NA interpolated valid_fake_na collab MAE\"\n",
    "model = 'collab'\n",
    "# col = (\"NA interpolated\",\"valid_fake_na\",\"collab\",\"MSE\")\n",
    "fig = px.scatter(metrics_long.query(f'model == \"{model}\"'),\n",
    "                 x=\"latent_dim\",\n",
    "                 y='metric_value',\n",
    "                 color=\"subset\",  # needs data in long format\n",
    "                 facet_row=\"metric_name\",\n",
    "                 facet_col=\"data_split\",\n",
    "                 title='Performance of collaborative filtering models',\n",
    "                 labels={**labels_dict, 'data_split': 'data split'},\n",
    "                 category_orders={'data_split': [\n",
    "                     'valid_fake_na', 'test_fake_na']},\n",
    "                 width=1600,\n",
    "                 height=700,\n",
    "                 template='none',\n",
    "                 )\n",
    "fig.update_layout(\n",
    "    font={'size': 18},\n",
    "    xaxis={'title': {'standoff': 15}},\n",
    "    yaxis={'title': {'standoff': 15}})\n",
    "fig.update_xaxes(dict(\n",
    "    tickmode='array',\n",
    "    tickvals=sorted(metrics_long[\"latent_dim\"].unique()),\n",
    ")\n",
    ")\n",
    "fname = FOLDER / 'collab_performance_overview.pdf'\n",
    "files_out['collab_performance_overview.pdf'] = fname\n",
    "fig.write_image(fname)\n",
    "logger.info(fname)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ba3f23-458e-4b20-8d02-527ef24c03a2",
   "metadata": {},
   "source": [
    "## Plot hyperparameter search results - overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31231114-7184-4c38-a0c5-5d399a6ba901",
   "metadata": {},
   "outputs": [],
   "source": [
    "hover_data = {k: ':,d' for k in\n",
    "              ['hidden_layers',\n",
    "               'latent_dim', 'n_params',\n",
    "               'batch_size', 'N'\n",
    "               ]}\n",
    "hover_data['data_split'] = True\n",
    "hover_data['metric_value'] = ':.4f'\n",
    "\n",
    "\n",
    "def plot_by_params(data_split: str = '', subset: str = ''):\n",
    "    selected = metrics_long\n",
    "    if data_split:\n",
    "        selected = selected.query(f'data_split == \"{data_split}\"')\n",
    "    if subset:\n",
    "        selected = selected.query(f'subset == \"{subset}\"')\n",
    "    fig = px.scatter(selected,\n",
    "                     x='n_params',\n",
    "                     y='metric_value',\n",
    "                     color=\"model\",\n",
    "                     facet_row=\"metric_name\",\n",
    "                     facet_col=\"subset_w_N\",\n",
    "                     hover_data=hover_data,\n",
    "                     title=f'Performance by number of parameters for {data_split.replace(\"_\", \" \")} data'.replace(\n",
    "                         \"  \", \" \"),\n",
    "                     labels=labels_dict,\n",
    "                     category_orders=category_orders,\n",
    "                     width=1600,\n",
    "                     height=700,\n",
    "                     template='none',\n",
    "                     )\n",
    "\n",
    "    fig.update_layout(\n",
    "        font={'size': 18},\n",
    "        xaxis={'title': {'standoff': 15}},\n",
    "        yaxis={'title': {'standoff': 15}})\n",
    "    return fig\n",
    "\n",
    "\n",
    "dataset = \"valid_fake_na\"\n",
    "fig = plot_by_params(dataset)\n",
    "files_out[f\"hyperpar_{dataset}_results_by_parameters_all.pdf\"] = (FOLDER /\n",
    "                                                                  f\"hyperpar_{dataset}_results_by_parameters_all.pdf\")\n",
    "fig.write_image(files_out[f\"hyperpar_{dataset}_results_by_parameters_all.pdf\"])\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ee0c78-a1b4-47a1-b862-d16c02767fdc",
   "metadata": {},
   "source": [
    "Only for NA which could also be interpolated for both test and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a8725a-4e94-4b6b-ba0a-f9fbc0e03937",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_by_params('', subset='NA interpolated')\n",
    "fname = FOLDER / f\"hyperpar_test_and_valid_fake_na_by_parameters_na_interpolated.pdf\"\n",
    "files_out[f\"hyperpar_test_and_valid_fake_na_by_parameters_na_interpolated.pdf\"] = fname\n",
    "fig.write_image(fname)\n",
    "logger.info(fname)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4833f4f-a7cc-46d0-9344-b336909f786b",
   "metadata": {},
   "source": [
    "Only for NA which could also be interpolated for both test and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8190d51-c4db-4aae-8b91-11641958a0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"test_fake_na\"\n",
    "fig = plot_by_params(dataset, 'NA interpolated')\n",
    "fname = (FOLDER /\n",
    "         f\"hyperpar_{dataset}_results_by_parameters_na_interpolated.pdf\")\n",
    "files_out[f\"hyperpar_{dataset}_results_by_parameters_na_interpolated.pdf\"] = fname\n",
    "fig.write_image(fname)\n",
    "logger.info(f\"Save to {fname}\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d309a52f-f955-4ff9-ae1b-e6082b451dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"valid_fake_na\"\n",
    "fig = plot_by_params(dataset, 'NA interpolated')\n",
    "fname = (FOLDER /\n",
    "         f\"hyperpar_{dataset}_results_by_parameters_na_interpolated.pdf\")\n",
    "files_out[f\"hyperpar_{dataset}_results_by_parameters_na_interpolated.pdf\"] = fname\n",
    "fig.write_image(fname)\n",
    "logger.info(f\"Save to {fname}\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47534519",
   "metadata": {},
   "source": [
    "## Select best model for each `latent_dim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdcc559-28de-4056-84b3-5f24ea175fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by = ['data_split', 'subset', 'latent_dim', 'metric_name', 'model']\n",
    "metrics_long_sel_min = metrics_long.reset_index(\n",
    "        ).groupby(by=group_by\n",
    "        ).apply(lambda df: df.sort_values(by='metric_value').iloc[0])\n",
    "metrics_long_sel_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43eceaa-ee22-456c-a3ca-1e830c17f750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plotly_figure(dataset: str, x='latent_dim'):\n",
    "    fig = px.scatter(metrics_long_sel_min.loc[dataset],\n",
    "                     x=x,\n",
    "                     y='metric_value',\n",
    "                     color=\"model\",\n",
    "                     facet_row=\"metric_name\",\n",
    "                     facet_col=\"subset_w_N\",\n",
    "                     hover_data=hover_data,\n",
    "                     title=f'Performance on {dataset.replace(\"_\", \" \")} data',\n",
    "                     labels=labels_dict,\n",
    "                     category_orders=category_orders,\n",
    "                     width=1600,\n",
    "                     height=700,\n",
    "                     template='none',\n",
    "                     )\n",
    "    fig.update_xaxes(dict(\n",
    "        tickmode='array',\n",
    "        tickvals=sorted(metrics_long[x].unique()),\n",
    "    )\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        font={'size': 18},\n",
    "        xaxis={'title': {'standoff': 15}},\n",
    "        yaxis={'title': {'standoff': 15}})\n",
    "    return fig\n",
    "\n",
    "\n",
    "dataset = 'test_fake_na'\n",
    "fig = get_plotly_figure(dataset)\n",
    "fname = FOLDER / f\"hyperpar_{dataset}_results_best.pdf\"\n",
    "files_out[f\"hyperpar_{dataset}_results_best.pdf\"] = fname\n",
    "fig.write_image(fname)\n",
    "logger.info(f\"Save to {fname}\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7061609a-7179-45dc-a8eb-3172731fce1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'valid_fake_na'\n",
    "fig = get_plotly_figure(dataset)\n",
    "fname = FOLDER / f\"hyperpar_{dataset}_results_best.pdf\"\n",
    "files_out[f\"hyperpar_{dataset}_results_best.pdf\"] = fname\n",
    "fig.write_image(fname)\n",
    "logger.info(f\"Save to {fname}\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa690af-5dcc-4857-a248-1860999f79d1",
   "metadata": {},
   "source": [
    "## Performance along feature prevalence in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655092b2-0ba2-4ff0-b408-d982df1501c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'valid_fake_na'\n",
    "group_by = ['data_split', 'subset', 'metric_name', 'model', 'latent_dim']\n",
    "METRIC = 'MAE'\n",
    "selected = metrics_long.reset_index(\n",
    ").groupby(by=group_by\n",
    "          ).apply(lambda df: df.sort_values(by='metric_value').iloc[0]).loc[dataset]\n",
    "files_out['best_models_metrics_per_latent.csv'] = FOLDER / 'best_models_metrics_per_latent.csv'\n",
    "selected.to_csv(files_out['best_models_metrics_per_latent.csv'])\n",
    "selected.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf60875d-7062-4a51-abdf-59d3f8463b67",
   "metadata": {},
   "source": [
    "### For best latent dimension (on average)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b26f17-7912-49f3-a2cd-07c8115bfa00",
   "metadata": {},
   "source": [
    "select minimum value of latent dim over trained models on average\n",
    " 1. select for each latent the best model configuration (by DL model)\n",
    " 2. Choose the on average best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98e3210-bafe-4456-b7e8-2bad861d0d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_latent = (selected.loc['NA interpolated']\n",
    "                      .loc[METRIC].loc[['DAE', 'VAE', 'collab']]\n",
    "                      .groupby(level='latent_dim')\n",
    "                      .mean()\n",
    "                      .sort_values('metric_value')\n",
    "              )\n",
    "min_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61abef54-3dff-4b89-b4df-50bc6ceb87a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_latent = min_latent.index[0]\n",
    "print(\"Minimum latent value for average of models:\", min_latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19d4f94-2734-4352-a71e-4574aa1742ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = selected.loc['NA interpolated'].loc['MAE'].loc[[\n",
    "    'collab', 'DAE', 'VAE']].loc[pd.IndexSlice[:, min_latent], :]\n",
    "selected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58717630-1cd3-49c4-bbb1-6fd72c3688b1",
   "metadata": {},
   "source": [
    "load predictions (could be made better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4f0b38-c04f-4161-b8f9-b9c224b45bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'test_fake_na'  # load test split predictions\n",
    "selected['pred_to_load'] = (\n",
    "    selected['out_preds']\n",
    "    + ('/pred_val' if 'val' in dataset else '/pred_test')  # not good...\n",
    "    + selected['hidden_layers'].apply(lambda s: '_hl_' + '_'.join(str(x)\n",
    "                                      for x in s) + '_' if s is not None else '_')\n",
    "    + selected.model.str.lower()\n",
    "    + '.csv'\n",
    ")\n",
    "selected['pred_to_load'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd2b861-4c89-4477-8961-b5009afaacc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248050bc-5dfc-4923-aa16-7bc4888813d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = {k: f'{k} - ' + \"HL: {}\".format(\n",
    "    str(selected.loc[k, 'hidden_layers'].to_list()[0]))\n",
    "    for k in selected.model\n",
    "}\n",
    "mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daea0ae-755d-4787-a319-4c737edc1252",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_split = compare_predictions.load_predictions(\n",
    "    selected['pred_to_load'].to_list())[['observed', *category_orders['model']]]\n",
    "pred_split = pred_split.rename(mapper, axis=1)\n",
    "category_orders['model'] = list(pred_split.columns[1:])\n",
    "pred_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7daaae4-0519-4346-a025-a8647545eb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasplits.DataSplits.from_folder(FOLDER / 'data', file_format='pkl')\n",
    "\n",
    "N_SAMPLES = int(data.train_X.index.levels[0].nunique())\n",
    "# selection criteria # maybe to be set externally (depends on data selection)\n",
    "FREQ_MIN = int(N_SAMPLES * 0.25)\n",
    "\n",
    "logger.info(\n",
    "    f\"N Samples: {N_SAMPLES:,d} - set minumum: {FREQ_MIN:,d} for plotting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a09507-9cd0-4210-ba10-8cd514a9ed1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_feat = sampling.frequency_by_index(data.train_X, 0)\n",
    "freq_feat.name = 'freq'\n",
    "# freq_feat = vaep.io.datasplits.load_freq(data_folder) # could be loaded from datafolder\n",
    "freq_feat.head()  # training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea944a52-9a1c-4375-9ca8-f759edf6933b",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = vaep.pandas.calc_errors_per_feat(\n",
    "    pred=pred_split, freq_feat=freq_feat, target_col='observed')\n",
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815abad4-4224-4c7c-a793-dc949ad73940",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_out[f'n_obs_error_counts_{dataset}.pdf'] = (FOLDER /\n",
    "                                                  f'n_obs_error_counts_{dataset}.pdf')\n",
    "ax = errors['n_obs'].value_counts().sort_index().plot(style='.')\n",
    "vaep.savefig(ax.get_figure(),  files_out[f'n_obs_error_counts_{dataset}.pdf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47911dc8-f434-4d45-963d-579b75444508",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = errors.plot.scatter('freq', 'n_obs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e803cf3-93ca-4588-9eed-0f43cf1ab593",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs_error_is_based_on = errors['n_obs']\n",
    "errors = errors.drop('n_obs', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52170533-1a9d-469a-9300-8811f50fc628",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "M_feat = len(errors)\n",
    "window_size = int(M_feat / 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e65cb0-d3c1-4c60-b34f-9507de6becb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_smoothed = errors.copy()\n",
    "# errors_smoothed[errors.columns[:-1]] = errors[errors.columns[:-1]].rolling(window=window_size, min_periods=1).mean()\n",
    "errors_smoothed[category_orders['model']] = errors[category_orders['model']].rolling(\n",
    "    window=window_size, min_periods=1).mean()\n",
    "errors_smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7743f5b0-ce83-4471-a69b-4e9f55f49467",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bfa6d1-503a-412c-a01f-8739e318b6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = errors_smoothed[freq_feat.name] >= FREQ_MIN\n",
    "ax = errors_smoothed.loc[mask].rename_axis('', axis=1).plot(x=freq_feat.name,\n",
    "                                                            xlabel='freq/feature prevalence (across samples)',\n",
    "                                                            ylabel=f'rolling average error ({METRIC})',\n",
    "                                                            xlim=(\n",
    "                                                                FREQ_MIN, errors_smoothed[freq_feat.name].max()),\n",
    "                                                            # title=f'Rolling average error by feature frequency {msg_annotation}'\n",
    "                                                            )\n",
    "\n",
    "msg_annotation = f\"(Latend dim: {min_latent}, No. of feat: {M_feat}, window_size: {window_size})\"\n",
    "print(msg_annotation)\n",
    "\n",
    "files_out[f'best_models_ld_{min_latent}_rolling_errors_by_freq'] = (FOLDER /\n",
    "                                                                    f'best_models_ld_{min_latent}_rolling_errors_by_freq')\n",
    "vaep.savefig(\n",
    "    ax.get_figure(),\n",
    "    name=files_out[f'best_models_ld_{min_latent}_rolling_errors_by_freq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38a4be1-26d2-4983-ac39-b32d4918e2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_smoothed_long = errors_smoothed.drop('freq', axis=1).stack().to_frame(\n",
    "    'rolling error average').reset_index(-1).join(freq_feat)\n",
    "errors_smoothed_long"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d646009-eff7-46f2-b3f4-7f28782e2435",
   "metadata": {},
   "source": [
    "Save html versin of curve with annotation of errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2199ed3-7593-49f8-8620-f8eb779250fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = px_vaep.line(errors_smoothed_long.loc[errors_smoothed_long[freq_feat.name] >= FREQ_MIN].join(n_obs_error_is_based_on).sort_values(by='freq'),\n",
    "                   x=freq_feat.name,\n",
    "                   color='model',\n",
    "                   y='rolling error average',\n",
    "                   hover_data=['n_obs'],\n",
    "                   # title=f'Rolling average error by feature frequency {msg_annotation}',\n",
    "                   labels=labels_dict,\n",
    "                   category_orders=category_orders,\n",
    "                   )\n",
    "fig = px_vaep.apply_default_layout(fig)\n",
    "fig.update_layout(legend_title_text='')  # remove legend title\n",
    "files_out[f'best_models_ld_{min_latent}_errors_by_freq_plotly.html'] = (FOLDER /\n",
    "                                                                        f'best_models_ld_{min_latent}_errors_by_freq_plotly.html')\n",
    "fig.write_html(\n",
    "    files_out[f'best_models_ld_{min_latent}_errors_by_freq_plotly.html'])\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97748fea",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "#### Average error by feature frequency.\n",
    "Group all features with same frequency and calculate average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68be9bc-4327-4df9-bc6b-67d39fd77318",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_smoothed = errors.copy()\n",
    "\n",
    "ax = errors_smoothed.loc[errors_smoothed['freq'] >= FREQ_MIN].groupby(by='freq'\n",
    "                                                                      ).mean(\n",
    ").sort_index(\n",
    ").rolling(window=3, min_periods=1\n",
    "          ).mean(\n",
    ").rename_axis('', axis=1\n",
    "              ).plot(\n",
    "    xlabel='freq/ feature prevalence (across samples)',\n",
    "    ylabel='rolling error average',\n",
    "    # title='mean error for features averaged for each frequency'\n",
    "    xlim=(FREQ_MIN, freq_feat.max())\n",
    ")\n",
    "files_out[f'best_models_ld_{min_latent}_errors_by_freq_averaged'] = (FOLDER /\n",
    "                                                                     f'best_models_ld_{min_latent}_errors_by_freq_averaged')\n",
    "vaep.savefig(\n",
    "    ax.get_figure(),\n",
    "    files_out[f'best_models_ld_{min_latent}_errors_by_freq_averaged'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb3550a-059a-4bbd-a019-2a4423da1b99",
   "metadata": {},
   "source": [
    "### For best models per model class\n",
    "\n",
    "- select on validation data, report on prediction on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc323b26-d835-4e5a-beda-7c95d7c6c3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by = ['data_split', 'subset', 'metric_name', 'model']\n",
    "\n",
    "order_categories = {'data level': ['proteinGroups', 'aggPeptides', 'evidence'],\n",
    "                    'model': ['median', 'interpolated', 'collab', 'DAE', 'VAE']}\n",
    "order_models = order_categories['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19188ecb-e5ae-40fe-a255-c0a0053be87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'valid_fake_na'  # select on validation split\n",
    "\n",
    "selected = metrics_long.reset_index(\n",
    ").groupby(by=group_by\n",
    "          ).apply(lambda df: df.sort_values(by='metric_value').iloc[0]).loc[dataset]\n",
    "selected = selected.loc[pd.IndexSlice['NA interpolated',\n",
    "                                      'MAE']].loc[order_models]\n",
    "selected.to_csv(FOLDER / 'best_models_metrics.csv')\n",
    "selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e10c4d2-fb7a-402b-ad9c-3f8cf03a3896",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = selected.loc[['collab', 'DAE', 'VAE']]\n",
    "selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64c09c6-acd3-46c1-b9d1-c26cb0c4b152",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'test_fake_na'  # load test split predictions\n",
    "selected['pred_to_load'] = (\n",
    "    selected['out_preds']\n",
    "    + ('/pred_val' if 'val' in dataset else '/pred_test')  # not good...\"\n",
    "    + selected['hidden_layers'].apply(lambda s: '_hl_' + '_'.join(str(x)\n",
    "                                      for x in s) + '_' if s is not None else '_')\n",
    "    + selected.model.str.lower()\n",
    "    + '.csv'\n",
    ")\n",
    "selected['pred_to_load'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848c8206-37fe-401c-b9d4-d450388571cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = {k: f'{k} - LD: {selected.loc[k, \"latent_dim\"]} - HL: {selected.loc[k, \"hidden_layers\"]} '\n",
    "          for k in selected.model\n",
    "          }\n",
    "mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6831af84-5ace-4191-9b8d-eb5f518a8d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_split = compare_predictions.load_predictions(\n",
    "    selected['pred_to_load'].to_list())[['observed', *order_models]]\n",
    "pred_split = pred_split.rename(mapper, axis=1)\n",
    "order_models = list(pred_split.columns[1:])\n",
    "pred_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d0299b-6188-451d-9a52-5e6e188354a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_count = pred_split.groupby(\n",
    "    by=pred_split.index.names[-1])[pred_split.columns[0]].count()\n",
    "ax = feat_count.hist(legend=False)\n",
    "ax.set_xlabel('feat used for comparison (in split)')\n",
    "ax.set_ylabel('observations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c1bcf4-5022-4dfb-89ba-f5279d8a8a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded above\n",
    "freq_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69004a4-d9c6-4c4e-9508-9bb30ff4364b",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = vaep.pandas.calc_errors_per_feat(\n",
    "    pred=pred_split, freq_feat=freq_feat, target_col='observed')\n",
    "idx_name = errors.index.name\n",
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497ea5d5-1b4f-48fa-8d71-4c52c0a8cacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_out[f'best_models_errors_counts_obs_{dataset}.pdf'] = (FOLDER /\n",
    "                                                             f'n_obs_error_counts_{dataset}.pdf')\n",
    "ax = errors['n_obs'].value_counts().sort_index().plot(style='.')\n",
    "vaep.savefig(ax.get_figure(),\n",
    "             files_out[f'best_models_errors_counts_obs_{dataset}.pdf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70524224-eadf-4c76-aec2-a2ea0e92360f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs_error_is_based_on = errors['n_obs']\n",
    "errors = errors.drop('n_obs', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427806b6-f8c9-44d5-9641-1cb742f254e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shoudl be the same\n",
    "M_feat = len(errors)\n",
    "window_size = int(M_feat / 50)\n",
    "print(\n",
    "    f\"Features in split: {M_feat}, set window size for smoothing: {window_size}\")\n",
    "msg_annotation = f\"(No. of feat: {M_feat}, window_size: {window_size})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df01642a-726f-4375-8057-749a8f046d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_smoothed = errors.copy()\n",
    "errors_smoothed[order_models] = errors[order_models].rolling(\n",
    "    window=window_size, min_periods=1).mean()\n",
    "mask = errors_smoothed[freq_feat.name] >= FREQ_MIN\n",
    "ax = errors_smoothed.loc[mask].rename_axis('', axis=1).plot(x=freq_feat.name,\n",
    "                                                            ylabel='rolling error average',\n",
    "                                                            xlabel='freq/feature prevalence (across samples)',\n",
    "                                                            xlim=(\n",
    "                                                                FREQ_MIN, freq_feat.max()),\n",
    "                                                            # title=f'Rolling average error by feature frequency {msg_annotation}'\n",
    "                                                            )\n",
    "\n",
    "vaep.savefig(\n",
    "    ax.get_figure(),\n",
    "    folder=FOLDER,\n",
    "    name=f'best_models_rolling_errors_{dataset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea784fe-018a-4baa-95c0-75125ac1de17",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_smoothed_long = errors_smoothed.drop('freq', axis=1).stack().to_frame(\n",
    "    'rolling error average').reset_index(-1).join(freq_feat).join(feat_count).reset_index()\n",
    "errors_smoothed_long"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4321ff5e-f9ad-48f4-8eb1-d78d6a01daf5",
   "metadata": {},
   "source": [
    "Save html versin of curve with annotation of errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc280dd7-a018-430f-bf71-bdb0e5ed2794",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px_vaep.line((errors_smoothed_long.loc[errors_smoothed_long[freq_feat.name] >= FREQ_MIN]\n",
    "                                        .join(n_obs_error_is_based_on)\n",
    "                                        .sort_values(by='freq')),\n",
    "                   x=freq_feat.name,\n",
    "                   color='model',\n",
    "                   y='rolling error average',\n",
    "                   title=f'Rolling average error by feature frequency {msg_annotation}',\n",
    "                   labels=labels_dict,\n",
    "                   hover_data=[feat_count.name, idx_name, 'n_obs'],\n",
    "                   category_orders={'model': order_models})\n",
    "fig = px_vaep.apply_default_layout(fig)\n",
    "fig.update_layout(legend_title_text='')  # remove legend title\n",
    "files_out[f'best_models_errors_{dataset}_by_freq_plotly.html'] = (FOLDER /\n",
    "                                                                  f'best_models_errors_{dataset}_by_freq_plotly.html')\n",
    "# fig.write_image(FOLDER / f'best_models_errors_{dataset}_by_freq_plotly.pdf')\n",
    "fig.write_html(files_out[f'best_models_errors_{dataset}_by_freq_plotly.html'])\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a3c083-cdad-4ff3-a1d1-d7f9110f59bd",
   "metadata": {},
   "source": [
    "## Correlation plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877712f6-a4a1-428b-b944-748cf7122704",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8f6ba9-85d8-41a7-b909-1fee89ee3344",
   "metadata": {},
   "source": [
    "### by feature across samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4af7b37-456d-45cc-abaf-68a0275d10b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_per_feat = pred_split.groupby(idx_name).aggregate(\n",
    "    lambda df: df.corr().loc['observed'])[order_models]\n",
    "corr_per_feat = corr_per_feat.join(pred_split.groupby(idx_name)[\n",
    "                                   'median'].count().rename('n_obs'))\n",
    "too_few_obs = corr_per_feat['n_obs'] < 3\n",
    "corr_per_feat.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60519011-1aba-4e6c-8495-3ccb6454e307",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_per_feat.loc[~too_few_obs].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195d9117-c1dc-49ef-8acb-6f76d1e56970",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_per_feat.loc[too_few_obs].dropna(thresh=3, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeb84ed-33fe-4b7f-b8f6-bbe6e2687bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = 8, 8  # None\n",
    "fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "kwargs = dict(rot=45,\n",
    "              # title='Corr. betw. simulated NA and model pred. per feat',\n",
    "              ylabel=f'correlation per feature ({idx_name})')\n",
    "ax = corr_per_feat.loc[~too_few_obs].drop(\n",
    "    'n_obs', axis=1).plot.box(**kwargs, ax=ax)\n",
    "_ = ax.set_xticklabels(ax.get_xticklabels(), rotation=45,\n",
    "                       horizontalalignment='right')\n",
    "files_out[f'pred_corr_per_feat_{dataset}'] = (FOLDER /\n",
    "                                              f'pred_corr_per_feat_{dataset}')\n",
    "vaep.savefig(ax.get_figure(), name=files_out[f'pred_corr_per_feat_{dataset}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51415a04-7840-4132-945e-cd1de04de7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_out[f'pred_corr_per_feat_{dataset}.xlsx'] = (FOLDER /\n",
    "                                                   f'pred_corr_per_feat_{dataset}.xlsx')\n",
    "with pd.ExcelWriter(files_out[f'pred_corr_per_feat_{dataset}.xlsx']) as writer:\n",
    "    corr_per_feat.loc[~too_few_obs].describe().to_excel(\n",
    "        writer, sheet_name='summary')  # excluded -1 and 1 version\n",
    "    # complete information\n",
    "    corr_per_feat.to_excel(writer, sheet_name='correlations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b375fcbc-6cf0-4797-be5c-0ddd25468f75",
   "metadata": {},
   "source": [
    "### within sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e1de68-71ba-4868-99c0-54201e8a20af",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_per_sample = pred_split.groupby('Sample ID').aggregate(\n",
    "    lambda df: df.corr().loc['observed'])[order_models]\n",
    "corr_per_sample = corr_per_sample.join(pred_split.groupby('Sample ID')[\n",
    "                                       'median'].count().rename('n_obs'))\n",
    "corr_per_sample.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803aea2b-1a56-4fc7-ac7a-c504949f3e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "kwargs = dict(ylim=(0.7, 1), rot=45,\n",
    "              # title='Corr. betw. simulated NA and model pred. per sample',\n",
    "              ylabel='correlation per sample')\n",
    "ax = corr_per_sample.drop('n_obs', axis=1).plot.box(**kwargs, ax=ax)\n",
    "_ = ax.set_xticklabels(ax.get_xticklabels(), rotation=45,\n",
    "                       horizontalalignment='right')\n",
    "files_out[f'pred_corr_per_sample_{dataset}'] = (FOLDER /\n",
    "                                                f'pred_corr_per_sample_{dataset}')\n",
    "vaep.savefig(ax.get_figure(),\n",
    "             name=files_out[f'pred_corr_per_sample_{dataset}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ad6407-2eb0-4ad2-bd43-d6938b6139d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_out[f'pred_corr_per_sample_{dataset}.xlsx'] = (FOLDER /\n",
    "                                                     f'pred_corr_per_sample_{dataset}.xlsx')\n",
    "with pd.ExcelWriter(files_out[f'pred_corr_per_sample_{dataset}.xlsx']) as writer:\n",
    "    corr_per_sample.describe().to_excel(writer, sheet_name='summary')\n",
    "    corr_per_sample.to_excel(writer, sheet_name='correlations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432b7824-12c0-41a5-87a7-9d921ee8e5f9",
   "metadata": {},
   "source": [
    "# Files written to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47abbd4b-c617-4be1-8b5a-d1f1963d9607",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_out"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "vaep",
   "language": "python",
   "name": "vaep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "cf83e9cb890c7f96eb0ae04f39a82254555f56a1a0ed2f03b23a8b40fe6cd31c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
